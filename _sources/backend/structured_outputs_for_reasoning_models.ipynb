{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs For Reasoning Models\n",
    "\n",
    "When working with reasoning models that use special tokens like `<think>...</think>` to denote reasoning sections, you might want to allow free-form text within these sections while still enforcing grammar constraints on the rest of the output.\n",
    "\n",
    "SGLang provides a feature to disable grammar restrictions within reasoning sections. This is particularly useful for models that need to perform complex reasoning steps before providing a structured output.\n",
    "\n",
    "To enable this feature, use the `--reasoning-parser` flag which decide the think_end_token, such as `</think>`, when launching the server. You can also specify the reasoning parser using the `--reasoning-parser` flag.\n",
    "\n",
    "## Supported Models\n",
    "\n",
    "Currently, SGLang supports the following reasoning models:\n",
    "- [DeepSeek R1 series](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "- [QwQ](https://huggingface.co/Qwen/QwQ-32B): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "## OpenAI Compatible API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `--grammar-backend`, `--reasoning-parser` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:29:47.745667Z",
     "iopub.status.busy": "2025-04-19T20:29:47.745382Z",
     "iopub.status.idle": "2025-04-19T20:30:25.277781Z",
     "shell.execute_reply": "2025-04-19T20:30:25.277234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:03] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30162, mem_fraction_static=0.88, max_running_requests=200, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=803327283, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='deepseek-r1', dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_llama4_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:14 TP0] Attention backend not set. Use fa3 backend by default.\n",
      "[2025-04-19 20:30:14 TP0] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:14 TP0] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-04-19 20:30:14 TP0] Load weight begin. avail mem=38.12 GB\n",
      "[2025-04-19 20:30:15 TP0] Ignore import error when loading sglang.srt.models.llama4. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:15 TP0] Using model weights format ['*.safetensors']\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]\n",
      "\n",
      "[2025-04-19 20:30:18 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=23.68 GB, mem usage=14.44 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:18 TP0] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB\n",
      "[2025-04-19 20:30:18 TP0] Memory pool end. avail mem=19.53 GB\n",
      "[2025-04-19 20:30:18 TP0] \n",
      "\n",
      "CUDA Graph is DISABLED.\n",
      "This will cause significant performance degradation.\n",
      "CUDA Graph should almost never be disabled in most usage scenarios.\n",
      "If you encounter OOM issues, please try setting --mem-fraction-static to a lower value (such as 0.8 or 0.7) instead of disabling CUDA Graph.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:19 TP0] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=200, context_len=131072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:19] INFO:     Started server process [2516541]\n",
      "[2025-04-19 20:30:19] INFO:     Waiting for application startup.\n",
      "[2025-04-19 20:30:19] INFO:     Application startup complete.\n",
      "[2025-04-19 20:30:19] INFO:     Uvicorn running on http://0.0.0.0:30162 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:20] INFO:     127.0.0.1:51140 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:20] INFO:     127.0.0.1:51144 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-04-19 20:30:20 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --reasoning-parser deepseek-r1\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")\n",
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "you can directly define a JSON schema or use [Pydantic](https://docs.pydantic.dev/latest/) to define and validate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:30:25.279901Z",
     "iopub.status.busy": "2025-04-19T20:30:25.279522Z",
     "iopub.status.idle": "2025-04-19T20:30:47.845750Z",
     "shell.execute_reply": "2025-04-19T20:30:47.845343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:26 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 1, token usage: 0.00, #running-req: 1, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:27] INFO:     127.0.0.1:51148 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-04-19 20:30:27] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:27 TP0] Decode batch. #running-req: 1, #token: 57, token usage: 0.00, gen throughput (token/s): 5.16, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:28 TP0] Decode batch. #running-req: 1, #token: 97, token usage: 0.00, gen throughput (token/s): 102.08, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:28 TP0] Decode batch. #running-req: 1, #token: 137, token usage: 0.01, gen throughput (token/s): 97.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:28 TP0] Decode batch. #running-req: 1, #token: 177, token usage: 0.01, gen throughput (token/s): 101.21, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:29 TP0] Decode batch. #running-req: 1, #token: 217, token usage: 0.01, gen throughput (token/s): 102.28, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:29 TP0] Decode batch. #running-req: 1, #token: 257, token usage: 0.01, gen throughput (token/s): 99.17, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:30 TP0] Decode batch. #running-req: 1, #token: 297, token usage: 0.01, gen throughput (token/s): 101.85, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:30 TP0] Decode batch. #running-req: 1, #token: 337, token usage: 0.02, gen throughput (token/s): 102.26, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:30 TP0] Decode batch. #running-req: 1, #token: 377, token usage: 0.02, gen throughput (token/s): 102.26, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:31 TP0] Decode batch. #running-req: 1, #token: 417, token usage: 0.02, gen throughput (token/s): 102.16, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:31 TP0] Decode batch. #running-req: 1, #token: 457, token usage: 0.02, gen throughput (token/s): 102.71, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:32 TP0] Decode batch. #running-req: 1, #token: 497, token usage: 0.02, gen throughput (token/s): 103.38, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:32 TP0] Decode batch. #running-req: 1, #token: 537, token usage: 0.03, gen throughput (token/s): 98.08, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:32 TP0] Decode batch. #running-req: 1, #token: 577, token usage: 0.03, gen throughput (token/s): 100.71, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:33 TP0] Decode batch. #running-req: 1, #token: 617, token usage: 0.03, gen throughput (token/s): 101.08, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:33 TP0] Decode batch. #running-req: 1, #token: 657, token usage: 0.03, gen throughput (token/s): 100.41, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:34 TP0] Decode batch. #running-req: 1, #token: 697, token usage: 0.03, gen throughput (token/s): 100.25, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:34 TP0] Decode batch. #running-req: 1, #token: 737, token usage: 0.04, gen throughput (token/s): 99.06, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:34 TP0] Decode batch. #running-req: 1, #token: 777, token usage: 0.04, gen throughput (token/s): 98.96, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:35 TP0] Decode batch. #running-req: 1, #token: 817, token usage: 0.04, gen throughput (token/s): 99.58, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:35 TP0] Decode batch. #running-req: 1, #token: 857, token usage: 0.04, gen throughput (token/s): 101.44, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:36 TP0] Decode batch. #running-req: 1, #token: 897, token usage: 0.04, gen throughput (token/s): 98.16, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:36 TP0] Decode batch. #running-req: 1, #token: 937, token usage: 0.05, gen throughput (token/s): 100.48, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:36 TP0] Decode batch. #running-req: 1, #token: 977, token usage: 0.05, gen throughput (token/s): 99.78, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:37 TP0] Decode batch. #running-req: 1, #token: 1017, token usage: 0.05, gen throughput (token/s): 100.28, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:37 TP0] Decode batch. #running-req: 1, #token: 1057, token usage: 0.05, gen throughput (token/s): 99.78, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:38 TP0] Decode batch. #running-req: 1, #token: 1097, token usage: 0.05, gen throughput (token/s): 99.50, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:38 TP0] Decode batch. #running-req: 1, #token: 1137, token usage: 0.06, gen throughput (token/s): 100.23, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:38 TP0] Decode batch. #running-req: 1, #token: 1177, token usage: 0.06, gen throughput (token/s): 100.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:39 TP0] Decode batch. #running-req: 1, #token: 1217, token usage: 0.06, gen throughput (token/s): 100.24, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:39 TP0] Decode batch. #running-req: 1, #token: 1257, token usage: 0.06, gen throughput (token/s): 102.59, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:40 TP0] Decode batch. #running-req: 1, #token: 1297, token usage: 0.06, gen throughput (token/s): 100.09, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:40 TP0] Decode batch. #running-req: 1, #token: 1337, token usage: 0.07, gen throughput (token/s): 98.13, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:40 TP0] Decode batch. #running-req: 1, #token: 1377, token usage: 0.07, gen throughput (token/s): 95.07, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:41 TP0] Decode batch. #running-req: 1, #token: 1417, token usage: 0.07, gen throughput (token/s): 100.40, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:41 TP0] Decode batch. #running-req: 1, #token: 1457, token usage: 0.07, gen throughput (token/s): 100.44, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:42 TP0] Decode batch. #running-req: 1, #token: 1497, token usage: 0.07, gen throughput (token/s): 100.35, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:42 TP0] Decode batch. #running-req: 1, #token: 1537, token usage: 0.08, gen throughput (token/s): 100.17, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:42 TP0] Decode batch. #running-req: 1, #token: 1577, token usage: 0.08, gen throughput (token/s): 97.58, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:43 TP0] Decode batch. #running-req: 1, #token: 1617, token usage: 0.08, gen throughput (token/s): 102.79, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:43 TP0] Decode batch. #running-req: 1, #token: 1657, token usage: 0.08, gen throughput (token/s): 99.18, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:44 TP0] Decode batch. #running-req: 1, #token: 1697, token usage: 0.08, gen throughput (token/s): 98.42, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:44 TP0] Decode batch. #running-req: 1, #token: 1737, token usage: 0.08, gen throughput (token/s): 98.98, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:44 TP0] Decode batch. #running-req: 1, #token: 1777, token usage: 0.09, gen throughput (token/s): 101.04, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:45 TP0] Decode batch. #running-req: 1, #token: 1817, token usage: 0.09, gen throughput (token/s): 100.86, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:45 TP0] Decode batch. #running-req: 1, #token: 1857, token usage: 0.09, gen throughput (token/s): 99.20, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:46 TP0] Decode batch. #running-req: 1, #token: 1897, token usage: 0.09, gen throughput (token/s): 101.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:46 TP0] Decode batch. #running-req: 1, #token: 1937, token usage: 0.09, gen throughput (token/s): 98.81, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:46 TP0] Decode batch. #running-req: 1, #token: 1977, token usage: 0.10, gen throughput (token/s): 102.18, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:47 TP0] Decode batch. #running-req: 1, #token: 2017, token usage: 0.10, gen throughput (token/s): 97.94, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:47 TP0] Decode batch. #running-req: 1, #token: 2057, token usage: 0.10, gen throughput (token/s): 98.32, #queue-req: 0, \n",
      "[2025-04-19 20:30:47] INFO:     127.0.0.1:51164 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to generate information about the capital of France, Paris, in JSON format. Let me think about how to approach this. First, I should recall what Paris is known for. It's the main city in France, right? I know it's a major cultural, economic, and political center. <br><br>I should probably start by listing the basic facts. Paris is the capital, so I'll note that. Then, I remember it's located in the northern part of the country, in the Île-de-France region. The coordinates are something like 48°51′N 2°28′E, but I'm not sure about the exact minutes and seconds. Maybe I should look that up, but since I'm just brainstorming, I'll approximate it.<br><br>Next, Paris is famous for landmarks. The Eiffel Tower is a must. The Louvre Museum is another major attraction. The Arc de Triomphe and Notre-Dame come to mind too. I should include those. Also, the city is known for its cuisine, like baguettes and croissants, and its fashion scene, which is quite influential.<br><br>I should think about Paris's history. It's been a political and cultural hub for centuries. It's the birthplace of many famous people, like Charlie de Gaulle and Victor Hugo. The Paris Agreement is another important fact, as it's related to climate change.<br><br>Economically, Paris is a significant center. It's home to major companies and financial institutions. The Île-de-France region is part of theGreater Paris metropolitan area, which is quite large.<br><br>Transportation-wise, Paris has an extensive public transit system, including the Métro, which is a large subway network. The Château de Invalides is a notable landmark with the statue of the Last Knight of the French Revolution.<br><br>I should also mention the Paris Agreement, which was signed in 2015. It's about reducing greenhouse gas emissions. That's an important environmental fact.<br><br>Now, putting this all together into a JSON structure. I'll start with a \"name\" field as \"Paris, France\". Then, an \"overview\" that summarizes its significance. The \"facts\" array will contain key points like being the capital, location, landmarks, cuisine, history, economy, transportation, and the Paris Agreement.<br><br>I need to make sure each fact is a string, so I'll just list them as strings without any special formatting. I'll avoid any markdown and keep it simple. Let me check if I missed anything. Maybe the population? But since it's a city, it's over a million, but perhaps that's too detailed for a basic JSON. I'll stick to the main points.<br><br>Wait, I should also include the coordinates more accurately. I think the Eiffel Tower is at 48.8584°N 2.2945°E, but I'm not entirely sure. I might need to double-check that. Also, the Arc de Triomphe is near the Eiffel Tower, so that's correct.<br><br>I think that's a solid list. Now, I'll structure it into JSON, making sure each key is in quotes and the values are strings. I'll separate each fact with a comma. Let me make sure there are no trailing commas to keep it valid JSON.<br><br><br>content: {<br><br>\"name\": \"Paris\",<br>\"population\": 120000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Please generate the information of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"foo\",\n",
    "            # convert the pydantic model to json schema\n",
    "            \"schema\": CapitalInfo.model_json_schema(),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:30:47.847478Z",
     "iopub.status.busy": "2025-04-19T20:30:47.847255Z",
     "iopub.status.idle": "2025-04-19T20:31:08.472287Z",
     "shell.execute_reply": "2025-04-19T20:31:08.471872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:47 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:48 TP0] Decode batch. #running-req: 1, #token: 49, token usage: 0.00, gen throughput (token/s): 73.34, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:48 TP0] Decode batch. #running-req: 1, #token: 89, token usage: 0.00, gen throughput (token/s): 100.24, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:49 TP0] Decode batch. #running-req: 1, #token: 129, token usage: 0.01, gen throughput (token/s): 96.72, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:49 TP0] Decode batch. #running-req: 1, #token: 169, token usage: 0.01, gen throughput (token/s): 98.55, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:49 TP0] Decode batch. #running-req: 1, #token: 209, token usage: 0.01, gen throughput (token/s): 100.75, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:50 TP0] Decode batch. #running-req: 1, #token: 249, token usage: 0.01, gen throughput (token/s): 101.76, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:50 TP0] Decode batch. #running-req: 1, #token: 289, token usage: 0.01, gen throughput (token/s): 101.68, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:51 TP0] Decode batch. #running-req: 1, #token: 329, token usage: 0.02, gen throughput (token/s): 101.40, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:51 TP0] Decode batch. #running-req: 1, #token: 369, token usage: 0.02, gen throughput (token/s): 101.69, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:51 TP0] Decode batch. #running-req: 1, #token: 409, token usage: 0.02, gen throughput (token/s): 102.26, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:52 TP0] Decode batch. #running-req: 1, #token: 449, token usage: 0.02, gen throughput (token/s): 102.08, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:52 TP0] Decode batch. #running-req: 1, #token: 489, token usage: 0.02, gen throughput (token/s): 102.78, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:53 TP0] Decode batch. #running-req: 1, #token: 529, token usage: 0.03, gen throughput (token/s): 100.57, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:53 TP0] Decode batch. #running-req: 1, #token: 569, token usage: 0.03, gen throughput (token/s): 99.16, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:53 TP0] Decode batch. #running-req: 1, #token: 609, token usage: 0.03, gen throughput (token/s): 99.58, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:54 TP0] Decode batch. #running-req: 1, #token: 649, token usage: 0.03, gen throughput (token/s): 99.36, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:54 TP0] Decode batch. #running-req: 1, #token: 689, token usage: 0.03, gen throughput (token/s): 100.32, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:55 TP0] Decode batch. #running-req: 1, #token: 729, token usage: 0.04, gen throughput (token/s): 96.64, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:55 TP0] Decode batch. #running-req: 1, #token: 769, token usage: 0.04, gen throughput (token/s): 99.29, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:55 TP0] Decode batch. #running-req: 1, #token: 809, token usage: 0.04, gen throughput (token/s): 100.97, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:56 TP0] Decode batch. #running-req: 1, #token: 849, token usage: 0.04, gen throughput (token/s): 99.06, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:56 TP0] Decode batch. #running-req: 1, #token: 889, token usage: 0.04, gen throughput (token/s): 99.32, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:57 TP0] Decode batch. #running-req: 1, #token: 929, token usage: 0.05, gen throughput (token/s): 99.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:57 TP0] Decode batch. #running-req: 1, #token: 969, token usage: 0.05, gen throughput (token/s): 98.76, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:57 TP0] Decode batch. #running-req: 1, #token: 1009, token usage: 0.05, gen throughput (token/s): 104.12, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:58 TP0] Decode batch. #running-req: 1, #token: 1049, token usage: 0.05, gen throughput (token/s): 101.55, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:58 TP0] Decode batch. #running-req: 1, #token: 1089, token usage: 0.05, gen throughput (token/s): 98.98, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:59 TP0] Decode batch. #running-req: 1, #token: 1129, token usage: 0.06, gen throughput (token/s): 102.84, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:59 TP0] Decode batch. #running-req: 1, #token: 1169, token usage: 0.06, gen throughput (token/s): 100.68, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:30:59 TP0] Decode batch. #running-req: 1, #token: 1209, token usage: 0.06, gen throughput (token/s): 100.83, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:00 TP0] Decode batch. #running-req: 1, #token: 1249, token usage: 0.06, gen throughput (token/s): 100.26, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:00 TP0] Decode batch. #running-req: 1, #token: 1289, token usage: 0.06, gen throughput (token/s): 98.00, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:01 TP0] Decode batch. #running-req: 1, #token: 1329, token usage: 0.06, gen throughput (token/s): 102.95, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:01 TP0] Decode batch. #running-req: 1, #token: 1369, token usage: 0.07, gen throughput (token/s): 97.75, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:01 TP0] Decode batch. #running-req: 1, #token: 1409, token usage: 0.07, gen throughput (token/s): 100.32, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:02 TP0] Decode batch. #running-req: 1, #token: 1449, token usage: 0.07, gen throughput (token/s): 99.66, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:02 TP0] Decode batch. #running-req: 1, #token: 1489, token usage: 0.07, gen throughput (token/s): 98.95, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:03 TP0] Decode batch. #running-req: 1, #token: 1529, token usage: 0.07, gen throughput (token/s): 98.04, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:03 TP0] Decode batch. #running-req: 1, #token: 1569, token usage: 0.08, gen throughput (token/s): 99.33, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:03 TP0] Decode batch. #running-req: 1, #token: 1609, token usage: 0.08, gen throughput (token/s): 98.44, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:04 TP0] Decode batch. #running-req: 1, #token: 1649, token usage: 0.08, gen throughput (token/s): 100.74, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:04 TP0] Decode batch. #running-req: 1, #token: 1689, token usage: 0.08, gen throughput (token/s): 99.50, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:05 TP0] Decode batch. #running-req: 1, #token: 1729, token usage: 0.08, gen throughput (token/s): 97.48, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:05 TP0] Decode batch. #running-req: 1, #token: 1769, token usage: 0.09, gen throughput (token/s): 103.67, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:05 TP0] Decode batch. #running-req: 1, #token: 1809, token usage: 0.09, gen throughput (token/s): 102.15, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:06 TP0] Decode batch. #running-req: 1, #token: 1849, token usage: 0.09, gen throughput (token/s): 99.45, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:06 TP0] Decode batch. #running-req: 1, #token: 1889, token usage: 0.09, gen throughput (token/s): 99.86, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:07 TP0] Decode batch. #running-req: 1, #token: 1929, token usage: 0.09, gen throughput (token/s): 95.11, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:07 TP0] Decode batch. #running-req: 1, #token: 1969, token usage: 0.10, gen throughput (token/s): 97.57, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:07 TP0] Decode batch. #running-req: 1, #token: 2009, token usage: 0.10, gen throughput (token/s): 98.31, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:08 TP0] Decode batch. #running-req: 1, #token: 2049, token usage: 0.10, gen throughput (token/s): 101.83, #queue-req: 0, \n",
      "[2025-04-19 20:31:08] INFO:     127.0.0.1:51164 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out the information about the capital of France and present it in JSON format. Let me start by recalling what I know about Paris. I know it's the capital, but I should probably double-check that. Yeah, I'm pretty sure Paris is the administrative capital, but sometimes people might confuse it with other cities like Lyon or Marseille. But no, Paris is definitely the official capital.<br><br>Now, moving on to the population. I think Paris is a very large city, one of the biggest in the world. I remember reading somewhere that it's over 3 million people, but I'm not sure of the exact figure. Maybe around 3.5 million? I should look that up to be accurate, but since I'm just brainstorming, I'll go with that estimate.<br><br>Next, the area. Paris is a big city, but it's also a dense urban area. I think the metropolitan area covers a large region, maybe around 12,000 square kilometers? But the city proper is smaller. I'm not exactly sure, but I'll put 10,500 square kilometers for the city area and 12,000 for the metropolitan area.<br><br>Language is another point. Paris is a center for French culture, so the predominant language there is definitely French. I don't think they speak any other language there predominantly, though there might be some English, especially in tourist areas or with expatriates.<br><br>Cuisine is interesting. Paris is known for its high-end, fine dining, especially French cuisine. I know places like Le Faitout and others that are famous for their intricate dishes. Parisians are also known for their coffee culture, so maybe that's another point to include.<br><br>Transportation-wise, Paris has an extensive public transit system. The RER and地铁 are part of the BTP, which I think stands for Bahn, Tram, and Metro in German, but in French, it's the same. The city is well-connected by train, with major stations like Gare du Nord and Châtelet. The Eiffel Tower is a major landmark, and it's accessible by train from Paris.<br><br>I should also mention some of the main attractions. The Eiffel Tower is iconic, along with the Louvre Museum, Notre-Dame Cathedral, and the Sacré-Cœur Basilica. These are must-see spots for tourists.<br><br>Now, putting this all together into JSON format. I'll structure it with an \"info\" key that contains a \"capital\" object with \"name,\" \"population,\" \"area,\" and \"language.\" Then, an \"attractions\" array that lists the main points of interest. I'll make sure the numbers are approximate since I don't have exact figures on hand.<br><br>Wait, I should check if the population is over 3 million or 3.5. I think it's around 3.5 million as of recent estimates. The area, I'm pretty sure the metropolitan area is about 12,000 km², and the city proper is a bit less, maybe 10,500 km². That seems right.<br><br>So, the JSON structure would have the info object with the necessary details, and the attractions array listing the main landmarks. I think that covers everything the user asked for. I should present it clearly, making sure the JSON is properly formatted with commas and quotes.<br><br><br>content: {<br><br>\"name\": \"Paris\",<br>\"population\": 350000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me the information of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\"name\": \"foo\", \"schema\": json.loads(json_schema)},\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:31:08.473926Z",
     "iopub.status.busy": "2025-04-19T20:31:08.473733Z",
     "iopub.status.idle": "2025-04-19T20:31:10.214427Z",
     "shell.execute_reply": "2025-04-19T20:31:10.214036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:08 TP0] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:08 TP0] Decode batch. #running-req: 1, #token: 44, token usage: 0.00, gen throughput (token/s): 78.82, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:09 TP0] Decode batch. #running-req: 1, #token: 84, token usage: 0.00, gen throughput (token/s): 67.00, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:09 TP0] Decode batch. #running-req: 1, #token: 124, token usage: 0.01, gen throughput (token/s): 66.11, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:10] INFO:     127.0.0.1:51164 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out the capital of France. I remember that France is a country in Europe, and I think its capital is Paris. But wait, I'm not entirely sure. Let me think about other capitals I know. Germany's capital is Berlin, Italy's is Rome, Spain's is Madrid, and the UK's is London. Yeah, Paris seems right for France. I don't recall any other city being the capital of France. Maybe I should double-check, but I'm pretty confident it's Paris.<br><br><br>content: Paris is the capital of France</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ebnf_grammar = \"\"\"\n",
    "root ::= city | description\n",
    "city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\n",
    "description ::= city \" is \" status\n",
    "status ::= \"the capital of \" country\n",
    "country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful geography bot.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me the information of the capital of France.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"ebnf\": ebnf_grammar},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:31:10.216017Z",
     "iopub.status.busy": "2025-04-19T20:31:10.215793Z",
     "iopub.status.idle": "2025-04-19T20:31:15.725281Z",
     "shell.execute_reply": "2025-04-19T20:31:15.724877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:10 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:10 TP0] Decode batch. #running-req: 1, #token: 38, token usage: 0.00, gen throughput (token/s): 64.97, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:11 TP0] Decode batch. #running-req: 1, #token: 78, token usage: 0.00, gen throughput (token/s): 67.71, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:11 TP0] Decode batch. #running-req: 1, #token: 118, token usage: 0.01, gen throughput (token/s): 65.06, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:12 TP0] Decode batch. #running-req: 1, #token: 158, token usage: 0.01, gen throughput (token/s): 64.64, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:13 TP0] Decode batch. #running-req: 1, #token: 198, token usage: 0.01, gen throughput (token/s): 64.46, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:13 TP0] Decode batch. #running-req: 1, #token: 238, token usage: 0.01, gen throughput (token/s): 64.66, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:14 TP0] Decode batch. #running-req: 1, #token: 278, token usage: 0.01, gen throughput (token/s): 64.74, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:14 TP0] Decode batch. #running-req: 1, #token: 318, token usage: 0.02, gen throughput (token/s): 64.57, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:15 TP0] Decode batch. #running-req: 1, #token: 358, token usage: 0.02, gen throughput (token/s): 64.38, #queue-req: 0, \n",
      "[2025-04-19 20:31:15] INFO:     127.0.0.1:51164 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out the capital of France. Hmm, I remember learning a bit about France in school, but I'm not 100% sure. Let me think. I know that Paris is a major city in France, and it's often referred to as the \"City of Light.\" People go there for museums, landmarks like the Eiffel Tower, and it's a cultural hub. But is it the capital?<br><br>Wait, I think the capital is the official seat of government, right? So maybe Paris is both the capital and the most famous city. But I'm not entirely certain. I recall that some countries have their capital in a different city than their main tourist attraction. For example, I think Brazil's capital is not Rio de Janeiro, which is more famous. So maybe France is like that too.<br><br>Let me try to remember any specific information. I think the French government declares Paris as the capital. Yeah, that sounds right. I also remember that the Eiffel Tower is in Paris, and it's a symbol of the country. So if Paris is the capital, then that makes sense. But I'm a bit confused because sometimes people say \"the capital of France is Paris,\" but I also think about other capitals I know, like London for the UK or Berlin for Germany. So maybe it's the same for France.<br><br>I should also consider if there are any other capitals in France. I don't think so. France has only one capital city, which is Paris. So, putting it all together, I'm pretty confident that Paris is the capital of France. It's the main government building area, and it's the most well-known city in the country. Yeah, I think that's correct.<br><br><br>content: Paris</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"regex\": \"(Paris|London)\"},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:31:15.727048Z",
     "iopub.status.busy": "2025-04-19T20:31:15.726837Z",
     "iopub.status.idle": "2025-04-19T20:31:20.613872Z",
     "shell.execute_reply": "2025-04-19T20:31:20.613456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:16 TP0] Prefill batch. #new-seq: 1, #new-token: 471, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:16 TP0] Decode batch. #running-req: 1, #token: 500, token usage: 0.02, gen throughput (token/s): 49.18, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:16 TP0] Decode batch. #running-req: 1, #token: 540, token usage: 0.03, gen throughput (token/s): 100.13, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:17 TP0] Decode batch. #running-req: 1, #token: 580, token usage: 0.03, gen throughput (token/s): 100.05, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:17 TP0] Decode batch. #running-req: 1, #token: 620, token usage: 0.03, gen throughput (token/s): 86.27, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:18 TP0] Decode batch. #running-req: 1, #token: 660, token usage: 0.03, gen throughput (token/s): 84.33, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:18 TP0] Decode batch. #running-req: 1, #token: 700, token usage: 0.03, gen throughput (token/s): 64.56, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:19 TP0] Decode batch. #running-req: 1, #token: 740, token usage: 0.04, gen throughput (token/s): 64.09, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:19 TP0] Decode batch. #running-req: 1, #token: 780, token usage: 0.04, gen throughput (token/s): 64.70, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:20 TP0] Decode batch. #running-req: 1, #token: 820, token usage: 0.04, gen throughput (token/s): 76.82, #queue-req: 0, \n",
      "[2025-04-19 20:31:20] INFO:     127.0.0.1:51164 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, the user is in New York and wants the current date and time along with the weather. I need to figure out how to provide this using the available functions.<br><br>First, I should determine which function to use. The user needs both the date and time, which is handled by 'get_current_date'. They also need the weather, so I'll use 'get_current_weather' for that.<br><br>For 'get_current_date', the parameter is 'timezone', so I'll set it to 'America/New_York'. That way, it returns the local date and time.<br><br>Next, for the weather, I'll need the city and state. The city is 'New York', and the state is 'NY'. The unit isn't specified, so I'll default to Celsius.<br><br>I should structure the response by calling each function separately. Each function call will be on its own line with the appropriate parameters. I'll include a brief description with each function call to make it clear what each part is for.<br><br>I also need to make sure the format is correct: start with <function=, then the parameters as a JSON object, and end with </function>. Each function call will be a separate message, so I'll send them one after the other.<br><br>Finally, I'll remind the user to use the provided functions and to add sources if they use search results. But in this case, I'm using the functions directly, so maybe that's not necessary here.<br><br><br>content: <function=get_current_date>{\"timezone\": \"America/New_York\"}</function>  <br><function=get_current_weather>{\"city\": \"New York\", \"state\": \"NY\", \"unit\": \"celsius\"}</function>  <br><br>This will provide you with the current date and time in New York and the current weather in Celsius.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_get_current_weather = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\",\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tool_get_current_date = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"Get the current date and time for a given timezone\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"timezone\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The timezone to fetch the current date and time for, e.g. 'America/New_York'\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"timezone\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "schema_get_current_weather = tool_get_current_weather[\"function\"][\"parameters\"]\n",
    "schema_get_current_date = tool_get_current_date[\"function\"][\"parameters\"]\n",
    "\n",
    "\n",
    "def get_messages():\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "# Tool Instructions\n",
    "- Always execute python code in messages that you share.\n",
    "- When looking for real time information use relevant functions if available else fallback to brave_search\n",
    "You have access to the following functions:\n",
    "Use the function 'get_current_weather' to: Get the current weather in a given location\n",
    "{tool_get_current_weather[\"function\"]}\n",
    "Use the function 'get_current_date' to: Get the current date and time for a given timezone\n",
    "{tool_get_current_date[\"function\"]}\n",
    "If a you choose to call a function ONLY reply in the following format:\n",
    "<{{start_tag}}={{function_name}}>{{parameters}}{{end_tag}}\n",
    "where\n",
    "start_tag => `<function`\n",
    "parameters => a JSON dict with the function argument name as key and function argument value as value.\n",
    "end_tag => `</function>`\n",
    "Here is an example,\n",
    "<function=example_function_name>{{\"example_name\": \"example_value\"}}</function>\n",
    "Reminder:\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "- Always add your sources when using search results to answer the user query\n",
    "You are a helpful assistant.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are in New York. Please get the current date and time, and the weather.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "messages = get_messages()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=messages,\n",
    "    response_format={\n",
    "        \"type\": \"structural_tag\",\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structures\": [\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_weather>\",\n",
    "                \"schema\": schema_get_current_weather,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_date>\",\n",
    "                \"schema\": schema_get_current_date,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "        ],\n",
    "        \"triggers\": [\"<function=\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native API and SGLang Runtime (SRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:31:20.615638Z",
     "iopub.status.busy": "2025-04-19T20:31:20.615416Z",
     "iopub.status.idle": "2025-04-19T20:31:41.993313Z",
     "shell.execute_reply": "2025-04-19T20:31:41.992892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:21 TP0] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:21 TP0] Decode batch. #running-req: 1, #token: 45, token usage: 0.00, gen throughput (token/s): 41.41, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:21 TP0] Decode batch. #running-req: 1, #token: 85, token usage: 0.00, gen throughput (token/s): 102.83, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:22 TP0] Decode batch. #running-req: 1, #token: 125, token usage: 0.01, gen throughput (token/s): 105.83, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:22 TP0] Decode batch. #running-req: 1, #token: 165, token usage: 0.01, gen throughput (token/s): 100.42, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:22 TP0] Decode batch. #running-req: 1, #token: 205, token usage: 0.01, gen throughput (token/s): 104.48, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:23 TP0] Decode batch. #running-req: 1, #token: 245, token usage: 0.01, gen throughput (token/s): 99.97, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:23 TP0] Decode batch. #running-req: 1, #token: 285, token usage: 0.01, gen throughput (token/s): 103.67, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:24 TP0] Decode batch. #running-req: 1, #token: 325, token usage: 0.02, gen throughput (token/s): 104.49, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:24 TP0] Decode batch. #running-req: 1, #token: 365, token usage: 0.02, gen throughput (token/s): 104.41, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:24 TP0] Decode batch. #running-req: 1, #token: 405, token usage: 0.02, gen throughput (token/s): 105.23, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:25 TP0] Decode batch. #running-req: 1, #token: 445, token usage: 0.02, gen throughput (token/s): 104.48, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:25 TP0] Decode batch. #running-req: 1, #token: 485, token usage: 0.02, gen throughput (token/s): 104.11, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:26 TP0] Decode batch. #running-req: 1, #token: 525, token usage: 0.03, gen throughput (token/s): 101.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:26 TP0] Decode batch. #running-req: 1, #token: 565, token usage: 0.03, gen throughput (token/s): 104.53, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:26 TP0] Decode batch. #running-req: 1, #token: 605, token usage: 0.03, gen throughput (token/s): 101.46, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:27 TP0] Decode batch. #running-req: 1, #token: 645, token usage: 0.03, gen throughput (token/s): 101.70, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:27 TP0] Decode batch. #running-req: 1, #token: 685, token usage: 0.03, gen throughput (token/s): 99.02, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:28 TP0] Decode batch. #running-req: 1, #token: 725, token usage: 0.04, gen throughput (token/s): 102.91, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:28 TP0] Decode batch. #running-req: 1, #token: 765, token usage: 0.04, gen throughput (token/s): 98.80, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:28 TP0] Decode batch. #running-req: 1, #token: 805, token usage: 0.04, gen throughput (token/s): 101.21, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:29 TP0] Decode batch. #running-req: 1, #token: 845, token usage: 0.04, gen throughput (token/s): 103.79, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:29 TP0] Decode batch. #running-req: 1, #token: 885, token usage: 0.04, gen throughput (token/s): 98.22, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:30 TP0] Decode batch. #running-req: 1, #token: 925, token usage: 0.05, gen throughput (token/s): 95.93, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:30 TP0] Decode batch. #running-req: 1, #token: 965, token usage: 0.05, gen throughput (token/s): 96.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:30 TP0] Decode batch. #running-req: 1, #token: 1005, token usage: 0.05, gen throughput (token/s): 99.72, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:31 TP0] Decode batch. #running-req: 1, #token: 1045, token usage: 0.05, gen throughput (token/s): 102.94, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:31 TP0] Decode batch. #running-req: 1, #token: 1085, token usage: 0.05, gen throughput (token/s): 99.66, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:32 TP0] Decode batch. #running-req: 1, #token: 1125, token usage: 0.05, gen throughput (token/s): 96.95, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:32 TP0] Decode batch. #running-req: 1, #token: 1165, token usage: 0.06, gen throughput (token/s): 96.02, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:32 TP0] Decode batch. #running-req: 1, #token: 1205, token usage: 0.06, gen throughput (token/s): 95.98, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:33 TP0] Decode batch. #running-req: 1, #token: 1245, token usage: 0.06, gen throughput (token/s): 101.80, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:33 TP0] Decode batch. #running-req: 1, #token: 1285, token usage: 0.06, gen throughput (token/s): 96.12, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:34 TP0] Decode batch. #running-req: 1, #token: 1325, token usage: 0.06, gen throughput (token/s): 99.14, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:34 TP0] Decode batch. #running-req: 1, #token: 1365, token usage: 0.07, gen throughput (token/s): 101.76, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:34 TP0] Decode batch. #running-req: 1, #token: 1405, token usage: 0.07, gen throughput (token/s): 100.10, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:35 TP0] Decode batch. #running-req: 1, #token: 1445, token usage: 0.07, gen throughput (token/s): 100.09, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:35 TP0] Decode batch. #running-req: 1, #token: 1485, token usage: 0.07, gen throughput (token/s): 94.62, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:36 TP0] Decode batch. #running-req: 1, #token: 1525, token usage: 0.07, gen throughput (token/s): 99.73, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:36 TP0] Decode batch. #running-req: 1, #token: 1565, token usage: 0.08, gen throughput (token/s): 99.11, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:36 TP0] Decode batch. #running-req: 1, #token: 1605, token usage: 0.08, gen throughput (token/s): 96.76, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:37 TP0] Decode batch. #running-req: 1, #token: 1645, token usage: 0.08, gen throughput (token/s): 92.00, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:37 TP0] Decode batch. #running-req: 1, #token: 1685, token usage: 0.08, gen throughput (token/s): 89.54, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:38 TP0] Decode batch. #running-req: 1, #token: 1725, token usage: 0.08, gen throughput (token/s): 90.30, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:38 TP0] Decode batch. #running-req: 1, #token: 1765, token usage: 0.09, gen throughput (token/s): 87.30, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:39 TP0] Decode batch. #running-req: 1, #token: 1805, token usage: 0.09, gen throughput (token/s): 93.87, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:39 TP0] Decode batch. #running-req: 1, #token: 1845, token usage: 0.09, gen throughput (token/s): 93.05, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:40 TP0] Decode batch. #running-req: 1, #token: 1885, token usage: 0.09, gen throughput (token/s): 89.99, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:40 TP0] Decode batch. #running-req: 1, #token: 1925, token usage: 0.09, gen throughput (token/s): 94.72, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:40 TP0] Decode batch. #running-req: 1, #token: 1965, token usage: 0.10, gen throughput (token/s): 95.41, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:41 TP0] Decode batch. #running-req: 1, #token: 2005, token usage: 0.10, gen throughput (token/s): 90.28, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:41 TP0] Decode batch. #running-req: 1, #token: 2045, token usage: 0.10, gen throughput (token/s): 94.58, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:41] INFO:     127.0.0.1:39804 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "{'text': 'Okay, so I need to provide the information about the capital of France in JSON format. Hmm, I\\'m not entirely sure about all the details, but I\\'ll try to think it through.\\n\\nFirst, I know that the capital of France is Paris. That\\'s pretty much a given, right? But I should double-check that. Maybe I can recall any other capitals I know. London is the capital of the UK, Rome is Italy, and maybe Tokyo is Japan\\'s. Yeah, Paris seems correct for France.\\n\\nNow, moving on to the population. I think Paris is a very large city, but I\\'m not sure of the exact number. I remember it\\'s over 3 million, but I\\'m not certain. Maybe around 3.5 million? I should probably look that up, but since I can\\'t right now, I\\'ll go with 3,500,000 as an estimate.\\n\\nNext, the area. Paris is a big city, but I think it\\'s not as large as Tokyo or London. Maybe around 10 square kilometers? I\\'m not sure, but that seems plausible. I\\'ll note that as 10,000,000 square meters.\\n\\nCoordinates are next. Paris is in France, so the country code is \"FR\". The latitude and longitude... I think the approximate coordinates are around 48.8566° N latitude and 2.3522° E longitude. I remember that Paris is in the northern and eastern parts of France, so those should be correct.\\n\\nOfficial languages. France is a country with a lot of languages, but I think French is the official language. I\\'m not sure if they have others, but French is definitely the primary one. Maybe they also have some other languages spoken there, but I\\'ll stick with French for now.\\n\\nOfficial currency is the euro, right? Yeah, I\\'m pretty sure that\\'s correct. They use the euro as their main currency.\\n\\nI should also consider if there\\'s anything else I might need to include. Maybe the capital\\'s nickname? I think Paris is called the \"City of Light\" or something like that. But the user didn\\'t ask for that, so maybe it\\'s not necessary.\\n\\nPutting it all together, I\\'ll structure the JSON with the key-value pairs. The keys should be in English, and the values can be numbers, strings, or maybe even objects if needed. Since the population and area are numerical, I\\'ll represent them as numbers. The rest can be strings.\\n\\nWait, but in JSON, numbers don\\'t have commas, right? So 3,500,000 should be written as 3500000 without the comma. Same with the area, 10,000,000 square meters becomes 10000000.\\n\\nLet me make sure I\\'m formatting the JSON correctly. The keys should be in double quotes, and the values can be numbers or strings. So the structure would be something like:\\n\\n{\\n  \"capital\": \"Paris\",\\n  \"population\": 3500000,\\n  \"area\": 10000000,\\n  \"country\": \"FR\",\\n  \"coordinates\": {\\n    \"latitude\": 48.8566,\\n    \"longitude\": 2.3522\\n  },\\n  \"languages\": \"French\",\\n  \"currency\": \"Euro\"\\n}\\n\\nWait, but the coordinates are just two numbers, so maybe I don\\'t need a nested object. So it would be:\\n\\n{\\n  \"capital\": \"Paris\",\\n  \"population\": 3500000,\\n  \"area\": 10000000,\\n  \"country\": \"FR\",\\n  \"coordinates\": {\\n    \"latitude\": 48.8566,\\n    \"longitude\": 2.3522\\n  },\\n  \"languages\": \"French\",\\n  \"currency\": \"Euro\"\\n}\\n\\nThat looks better. I think that\\'s all the information I need. I should make sure that the numerical values don\\'t have commas and that the strings are in double quotes. Also, the keys should be in lowercase letters as per JSON standards.\\n\\nI think I\\'ve covered everything. Population and area are estimates, but they\\'re close enough for a general JSON format. I don\\'t think I need to include more details unless specified. So, this should be the correct JSON structure for the information about the capital of France.\\n</think>{\\n\\n\"name\": \"Paris\",\\n\"population\": 3500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000', 'meta_info': {'id': '89e30979a8e946df9b876773078b5c2c', 'finish_reason': {'type': 'length', 'length': 2048}, 'prompt_tokens': 20, 'completion_tokens': 2048, 'cached_tokens': 1, 'e2e_latency': 20.807673454284668}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to provide the information about the capital of France in JSON format. Hmm, I'm not entirely sure about all the details, but I'll try to think it through.<br><br>First, I know that the capital of France is Paris. That's pretty much a given, right? But I should double-check that. Maybe I can recall any other capitals I know. London is the capital of the UK, Rome is Italy, and maybe Tokyo is Japan's. Yeah, Paris seems correct for France.<br><br>Now, moving on to the population. I think Paris is a very large city, but I'm not sure of the exact number. I remember it's over 3 million, but I'm not certain. Maybe around 3.5 million? I should probably look that up, but since I can't right now, I'll go with 3,500,000 as an estimate.<br><br>Next, the area. Paris is a big city, but I think it's not as large as Tokyo or London. Maybe around 10 square kilometers? I'm not sure, but that seems plausible. I'll note that as 10,000,000 square meters.<br><br>Coordinates are next. Paris is in France, so the country code is \"FR\". The latitude and longitude... I think the approximate coordinates are around 48.8566° N latitude and 2.3522° E longitude. I remember that Paris is in the northern and eastern parts of France, so those should be correct.<br><br>Official languages. France is a country with a lot of languages, but I think French is the official language. I'm not sure if they have others, but French is definitely the primary one. Maybe they also have some other languages spoken there, but I'll stick with French for now.<br><br>Official currency is the euro, right? Yeah, I'm pretty sure that's correct. They use the euro as their main currency.<br><br>I should also consider if there's anything else I might need to include. Maybe the capital's nickname? I think Paris is called the \"City of Light\" or something like that. But the user didn't ask for that, so maybe it's not necessary.<br><br>Putting it all together, I'll structure the JSON with the key-value pairs. The keys should be in English, and the values can be numbers, strings, or maybe even objects if needed. Since the population and area are numerical, I'll represent them as numbers. The rest can be strings.<br><br>Wait, but in JSON, numbers don't have commas, right? So 3,500,000 should be written as 3500000 without the comma. Same with the area, 10,000,000 square meters becomes 10000000.<br><br>Let me make sure I'm formatting the JSON correctly. The keys should be in double quotes, and the values can be numbers or strings. So the structure would be something like:<br><br>{<br>  \"capital\": \"Paris\",<br>  \"population\": 3500000,<br>  \"area\": 10000000,<br>  \"country\": \"FR\",<br>  \"coordinates\": {<br>    \"latitude\": 48.8566,<br>    \"longitude\": 2.3522<br>  },<br>  \"languages\": \"French\",<br>  \"currency\": \"Euro\"<br>}<br><br>Wait, but the coordinates are just two numbers, so maybe I don't need a nested object. So it would be:<br><br>{<br>  \"capital\": \"Paris\",<br>  \"population\": 3500000,<br>  \"area\": 10000000,<br>  \"country\": \"FR\",<br>  \"coordinates\": {<br>    \"latitude\": 48.8566,<br>    \"longitude\": 2.3522<br>  },<br>  \"languages\": \"French\",<br>  \"currency\": \"Euro\"<br>}<br><br>That looks better. I think that's all the information I need. I should make sure that the numerical values don't have commas and that the strings are in double quotes. Also, the keys should be in lowercase letters as per JSON standards.<br><br>I think I've covered everything. Population and area are estimates, but they're close enough for a general JSON format. I don't think I need to include more details unless specified. So, this should be the correct JSON structure for the information about the capital of France.<br><br><br>content: {<br><br>\"name\": \"Paris\",<br>\"population\": 3500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here is the information of the capital of France in the JSON format.\\n\",\n",
    "    }\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# Make API request\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())\n",
    "\n",
    "\n",
    "reasoing_content = response.json()[\"text\"].split(\"</think>\")[0]\n",
    "content = response.json()[\"text\"].split(\"</think>\")[1]\n",
    "print_highlight(f\"reasoing_content: {reasoing_content}\\n\\ncontent: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:31:42.001773Z",
     "iopub.status.busy": "2025-04-19T20:31:42.001544Z",
     "iopub.status.idle": "2025-04-19T20:31:49.960938Z",
     "shell.execute_reply": "2025-04-19T20:31:49.960524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:42 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:42 TP0] Decode batch. #running-req: 1, #token: 22, token usage: 0.00, gen throughput (token/s): 85.06, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:42 TP0] Decode batch. #running-req: 1, #token: 62, token usage: 0.00, gen throughput (token/s): 88.43, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:43 TP0] Decode batch. #running-req: 1, #token: 102, token usage: 0.00, gen throughput (token/s): 96.47, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:43 TP0] Decode batch. #running-req: 1, #token: 142, token usage: 0.01, gen throughput (token/s): 102.22, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:43 TP0] Decode batch. #running-req: 1, #token: 182, token usage: 0.01, gen throughput (token/s): 103.08, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:44 TP0] Decode batch. #running-req: 1, #token: 222, token usage: 0.01, gen throughput (token/s): 105.56, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:44 TP0] Decode batch. #running-req: 1, #token: 262, token usage: 0.01, gen throughput (token/s): 98.26, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:45 TP0] Decode batch. #running-req: 1, #token: 302, token usage: 0.01, gen throughput (token/s): 98.47, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:45 TP0] Decode batch. #running-req: 1, #token: 342, token usage: 0.02, gen throughput (token/s): 100.25, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:45 TP0] Decode batch. #running-req: 1, #token: 382, token usage: 0.02, gen throughput (token/s): 100.57, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:46 TP0] Decode batch. #running-req: 1, #token: 422, token usage: 0.02, gen throughput (token/s): 105.26, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:46 TP0] Decode batch. #running-req: 1, #token: 462, token usage: 0.02, gen throughput (token/s): 99.76, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:47 TP0] Decode batch. #running-req: 1, #token: 502, token usage: 0.02, gen throughput (token/s): 104.62, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:47 TP0] Decode batch. #running-req: 1, #token: 542, token usage: 0.03, gen throughput (token/s): 99.93, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:47 TP0] Decode batch. #running-req: 1, #token: 582, token usage: 0.03, gen throughput (token/s): 99.44, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:48 TP0] Decode batch. #running-req: 1, #token: 622, token usage: 0.03, gen throughput (token/s): 99.67, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:48 TP0] Decode batch. #running-req: 1, #token: 662, token usage: 0.03, gen throughput (token/s): 99.45, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:49 TP0] Decode batch. #running-req: 1, #token: 702, token usage: 0.03, gen throughput (token/s): 97.31, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:49 TP0] Decode batch. #running-req: 1, #token: 742, token usage: 0.04, gen throughput (token/s): 101.55, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:49 TP0] Decode batch. #running-req: 1, #token: 782, token usage: 0.04, gen throughput (token/s): 95.48, #queue-req: 0, \n",
      "[2025-04-19 20:31:49] INFO:     127.0.0.1:37804 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Okay, so I need to figure out how to solve this problem. Hmm, the problem is about finding the derivative of a function, right? Let me see. The function is f(x) = 3x^2 + 2x - 5. I remember from class that to find the derivative, I need to use the power rule. \\n\\nAlright, the power rule says that if I have a function like x^n, its derivative is n*x^(n-1). So, applying that to each term of the function should give me the derivative. Let\\'s break it down term by term.\\n\\nFirst term: 3x^2. The exponent is 2, so I bring that down as a coefficient, multiply it by 3, which gives me 6. Then, I reduce the exponent by 1, so it becomes x^(2-1) = x^1, which is just x. So, the derivative of 3x^2 is 6x.\\n\\nSecond term: 2x. The exponent here is 1 because x is the same as x^1. Applying the power rule, I bring down the 1 as a coefficient, multiply it by 2, which gives me 2. Then, reduce the exponent by 1, so it becomes x^(1-1) = x^0, which is 1. So, the derivative of 2x is 2.\\n\\nThird term: -5. This is a constant term. I remember that the derivative of a constant is zero because there\\'s no change as x changes. So, the derivative of -5 is 0.\\n\\nNow, putting it all together, the derivative of f(x) should be 6x + 2. Wait, let me double-check. The first term\\'s derivative is 6x, the second is 2, and the third is 0. So, yes, adding them up gives 6x + 2.\\n\\nI think that\\'s it. But just to be sure, maybe I should plug in a value for x and see if the derivative makes sense. Let\\'s say x = 1. The original function f(1) = 3(1)^2 + 2(1) - 5 = 3 + 2 - 5 = 0. The derivative at x=1 is 6(1) + 2 = 8. So, at the point (1,0), the slope of the tangent line should be 8. That seems reasonable.\\n\\nAnother check: x = 0. f(0) = 3(0)^2 + 2(0) -5 = -5. The derivative at x=0 is 6(0) + 2 = 2. So, the slope at (0,-5) is 2. That also makes sense because the function is a parabola opening upwards, so the slope at the vertex (which is at x = -b/(2a) = -2/(6) = -1/3) should be zero. Wait, but at x=0, it\\'s 2. Hmm, maybe I should calculate the slope at x = -1/3. Let\\'s see, f\\'(-1/3) = 6*(-1/3) + 2 = -2 + 2 = 0. Yes, that\\'s correct. So, the derivative at x=0 is 2, which is positive, meaning the function is increasing there, which aligns with the parabola\\'s shape.\\n\\nI think I\\'m confident now. The derivative is 6x + 2.\\n</think>{\\n\\n\"name\": \"StepbyStepSolution\",\\n\"population\": 1000000000\\n\\n\\n  \\t\\n\\t\\t\\t\\n \\t\\t}', 'meta_info': {'id': '6fea2c773d254b6db64fa10ea46df625', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 5, 'completion_tokens': 790, 'cached_tokens': 2, 'e2e_latency': 7.948591470718384}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# JSON\n",
    "text = tokenizer.apply_chat_template(text, tokenize=False, add_generation_prompt=True)\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json_schema,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:31:49.962600Z",
     "iopub.status.busy": "2025-04-19T20:31:49.962368Z",
     "iopub.status.idle": "2025-04-19T20:31:52.087810Z",
     "shell.execute_reply": "2025-04-19T20:31:52.087338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:49 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[2025-04-19 20:31:49 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:50 TP0] Decode batch. #running-req: 3, #token: 89, token usage: 0.00, gen throughput (token/s): 211.03, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:50 TP0] Decode batch. #running-req: 3, #token: 209, token usage: 0.01, gen throughput (token/s): 289.56, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:51 TP0] Decode batch. #running-req: 3, #token: 329, token usage: 0.02, gen throughput (token/s): 289.75, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:51 TP0] Decode batch. #running-req: 3, #token: 449, token usage: 0.02, gen throughput (token/s): 289.83, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:51 TP0] Decode batch. #running-req: 3, #token: 569, token usage: 0.03, gen throughput (token/s): 289.21, #queue-req: 0, \n",
      "[2025-04-19 20:31:52] INFO:     127.0.0.1:59906 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[{'text': \"\\nThe capital of France is Paris.\\n\\nThat's all the information I have.\\n\\nOkay, so I need to figure out the capital of France. I know that Paris is the capital, but I'm not entirely sure. Let me think about why I think that. I've heard it mentioned a lot, especially in movies and TV shows. People often go there for business or tourism. Also, I remember learning in school that Paris is a major city in France, known for landmarks like the Eiffel Tower and the Louvre Museum. Those places are famous worldwide, which makes me think that Paris is indeed the capital. Maybe I can cross-check this with some other sources or my notes. Wait, I don't have any other information right now, but based on what I know, Paris is the capital of France. I don't recall any other major city in France being referred to as the capital. So, I'm pretty confident that Paris is correct.\\n</think>Paris is the capital of France\", 'meta_info': {'id': '030c25d404664a32be9465110e426912', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'completion_tokens': 201, 'cached_tokens': 10, 'e2e_latency': 2.1180758476257324}}, {'text': \"\\nThe capital of France is Paris.\\n\\nThat's all the information I have.\\n\\nOkay, so I need to figure out the capital of France. I know that Paris is the capital, but I'm not entirely sure. Let me think about why I think that. I've heard it mentioned a lot, especially in movies and TV shows. People often go there for business or tourism. Also, I remember learning in school that Paris is a major city in France, known for landmarks like the Eiffel Tower and the Louvre Museum. Those places are famous worldwide, which makes me think that Paris is indeed the capital. Maybe I can cross-check this with some other sources or my notes. Wait, I don't have any other information right now, but based on what I know, Paris is the capital of France. I don't recall any other major city in France being referred to as the capital. So, I'm pretty confident that Paris is correct.\\n</think>Paris is the capital of France\", 'meta_info': {'id': '8e0a870929374fdbbb20a85f789c0482', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'completion_tokens': 201, 'cached_tokens': 10, 'e2e_latency': 2.1180806159973145}}, {'text': \"\\nThe capital of France is Paris.\\n\\nThat's all the information I have.\\n\\nOkay, so I need to figure out the capital of France. I know that Paris is the capital, but I'm not entirely sure. Let me think about why I think that. I've heard it mentioned a lot, especially in movies and TV shows. People often go there for business or tourism. Also, I remember learning in school that Paris is a major city in France, known for landmarks like the Eiffel Tower and the Louvre Museum. Those places are famous worldwide, which makes me think that Paris is indeed the capital. Maybe I can cross-check this with some other sources or my notes. Wait, I don't have any other information right now, but based on what I know, Paris is the capital of France. I don't recall any other major city in France being referred to as the capital. So, I'm pretty confident that Paris is correct.\\n</think>Paris is the capital of France\", 'meta_info': {'id': '5a8a8ed7acc94d67af7b564d432f2586', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'completion_tokens': 201, 'cached_tokens': 10, 'e2e_latency': 2.1180832386016846}}]\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Give me the information of the capital of France.\",\n",
    "        \"sampling_params\": {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"temperature\": 0,\n",
    "            \"n\": 3,\n",
    "            \"ebnf\": (\n",
    "                \"root ::= city | description\\n\"\n",
    "                'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "                'description ::= city \" is \" status\\n'\n",
    "                'status ::= \"the capital of \" country\\n'\n",
    "                'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "            ),\n",
    "        },\n",
    "        \"stream\": False,\n",
    "        \"return_logprob\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:31:52.089477Z",
     "iopub.status.busy": "2025-04-19T20:31:52.089259Z",
     "iopub.status.idle": "2025-04-19T20:32:12.782814Z",
     "shell.execute_reply": "2025-04-19T20:32:12.782355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:52 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:52 TP0] Decode batch. #running-req: 1, #token: 31, token usage: 0.00, gen throughput (token/s): 163.85, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:52 TP0] Decode batch. #running-req: 1, #token: 71, token usage: 0.00, gen throughput (token/s): 102.78, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:53 TP0] Decode batch. #running-req: 1, #token: 111, token usage: 0.01, gen throughput (token/s): 103.69, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:53 TP0] Decode batch. #running-req: 1, #token: 151, token usage: 0.01, gen throughput (token/s): 98.41, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:53 TP0] Decode batch. #running-req: 1, #token: 191, token usage: 0.01, gen throughput (token/s): 102.68, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:54 TP0] Decode batch. #running-req: 1, #token: 231, token usage: 0.01, gen throughput (token/s): 100.18, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:54 TP0] Decode batch. #running-req: 1, #token: 271, token usage: 0.01, gen throughput (token/s): 76.21, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:55 TP0] Decode batch. #running-req: 1, #token: 311, token usage: 0.02, gen throughput (token/s): 74.95, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:55 TP0] Decode batch. #running-req: 1, #token: 351, token usage: 0.02, gen throughput (token/s): 89.27, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:56 TP0] Decode batch. #running-req: 1, #token: 391, token usage: 0.02, gen throughput (token/s): 101.92, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:56 TP0] Decode batch. #running-req: 1, #token: 431, token usage: 0.02, gen throughput (token/s): 100.41, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:57 TP0] Decode batch. #running-req: 1, #token: 471, token usage: 0.02, gen throughput (token/s): 103.45, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:57 TP0] Decode batch. #running-req: 1, #token: 511, token usage: 0.02, gen throughput (token/s): 99.36, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:57 TP0] Decode batch. #running-req: 1, #token: 551, token usage: 0.03, gen throughput (token/s): 100.25, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:58 TP0] Decode batch. #running-req: 1, #token: 591, token usage: 0.03, gen throughput (token/s): 100.42, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:58 TP0] Decode batch. #running-req: 1, #token: 631, token usage: 0.03, gen throughput (token/s): 99.99, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:59 TP0] Decode batch. #running-req: 1, #token: 671, token usage: 0.03, gen throughput (token/s): 102.53, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:59 TP0] Decode batch. #running-req: 1, #token: 711, token usage: 0.03, gen throughput (token/s): 99.85, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:31:59 TP0] Decode batch. #running-req: 1, #token: 751, token usage: 0.04, gen throughput (token/s): 98.00, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:00 TP0] Decode batch. #running-req: 1, #token: 791, token usage: 0.04, gen throughput (token/s): 102.93, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:00 TP0] Decode batch. #running-req: 1, #token: 831, token usage: 0.04, gen throughput (token/s): 100.29, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:00 TP0] Decode batch. #running-req: 1, #token: 871, token usage: 0.04, gen throughput (token/s): 100.64, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:01 TP0] Decode batch. #running-req: 1, #token: 911, token usage: 0.04, gen throughput (token/s): 100.29, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:01 TP0] Decode batch. #running-req: 1, #token: 951, token usage: 0.05, gen throughput (token/s): 96.46, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:02 TP0] Decode batch. #running-req: 1, #token: 991, token usage: 0.05, gen throughput (token/s): 101.26, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:02 TP0] Decode batch. #running-req: 1, #token: 1031, token usage: 0.05, gen throughput (token/s): 103.95, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:02 TP0] Decode batch. #running-req: 1, #token: 1071, token usage: 0.05, gen throughput (token/s): 101.48, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:03 TP0] Decode batch. #running-req: 1, #token: 1111, token usage: 0.05, gen throughput (token/s): 101.45, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:03 TP0] Decode batch. #running-req: 1, #token: 1151, token usage: 0.06, gen throughput (token/s): 101.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:04 TP0] Decode batch. #running-req: 1, #token: 1191, token usage: 0.06, gen throughput (token/s): 99.10, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:04 TP0] Decode batch. #running-req: 1, #token: 1231, token usage: 0.06, gen throughput (token/s): 100.27, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:04 TP0] Decode batch. #running-req: 1, #token: 1271, token usage: 0.06, gen throughput (token/s): 99.82, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:05 TP0] Decode batch. #running-req: 1, #token: 1311, token usage: 0.06, gen throughput (token/s): 100.35, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:05 TP0] Decode batch. #running-req: 1, #token: 1351, token usage: 0.07, gen throughput (token/s): 100.15, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:06 TP0] Decode batch. #running-req: 1, #token: 1391, token usage: 0.07, gen throughput (token/s): 103.51, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:06 TP0] Decode batch. #running-req: 1, #token: 1431, token usage: 0.07, gen throughput (token/s): 100.73, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:06 TP0] Decode batch. #running-req: 1, #token: 1471, token usage: 0.07, gen throughput (token/s): 98.80, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:07 TP0] Decode batch. #running-req: 1, #token: 1511, token usage: 0.07, gen throughput (token/s): 104.01, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:07 TP0] Decode batch. #running-req: 1, #token: 1551, token usage: 0.08, gen throughput (token/s): 99.11, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:08 TP0] Decode batch. #running-req: 1, #token: 1591, token usage: 0.08, gen throughput (token/s): 103.98, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:08 TP0] Decode batch. #running-req: 1, #token: 1631, token usage: 0.08, gen throughput (token/s): 99.18, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:08 TP0] Decode batch. #running-req: 1, #token: 1671, token usage: 0.08, gen throughput (token/s): 99.76, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:09 TP0] Decode batch. #running-req: 1, #token: 1711, token usage: 0.08, gen throughput (token/s): 102.05, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:09 TP0] Decode batch. #running-req: 1, #token: 1751, token usage: 0.09, gen throughput (token/s): 100.36, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:10 TP0] Decode batch. #running-req: 1, #token: 1791, token usage: 0.09, gen throughput (token/s): 100.44, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:10 TP0] Decode batch. #running-req: 1, #token: 1831, token usage: 0.09, gen throughput (token/s): 97.94, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:10 TP0] Decode batch. #running-req: 1, #token: 1871, token usage: 0.09, gen throughput (token/s): 101.90, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:11 TP0] Decode batch. #running-req: 1, #token: 1911, token usage: 0.09, gen throughput (token/s): 100.23, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:11 TP0] Decode batch. #running-req: 1, #token: 1951, token usage: 0.10, gen throughput (token/s): 100.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:12 TP0] Decode batch. #running-req: 1, #token: 1991, token usage: 0.10, gen throughput (token/s): 95.45, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:12 TP0] Decode batch. #running-req: 1, #token: 2031, token usage: 0.10, gen throughput (token/s): 97.06, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:12] INFO:     127.0.0.1:59914 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "{'text': ' France, and the \\n\\\\( n \\\\)  \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\(', 'meta_info': {'id': '5220f287ced44b62940bb4c70466ada1', 'finish_reason': {'type': 'length', 'length': 2048}, 'prompt_tokens': 6, 'completion_tokens': 2048, 'cached_tokens': 1, 'e2e_latency': 20.687008380889893}}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Paris is the capital of\",\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"regex\": \"(France|England)\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:12.784513Z",
     "iopub.status.busy": "2025-04-19T20:32:12.784292Z",
     "iopub.status.idle": "2025-04-19T20:32:18.124858Z",
     "shell.execute_reply": "2025-04-19T20:32:18.124443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:12 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 19, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[2025-04-19 20:32:12 TP0] Decode batch. #running-req: 1, #token: 37, token usage: 0.00, gen throughput (token/s): 97.46, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:13 TP0] Decode batch. #running-req: 1, #token: 77, token usage: 0.00, gen throughput (token/s): 101.15, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:13 TP0] Decode batch. #running-req: 1, #token: 117, token usage: 0.01, gen throughput (token/s): 99.75, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:14 TP0] Decode batch. #running-req: 1, #token: 157, token usage: 0.01, gen throughput (token/s): 101.68, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:14 TP0] Decode batch. #running-req: 1, #token: 197, token usage: 0.01, gen throughput (token/s): 100.86, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:14 TP0] Decode batch. #running-req: 1, #token: 237, token usage: 0.01, gen throughput (token/s): 101.72, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:15 TP0] Decode batch. #running-req: 1, #token: 277, token usage: 0.01, gen throughput (token/s): 101.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:15 TP0] Decode batch. #running-req: 1, #token: 317, token usage: 0.02, gen throughput (token/s): 101.62, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:16 TP0] Decode batch. #running-req: 1, #token: 357, token usage: 0.02, gen throughput (token/s): 101.33, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:16 TP0] Decode batch. #running-req: 1, #token: 397, token usage: 0.02, gen throughput (token/s): 97.81, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:16 TP0] Decode batch. #running-req: 1, #token: 437, token usage: 0.02, gen throughput (token/s): 101.75, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:17 TP0] Decode batch. #running-req: 1, #token: 477, token usage: 0.02, gen throughput (token/s): 102.01, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:17 TP0] Decode batch. #running-req: 1, #token: 517, token usage: 0.03, gen throughput (token/s): 99.45, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:18 TP0] Decode batch. #running-req: 0, #token: 0, token usage: 0.00, gen throughput (token/s): 101.78, #queue-req: 0, \n",
      "[2025-04-19 20:32:18] INFO:     127.0.0.1:35908 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Alright, the user is asking for the information about the capital of France in JSON format. I need to provide a structured response. Let me think about what details are important.\\n\\nFirst, the obvious one is the capital city itself, which is Paris. The country is naturally France. The official name should be \"The City of Light and Love,\" a well-known tagline. \\n\\nNow, to include the population. I should get the approximate number. As of the latest data, it\\'s around 2,200,000 people. Coordinates are essential too; Paris is located at 48.8566° N latitude and 2.3522° E longitude. \\n\\nThe region it\\'s in is Île-de-France, which is both administrative and a significant region culturally and geographically. Unemployment rate might be relevant— I think it\\'s about 7.96% as of recent stats. GDP is a big topic, so including the approximate value in billion US dollars would be useful. \\n\\nExpenditure on public services is another key metric. France spends around $331 billion annually. The languages spoken there include French as the main, along with English, German, Italian, and Spanish. The official currency is the Euro. \\n\\nLastly, the time zone is Central European Time or CET. \\n\\nI should structure all this into a JSON format, making sure each key is clear and the information is accurate. Keeping the JSON clean and avoiding unnecessary complexity is important. I\\'ll double-check all the numbers to ensure they\\'re up to date and correct.\\n</think>\\n\\nHere is the information about the capital of France in JSON format:\\n\\n```json\\n{\\n  \"capital\": {\\n    \"city\": \"Paris\",\\n    \"country\": \"France\",\\n    \"official_name\": \"The City of Light and Love\",\\n    \"population\": 2200000,\\n    \"coordinates\": {\\n      \"latitude\": 48.8566,\\n      \"longitude\": 2.3522\\n    },\\n    \"region\": \"Île-de-France\",\\n    \"unemployment_rate\": 7.96,\\n    \"gdp\": 627152,\\n    \"public服务Expenditure\": 331,\\n    \"languages\": [\"French\", \"English\", \"German\", \"Italian\", \"Spanish\"],\\n    \"currency\": \"Euro\",\\n    \"timezone\": \"Central European Time (CET)\"\\n  }\\n}\\n```\\n\\nThis JSON structure provides key details about Paris, including its population, coordinates, region, and other relevant information.', 'meta_info': {'id': '914d86c61a2e4749a3f5e594bf939bc2', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 20, 'completion_tokens': 536, 'cached_tokens': 19, 'e2e_latency': 5.331255674362183}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "payload = {\n",
    "    \"text\": text,\n",
    "    \"sampling_params\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structural_tag\": json.dumps(\n",
    "            {\n",
    "                \"type\": \"structural_tag\",\n",
    "                \"structures\": [\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_weather>\",\n",
    "                        \"schema\": schema_get_current_weather,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_date>\",\n",
    "                        \"schema\": schema_get_current_date,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                ],\n",
    "                \"triggers\": [\"<function=\"],\n",
    "            }\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "response = requests.post(f\"http://localhost:{port}/generate\", json=payload)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:18.126445Z",
     "iopub.status.busy": "2025-04-19T20:32:18.126218Z",
     "iopub.status.idle": "2025-04-19T20:32:18.144351Z",
     "shell.execute_reply": "2025-04-19T20:32:18.143894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-19 20:32:18] Child process unexpectedly failed with an exit code 9. pid=2517161\n"
     ]
    }
   ],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:18.146286Z",
     "iopub.status.busy": "2025-04-19T20:32:18.146058Z",
     "iopub.status.idle": "2025-04-19T20:32:33.117097Z",
     "shell.execute_reply": "2025-04-19T20:32:33.116516Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.32s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.22s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sglang as sgl\n",
    "\n",
    "llm = sgl.Engine(\n",
    "    model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    reasoning_parser=\"deepseek-r1\",\n",
    "    grammar_backend=\"xgrammar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:33.119342Z",
     "iopub.status.busy": "2025-04-19T20:32:33.119076Z",
     "iopub.status.idle": "2025-04-19T20:32:43.148631Z",
     "shell.execute_reply": "2025-04-19T20:32:43.148169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of China, Beijing, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Beijing\",\n",
      "  \"capital\": \"Yes\",\n",
      "  \"population\": \"Over 30 million\",\n",
      "  \"founded\": \"1248\",\n",
      "  \"Nickname\": \"The Heaven on Earth\",\n",
      "  \"Location\": \"Northern China\",\n",
      "  \"OfficialLanguages\": [\n",
      "    \"Mandarin Chinese\",\n",
      "    \"Bingyuan Chinese\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\",\n",
      "    \"Yugoslav\",\n",
      "    \"Other\"\n",
      "  ],\n",
      "  \"KeySights\": [\n",
      "    \"The Great Wall\",\n",
      "    \"Tiananmen Square\",\n",
      "    \"Forbidden City\",\n",
      "    \"Beijing Museum\",\n",
      "    \"Yuanmingyuan\"\n",
      "  ],\n",
      "  \"Climate\": \"Temperate\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of France, Paris, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"country\": \"France\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 48.8566,\n",
      "    \"longitude\": 2.3522\n",
      "  },\n",
      "  \"founded\": \"1340\",\n",
      "  \"population\": \"9.7 million\",\n",
      "  \"area\": \"105.5 square kilometers\",\n",
      "  \"features\": {\n",
      "    \"bridges\": \"The Eiffel Tower, Notre-Dame, and the Seine River\",\n",
      "    \"landmarks\": \"The Louvre Museum, Montmartre, and the Champs-Élysées\"\n",
      "  },\n",
      "  \"elevation\": \"2 meters\",\n",
      "  \"time_zone\": \"Central European Time (CET)\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of Ireland in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": \"Dublin\",\n",
      "  \"official_name\": \"Dublin City\",\n",
      "  \"region\": \"Dublin\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 53.3489,\n",
      "    \"longitude\": -6.2009\n",
      "  },\n",
      "  \"founded\": \"1543\",\n",
      "  \"population\": 1,234,567,\n",
      "  \"area\": {\n",
      "    \"total\": 123.45,\n",
      "    \"land\": 112.34,\n",
      "    \"water\": 11.11\n",
      "  },\n",
      "  \"climate\": \" temperate\",\n",
      "  \"key_features\": [\n",
      "    \"City Walls\",\n",
      "    \"Trinity College\",\n",
      "    \"Leaving Certificate\",\n",
      "    \"St. Stephen's Cathedral\",\n",
      "    \"Glynn Bridge\"\n",
      "  ],\n",
      "  \"tourism\": [\n",
      "    \"The GAA\",\n",
      "    \"The National Library of Ireland\",\n",
      "    \"The SSE St. Patrick's Cathedral\",\n",
      "    \"The Phoenix Park\",\n",
      "    \"The Book of Kells\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any adjustments!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:43.150403Z",
     "iopub.status.busy": "2025-04-19T20:32:43.150162Z",
     "iopub.status.idle": "2025-04-19T20:32:46.061324Z",
     "shell.execute_reply": "2025-04-19T20:32:46.060859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of China, Beijing, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Beijing\",\n",
      "  \"capital\": \"Yes\",\n",
      "  \"population\": \"Over 30 million\",\n",
      "  \"founded\": \"1248\",\n",
      "  \"Nickname\": \"The Heaven on Earth\",\n",
      "  \"Location\": \"Northern China\",\n",
      "  \"OfficialLanguages\": [\n",
      "    \"Mandarin Chinese\",\n",
      "    \"Bingyuan Chinese\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\",\n",
      "    \"Yugoslav\",\n",
      "    \"Other\"\n",
      "  ],\n",
      "  \"KeySights\": [\n",
      "    \"The Great Wall\",\n",
      "    \"Forbidden City\",\n",
      "    \"Tiananmen Square\",\n",
      "    \"Beijing Museum\",\n",
      "    \"Yuanmingyuan\"\n",
      "  ],\n",
      "  \"Climate\": \"Temperate\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of France, Paris, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"country\": \"France\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 48.8566,\n",
      "    \"longitude\": 2.3522\n",
      "  },\n",
      "  \"founded\": \"1340\",\n",
      "  \"population\": \"9.7 million\",\n",
      "  \"area\": \"105.5 square kilometers\",\n",
      "  \"WX\": {\n",
      "    \"averageTemperature\": \"12°C\",\n",
      "    \"precipitation\": \"540 mm/year\"\n",
      "  },\n",
      "  \"landmarks\": [\n",
      "    {\n",
      "      \"name\": \"Eiffel Tower\",\n",
      "      \"location\": \"City of Light\",\n",
      "      \"height\": \"330 meters\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Notre-Dame Cathedral\",\n",
      "      \"location\": \"Center of Paris\",\n",
      "      \"height\": \"415 meters\"\n",
      "    }\n",
      "  ],\n",
      "  \"Transport\": {\n",
      "    \"publicTransport\": \"Boulevards, trams, and subways\",\n",
      "    \"airport\": \"Paris International Airport\",\n",
      "    \"railway\": \"Le巴黎-Charles de Gaulle\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of Ireland in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": \"Dublin\",\n",
      "  \"official_name\": \"Dublin City\",\n",
      "  \"region\": \"Dublin\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 53.3489,\n",
      "    \"longitude\": -6.2009\n",
      "  },\n",
      "  \"founded\": \"1241\",\n",
      "  \"population\": 1,234,567,\n",
      "  \"area\": {\n",
      "    \"total\": 123.45,\n",
      "    \"land\": 112.34,\n",
      "    \"water\": 11.11\n",
      "  },\n",
      "  \"climate\": \" temperate\",\n",
      "  \"key_features\": [\n",
      "    \"City Walls\",\n",
      "    \"Trinity College\",\n",
      "    \"Leaving Certificate\",\n",
      "    \"St. Stephen's Cathedral\",\n",
      "    \"Glynn Bridge\"\n",
      "  ],\n",
      "  \"tourism\": [\n",
      "    \"The GAA\",\n",
      "    \"The National Library of Ireland\",\n",
      "    \"The University of Dublin\",\n",
      "    \"The Phoenix Park\",\n",
      "    \"The SSE St. Patrick's Cathedral Quarter\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any adjustments!\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "sampling_params = {\"temperature\": 0, \"max_new_tokens\": 2048, \"json_schema\": json_schema}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:46.063083Z",
     "iopub.status.busy": "2025-04-19T20:32:46.062851Z",
     "iopub.status.idle": "2025-04-19T20:32:47.452340Z",
     "shell.execute_reply": "2025-04-19T20:32:47.451883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of France.\n",
      "Generated text:  a. The capital of France is called \"X.\" b. The capital of France has a population of 6 million. c. The capital of France is located in Europe. d. The capital of France is the most populous city in Europe. I need to determine which of these statements (a-d) are true.\n",
      "\n",
      "To solve this, I should start by recalling what I know about the capital of France. I believe the capital is Paris. Now, let's go through each option one by one.\n",
      "\n",
      "Option a states that the capital is called \"X.\" But I know the official name is Paris, not \"X.\" So,\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Germany.\n",
      "Generated text:  the capital is a city.\n",
      "\n",
      "I need to write down three different questions about the capital of Germany that can be answered with just the provided information. Each question should be phrased in a way that requires the person to infer the information based on the context. The questions should cover different aspects, like geography, culture, and maybe something else.\n",
      "\n",
      "Let me think about this. The capital of Germany is Berlin. So, maybe:\n",
      "\n",
      "1. What is the name of the capital city of Germany? - This is straightforward, but maybe it's too direct.\n",
      "\n",
      "2. In Germany, which city is known as the country's capital? - Again\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Italy.\n",
      "Generated text: \n",
      "The capital of Italy is Rome. It is located in central Italy, near the Italian coast and the city of Milan. Rome is an important city for its historical significance, being the cradle of Western civilization. It is known for its ancient ruins, such as the Colosseum, the Roman Forum, and the Vatican City. The city is also known for its rich history, beautiful museums, and the famous Roman Colosseum. Additionally, Rome is a major transportation hub, with trains, buses, and flights connecting it to many other cities. The city center is very historic, with numerous museums and churches. Rome is\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of France.\",\n",
    "    \"Give me the information of the capital of Germany.\",\n",
    "    \"Give me the information of the capital of Italy.\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"ebnf\": (\n",
    "        \"root ::= city | description\\n\"\n",
    "        'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "        'description ::= city \" is \" status\\n'\n",
    "        'status ::= \"the capital of \" country\\n'\n",
    "        'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "    ),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:47.454050Z",
     "iopub.status.busy": "2025-04-19T20:32:47.453822Z",
     "iopub.status.idle": "2025-04-19T20:32:48.774896Z",
     "shell.execute_reply": "2025-04-19T20:32:48.774445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Please provide information about London as a major global city:\n",
      "Generated text:  \n",
      "- Its location,\n",
      "- Cultural significance,\n",
      "- Major landmarks,\n",
      "- The role of transport in its economy,\n",
      "- How it maintains a balance between its historical and modern aspects.\n",
      "- The number of people living in London.\n",
      "\n",
      "**Note:** The information should be accurate, concise, and presented in English.\n",
      "Okay, so I need to provide information about London as a major global city. Let me start by listing what the user asked for: location, cultural significance, major landmarks, role of transport in its economy, balance between historical and modern aspects, and the population.\n",
      "\n",
      "First, location. I know London is in England, but where exactly?\n",
      "===============================\n",
      "Prompt: Please provide information about Paris as a major global city:\n",
      "Generated text:  its location, economic status, cultural significance, and as a tourist attraction.250-300 words, please.\n",
      "Okay, so I need to write about Paris as a major global city. The user specified four aspects: location, economic status, cultural significance, and as a tourist attraction. They want it between 250-300 words, so I should be concise but informative.\n",
      "\n",
      "First, location. Paris is in France, right in the heart of Europe. It's along the Seine River, which is a major river there. It's one of the oldest capital cities in the world, so that\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Please provide information about London as a major global city:\",\n",
    "    \"Please provide information about Paris as a major global city:\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"regex\": \"(France|England)\"}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:48.776470Z",
     "iopub.status.busy": "2025-04-19T20:32:48.776260Z",
     "iopub.status.idle": "2025-04-19T20:32:56.759736Z",
     "shell.execute_reply": "2025-04-19T20:32:56.759272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: <｜begin▁of▁sentence｜><｜User｜>Here is the information of the capital of France in the JSON format.\n",
      "<｜Assistant｜><think>\n",
      "\n",
      "Generated text: Okay, so I need to figure out the information about the capital of France, which is Paris, and present it in JSON format. I'm not entirely sure how to structure this, but I think I should start by listing all the necessary details. \n",
      "\n",
      "First, I know that Paris is the capital of France, so I'll include that. Then, I remember that Paris is located in Île-de-France, which is a significant region in France. I should note that it's situated in northern France. Next, the geographical coordinates are important—longitude and latitude. I think the approximate longitude is 2°20'East and the latitude is 48°44'North. \n",
      "\n",
      "Population-wise, I'm not exactly sure, but I believe Paris has a large population. Maybe around 2 million? I'm not certain about the exact number, though. I'll put that as an estimate. \n",
      "\n",
      "Cultural landmarks are another key aspect. The Eiffel Tower is iconic, so that's a must. The Louvre Museum is another major attraction. I should include those. \n",
      "\n",
      "The official languages in Paris are French, of course, and since it's a major city, English is also widely spoken, especially among tourists. \n",
      "\n",
      "Transportation-wise, Paris has an extensive public transit system. The地铁 network is well-developed, and they have trams and buses. I should mention that. Public phones are also common in the city, so I'll include that as well.\n",
      "\n",
      "Lastly, Paris is a significant cultural and economic hub, so I'll note that it's a global city. \n",
      "\n",
      "Putting this all together, I'll structure the JSON with these key points. I need to make sure each key is clear and the values are accurate, even if some numbers are estimates. I should double-check the population and coordinates if possible, but for now, I'll proceed with the information I have.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": {\n",
      "    \"name\": \"Paris\",\n",
      "    \"location\": {\n",
      "      \"region\": \"Île-de-France\",\n",
      "      \"country\": \"France\",\n",
      "      \"area\": \"Greater Paris\",\n",
      "      \"region_type\": \"carte blanche\"\n",
      "    },\n",
      "    \"coordinates\": {\n",
      "      \"longitude\": \"2°20'E\",\n",
      "      \"latitude\": \"48°44'N\"\n",
      "    },\n",
      "    \"population\": \"2,159,383 (2020 estimate)\",\n",
      "    \"geography\": {\n",
      "      \"mer Median\": \"Approx. 48.857°N / 2.352°E\",\n",
      "      \"region\": \"NorthFR\"\n",
      "    },\n",
      "    \"Culture\": {\n",
      "      \"landmarks\": [\n",
      "        \"Eiffel Tower\",\n",
      "        \"Louvre Museum\"\n",
      "      ],\n",
      "      \"monuments\": [\"Notre-Dame Cathedral\", \"illedes\" (approximate)]\n",
      "    },\n",
      "    \"Official_Languages\": [\"French\", \"English (esp.)\"],\n",
      "    \"Transportation\": {\n",
      "      \"public_transport\": [\"Médiatisés\", \"Tiltmáipur\", \"Bussées\"],\n",
      "      \"phones\": [\"+33 01 42 85 20 20\"]\n",
      "    },\n",
      "    \"Economy\": {\n",
      "      \"sectors\": [\n",
      "        \"Finance\",\n",
      "        \"Tourism\",\n",
      "        \"Technology\",\n",
      "        \"Manufacturing\"\n",
      "      ],\n",
      "      \"status\": \"Global city\"\n",
      "    },\n",
      "    \"Demographics\": {\n",
      "      \"average_age\": \"42 years\",\n",
      "      \"income_level\": \"High\",\n",
      "      \"diversity\": \"High\"\n",
      "    },\n",
      "    \"other\": {\n",
      "      \"area\": \"10.51 square kilometers\",\n",
      "      \"year_founded\": \"1340\",\n",
      "      \"founding\": \"Joachim II of Flandre\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "prompts = [text]\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"structural_tag\": json.dumps(\n",
    "        {\n",
    "            \"type\": \"structural_tag\",\n",
    "            \"structures\": [\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_weather>\",\n",
    "                    \"schema\": schema_get_current_weather,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_date>\",\n",
    "                    \"schema\": schema_get_current_date,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "            ],\n",
    "            \"triggers\": [\"<function=\"],\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T20:32:56.761452Z",
     "iopub.status.busy": "2025-04-19T20:32:56.761219Z",
     "iopub.status.idle": "2025-04-19T20:32:56.805415Z",
     "shell.execute_reply": "2025-04-19T20:32:56.804894Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
