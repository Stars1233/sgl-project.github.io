{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGLang Native APIs\n",
    "\n",
    "Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its native server APIs. We introduce the following APIs:\n",
    "\n",
    "- `/generate` (text generation model)\n",
    "- `/get_model_info`\n",
    "- `/get_server_info`\n",
    "- `/health`\n",
    "- `/health_generate`\n",
    "- `/flush_cache`\n",
    "- `/update_weights`\n",
    "- `/encode`(embedding model)\n",
    "- `/v1/rerank`(cross encoder rerank model)\n",
    "- `/classify`(reward model)\n",
    "- `/start_expert_distribution_record`\n",
    "- `/stop_expert_distribution_record`\n",
    "- `/dump_expert_distribution_record`\n",
    "- A full list of these APIs can be found at [http_server.py](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server.py)\n",
    "\n",
    "We mainly use `requests` to test these APIs in the following examples. You can also use `curl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:18:48.879160Z",
     "iopub.status.busy": "2025-10-07T12:18:48.879037Z",
     "iopub.status.idle": "2025-10-07T12:19:22.758912Z",
     "shell.execute_reply": "2025-10-07T12:19:22.758405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "WARNING:transformers.configuration_utils:`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-10-07 12:19:11] `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2025-10-07 12:19:13] MOE_RUNNER_BACKEND is not initialized, using triton backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.27it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=77.03 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=77.03 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.76it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.97 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.76it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.96 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.76it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.96 GB): 100%|██████████| 3/3 [00:00<00:00,  7.17it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.96 GB): 100%|██████████| 3/3 [00:00<00:00,  6.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (text generation model)\n",
    "Generate completions. This is similar to the `/v1/completions` in OpenAI API. Detailed parameters can be found in the [sampling parameters](sampling_params.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:22.764247Z",
     "iopub.status.busy": "2025-10-07T12:19:22.762595Z",
     "iopub.status.idle": "2025-10-07T12:19:22.997266Z",
     "shell.execute_reply": "2025-10-07T12:19:22.996795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': ' The capital of France is Paris. It is located in the centre of mainland Europe. Paris, officially Le Bourgeons de Paris, was founded in 788 AD and was an important center of art and culture in Europe.\\n\\nThe city is rich in history and is home to millions of tourists each year. Paris is also known for food, art, and culture. It is the capital of the French department of the same name and enjoys the position of the French Fifth-most populous metropolitan area. Paris received the nickname the \"Music City\" as the city\\'s streets are full of barrens and jazz bars.\\n\\nIt is important to visit', 'output_ids': [279, 6722, 315, 9625, 30, 576, 6722, 315, 9625, 374, 12095, 13, 1084, 374, 7407, 304, 279, 12261, 315, 50015, 4505, 13, 12095, 11, 18562, 1967, 38375, 709, 2382, 409, 12095, 11, 572, 18047, 304, 220, 22, 23, 23, 9630, 323, 572, 458, 2989, 4126, 315, 1947, 323, 7674, 304, 4505, 382, 785, 3283, 374, 9080, 304, 3840, 323, 374, 2114, 311, 11728, 315, 31653, 1817, 1042, 13, 12095, 374, 1083, 3881, 369, 3607, 11, 1947, 11, 323, 7674, 13, 1084, 374, 279, 6722, 315, 279, 8585, 9292, 315, 279, 1852, 829, 323, 31738, 279, 2309, 315, 279, 8585, 22843, 62398, 94451, 57406, 3082, 13, 12095, 3949, 279, 29399, 279, 330, 24128, 4311, 1, 438, 279, 3283, 594, 14371, 525, 2480, 315, 3619, 77340, 323, 33897, 15904, 382, 2132, 374, 2989, 311, 3947], 'meta_info': {'id': '20655c9d63ef460797cc255fbb0bca4e', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.22599196434020996}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Info\n",
    "\n",
    "Get the information of the model.\n",
    "\n",
    "- `model_path`: The path/name of the model.\n",
    "- `is_generation`: Whether the model is used as generation model or embedding model.\n",
    "- `tokenizer_path`: The path/name of the tokenizer.\n",
    "- `preferred_sampling_params`: The default sampling params specified via `--preferred-sampling-params`. `None` is returned in this example as we did not explicitly configure it in server args.\n",
    "- `weight_version`: This field contains the version of the model weights. This is often used to track changes or updates to the model’s trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:22.998892Z",
     "iopub.status.busy": "2025-10-07T12:19:22.998732Z",
     "iopub.status.idle": "2025-10-07T12:19:23.004785Z",
     "shell.execute_reply": "2025-10-07T12:19:23.004403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'model_path': 'qwen/qwen2.5-0.5b-instruct', 'tokenizer_path': 'qwen/qwen2.5-0.5b-instruct', 'is_generation': True, 'preferred_sampling_params': None, 'weight_version': 'default'}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_model_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"model_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"is_generation\"] is True\n",
    "assert response_json[\"tokenizer_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"preferred_sampling_params\"] is None\n",
    "assert response_json.keys() == {\n",
    "    \"model_path\",\n",
    "    \"is_generation\",\n",
    "    \"tokenizer_path\",\n",
    "    \"preferred_sampling_params\",\n",
    "    \"weight_version\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Server Info\n",
    "Gets the server information including CLI arguments, token limits, and memory pool sizes.\n",
    "- Note: `get_server_info` merges the following deprecated endpoints:\n",
    "  - `get_server_args`\n",
    "  - `get_memory_pool_size` \n",
    "  - `get_max_total_num_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:23.006059Z",
     "iopub.status.busy": "2025-10-07T12:19:23.005922Z",
     "iopub.status.idle": "2025-10-07T12:19:23.012337Z",
     "shell.execute_reply": "2025-10-07T12:19:23.011967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"model_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_mode\":\"auto\",\"tokenizer_worker_num\":1,\"skip_tokenizer_init\":false,\"load_format\":\"auto\",\"model_loader_extra_config\":\"{}\",\"trust_remote_code\":false,\"modelopt_quant\":null,\"context_length\":null,\"is_embedding\":false,\"enable_multimodal\":null,\"revision\":null,\"model_impl\":\"auto\",\"host\":\"0.0.0.0\",\"port\":37377,\"skip_server_warmup\":false,\"warmups\":null,\"nccl_port\":null,\"dtype\":\"auto\",\"quantization\":null,\"quantization_param_path\":null,\"kv_cache_dtype\":\"auto\",\"enable_fp32_lm_head\":false,\"mem_fraction_static\":0.841,\"max_running_requests\":128,\"max_queued_requests\":null,\"max_total_tokens\":20480,\"chunked_prefill_size\":8192,\"max_prefill_tokens\":16384,\"schedule_policy\":\"fcfs\",\"enable_priority_scheduling\":false,\"schedule_low_priority_values_first\":false,\"priority_scheduling_preemption_threshold\":10,\"schedule_conservativeness\":1.0,\"page_size\":1,\"hybrid_kvcache_ratio\":null,\"swa_full_tokens_ratio\":0.8,\"disable_hybrid_swa_memory\":false,\"radix_eviction_policy\":\"lru\",\"device\":\"cuda\",\"tp_size\":1,\"pp_size\":1,\"pp_max_micro_batch_size\":null,\"stream_interval\":1,\"stream_output\":false,\"random_seed\":228765910,\"constrained_json_whitespace_pattern\":null,\"watchdog_timeout\":300,\"dist_timeout\":null,\"download_dir\":null,\"base_gpu_id\":0,\"gpu_id_step\":1,\"sleep_on_idle\":false,\"log_level\":\"warning\",\"log_level_http\":null,\"log_requests\":false,\"log_requests_level\":2,\"crash_dump_folder\":null,\"show_time_cost\":false,\"enable_metrics\":false,\"enable_metrics_for_all_schedulers\":false,\"tokenizer_metrics_custom_labels_header\":\"x-custom-labels\",\"tokenizer_metrics_allowed_custom_labels\":null,\"bucket_time_to_first_token\":null,\"bucket_inter_token_latency\":null,\"bucket_e2e_request_latency\":null,\"collect_tokens_histogram\":false,\"prompt_tokens_buckets\":null,\"generation_tokens_buckets\":null,\"decode_log_interval\":40,\"enable_request_time_stats_logging\":false,\"kv_events_config\":null,\"gc_warning_threshold_secs\":0.0,\"enable_trace\":false,\"oltp_traces_endpoint\":\"localhost:4317\",\"api_key\":null,\"served_model_name\":\"qwen/qwen2.5-0.5b-instruct\",\"weight_version\":\"default\",\"chat_template\":null,\"completion_template\":null,\"file_storage_path\":\"sglang_storage\",\"enable_cache_report\":false,\"reasoning_parser\":null,\"tool_call_parser\":null,\"tool_server\":null,\"dp_size\":1,\"load_balance_method\":\"round_robin\",\"load_watch_interval\":0.1,\"prefill_round_robin_balance\":false,\"dist_init_addr\":null,\"nnodes\":1,\"node_rank\":0,\"json_model_override_args\":\"{}\",\"preferred_sampling_params\":null,\"enable_lora\":null,\"max_lora_rank\":null,\"lora_target_modules\":null,\"lora_paths\":null,\"max_loaded_loras\":null,\"max_loras_per_batch\":8,\"lora_backend\":\"triton\",\"max_lora_chunk_size\":16,\"attention_backend\":null,\"decode_attention_backend\":null,\"prefill_attention_backend\":null,\"sampling_backend\":\"flashinfer\",\"grammar_backend\":\"xgrammar\",\"mm_attention_backend\":null,\"nsa_prefill\":\"flashmla_prefill\",\"nsa_decode\":\"fa3\",\"speculative_algorithm\":null,\"speculative_draft_model_path\":null,\"speculative_draft_model_revision\":null,\"speculative_num_steps\":null,\"speculative_eagle_topk\":null,\"speculative_num_draft_tokens\":null,\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_token_map\":null,\"speculative_attention_mode\":\"prefill\",\"speculative_ngram_min_match_window_size\":1,\"speculative_ngram_max_match_window_size\":12,\"speculative_ngram_min_bfs_breadth\":1,\"speculative_ngram_max_bfs_breadth\":10,\"speculative_ngram_match_type\":\"BFS\",\"speculative_ngram_branch_length\":18,\"speculative_ngram_capacity\":10000000,\"ep_size\":1,\"moe_a2a_backend\":\"none\",\"moe_runner_backend\":\"auto\",\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"deepep_mode\":\"auto\",\"ep_num_redundant_experts\":0,\"ep_dispatch_algorithm\":\"static\",\"init_expert_location\":\"trivial\",\"enable_eplb\":false,\"eplb_algorithm\":\"auto\",\"eplb_rebalance_num_iterations\":1000,\"eplb_rebalance_layers_per_chunk\":null,\"eplb_min_rebalancing_utilization_threshold\":1.0,\"expert_distribution_recorder_mode\":null,\"expert_distribution_recorder_buffer_size\":1000,\"enable_expert_distribution_metrics\":false,\"deepep_config\":null,\"moe_dense_tp_size\":null,\"max_mamba_cache_size\":null,\"mamba_ssm_dtype\":\"float32\",\"enable_hierarchical_cache\":false,\"hicache_ratio\":2.0,\"hicache_size\":0,\"hicache_write_policy\":\"write_through\",\"hicache_io_backend\":\"kernel\",\"hicache_mem_layout\":\"layer_first\",\"hicache_storage_backend\":null,\"hicache_storage_prefetch_policy\":\"best_effort\",\"hicache_storage_backend_extra_config\":null,\"enable_lmcache\":false,\"enable_double_sparsity\":false,\"ds_channel_config_path\":null,\"ds_heavy_channel_num\":32,\"ds_heavy_token_num\":256,\"ds_heavy_channel_type\":\"qk\",\"ds_sparse_decode_threshold\":4096,\"cpu_offload_gb\":0,\"offload_group_size\":-1,\"offload_num_in_group\":1,\"offload_prefetch_step\":1,\"offload_mode\":\"cpu\",\"disable_radix_cache\":false,\"cuda_graph_max_bs\":4,\"cuda_graph_bs\":[1,2,4],\"disable_cuda_graph\":false,\"disable_cuda_graph_padding\":false,\"enable_profile_cuda_graph\":false,\"enable_cudagraph_gc\":false,\"enable_nccl_nvls\":false,\"enable_symm_mem\":false,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"enable_tokenizer_batch_encode\":false,\"disable_outlines_disk_cache\":false,\"disable_custom_all_reduce\":false,\"enable_mscclpp\":false,\"enable_torch_symm_mem\":false,\"disable_overlap_schedule\":false,\"enable_mixed_chunk\":false,\"enable_dp_attention\":false,\"enable_dp_lm_head\":false,\"enable_two_batch_overlap\":false,\"enable_single_batch_overlap\":false,\"tbo_token_distribution_threshold\":0.48,\"enable_torch_compile\":false,\"torch_compile_max_bs\":32,\"torchao_config\":\"\",\"enable_nan_detection\":false,\"enable_p2p_check\":false,\"triton_attention_reduce_in_fp32\":false,\"triton_attention_num_kv_splits\":8,\"triton_attention_split_tile_size\":null,\"num_continuous_decode_steps\":1,\"delete_ckpt_after_loading\":false,\"enable_memory_saver\":false,\"enable_weights_cpu_backup\":false,\"allow_auto_truncate\":false,\"enable_custom_logit_processor\":false,\"flashinfer_mla_disable_ragged\":false,\"disable_shared_experts_fusion\":false,\"disable_chunked_prefix_cache\":false,\"disable_fast_image_processor\":false,\"keep_mm_feature_on_device\":false,\"enable_return_hidden_states\":false,\"scheduler_recv_interval\":1,\"numa_node\":null,\"enable_deterministic_inference\":false,\"enable_dynamic_batch_tokenizer\":false,\"dynamic_batch_tokenizer_batch_size\":32,\"dynamic_batch_tokenizer_batch_timeout\":0.002,\"debug_tensor_dump_output_folder\":null,\"debug_tensor_dump_input_file\":null,\"debug_tensor_dump_inject\":false,\"debug_tensor_dump_prefill_only\":false,\"disaggregation_mode\":\"null\",\"disaggregation_transfer_backend\":\"mooncake\",\"disaggregation_bootstrap_port\":8998,\"disaggregation_decode_tp\":null,\"disaggregation_decode_dp\":null,\"disaggregation_prefill_pp\":1,\"disaggregation_ib_device\":null,\"disaggregation_decode_enable_offload_kvcache\":false,\"num_reserved_decode_tokens\":512,\"disaggregation_decode_polling_interval\":1,\"custom_weight_loader\":[],\"weight_loader_disable_mmap\":false,\"remote_instance_weight_loader_seed_instance_ip\":null,\"remote_instance_weight_loader_seed_instance_service_port\":null,\"remote_instance_weight_loader_send_weights_group_ports\":null,\"enable_pdmux\":false,\"sm_group_num\":3,\"status\":\"ready\",\"max_total_num_tokens\":20480,\"max_req_input_len\":20474,\"internal_states\":[{\"attention_backend\":\"fa3\",\"mm_attention_backend\":null,\"debug_tensor_dump_inject\":false,\"debug_tensor_dump_output_folder\":null,\"chunked_prefill_size\":8192,\"device\":\"cuda\",\"disable_chunked_prefix_cache\":true,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"disable_radix_cache\":false,\"enable_dp_lm_head\":false,\"enable_fp32_lm_head\":false,\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"moe_dense_tp_size\":null,\"ep_dispatch_algorithm\":\"static\",\"ep_num_redundant_experts\":0,\"enable_nan_detection\":false,\"flashinfer_mla_disable_ragged\":false,\"pp_max_micro_batch_size\":128,\"disable_shared_experts_fusion\":false,\"sampling_backend\":\"flashinfer\",\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_attention_mode\":\"prefill\",\"torchao_config\":\"\",\"triton_attention_reduce_in_fp32\":false,\"num_reserved_decode_tokens\":512,\"weight_loader_disable_mmap\":false,\"enable_multimodal\":null,\"enable_symm_mem\":false,\"enable_custom_logit_processor\":false,\"disaggregation_mode\":\"null\",\"enable_deterministic_inference\":false,\"nsa_prefill\":\"flashmla_prefill\",\"nsa_decode\":\"fa3\",\"use_mla_backend\":false,\"speculative_algorithm\":1,\"decode_attention_backend\":\"fa3\",\"prefill_attention_backend\":\"fa3\",\"last_gen_throughput\":667.4244381206901,\"memory_usage\":{\"weight\":1.06,\"kvcache\":0.23,\"token_capacity\":20480,\"graph\":0.07}}],\"version\":\"0.5.3\"}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_server_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check\n",
    "- `/health`: Check the health of the server.\n",
    "- `/health_generate`: Check the health of the server by generating one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:23.013573Z",
     "iopub.status.busy": "2025-10-07T12:19:23.013435Z",
     "iopub.status.idle": "2025-10-07T12:19:24.019645Z",
     "shell.execute_reply": "2025-10-07T12:19:24.019180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health_generate\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:24.021119Z",
     "iopub.status.busy": "2025-10-07T12:19:24.020964Z",
     "iopub.status.idle": "2025-10-07T12:19:25.028066Z",
     "shell.execute_reply": "2025-10-07T12:19:25.027601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flush Cache\n",
    "\n",
    "Flush the radix cache. It will be automatically triggered when the model weights are updated by the `/update_weights` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:25.029574Z",
     "iopub.status.busy": "2025-10-07T12:19:25.029420Z",
     "iopub.status.idle": "2025-10-07T12:19:25.042196Z",
     "shell.execute_reply": "2025-10-07T12:19:25.041818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Cache flushed.<br>Please check backend logs for more details. (When there are running or waiting requests, the operation will not be performed.)<br></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/flush_cache\"\n",
    "\n",
    "response = requests.post(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights From Disk\n",
    "\n",
    "Update model weights from disk without restarting the server. Only applicable for models with the same architecture and parameter size.\n",
    "\n",
    "SGLang support `update_weights_from_disk` API for continuous evaluation during training (save checkpoint to disk and update weights from disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:25.043452Z",
     "iopub.status.busy": "2025-10-07T12:19:25.043309Z",
     "iopub.status.idle": "2025-10-07T12:19:25.486850Z",
     "shell.execute_reply": "2025-10-07T12:19:25.486390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.24it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.23it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"success\":true,\"message\":\"Succeeded to update model weights.\",\"num_paused_requests\":0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful update with same architecture and size\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)\n",
    "assert response.json()[\"success\"] is True\n",
    "assert response.json()[\"message\"] == \"Succeeded to update model weights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:25.488255Z",
     "iopub.status.busy": "2025-10-07T12:19:25.488101Z",
     "iopub.status.idle": "2025-10-07T12:19:25.584786Z",
     "shell.execute_reply": "2025-10-07T12:19:25.584350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 12:19:25] Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'success': False, 'message': 'Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).', 'num_paused_requests': 0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# failed update with different parameter size or wrong name\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct-wrong\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"success\"] is False\n",
    "assert response_json[\"message\"] == (\n",
    "    \"Failed to get weights iterator: \"\n",
    "    \"qwen/qwen2.5-0.5b-instruct-wrong\"\n",
    "    \" (repository not found).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:25.586022Z",
     "iopub.status.busy": "2025-10-07T12:19:25.585878Z",
     "iopub.status.idle": "2025-10-07T12:19:25.622418Z",
     "shell.execute_reply": "2025-10-07T12:19:25.621600Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode (embedding model)\n",
    "\n",
    "Encode text into embeddings. Note that this API is only available for [embedding models](openai_api_embeddings.ipynb) and will raise an error for generation models.\n",
    "Therefore, we launch a new server to server an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:25.624782Z",
     "iopub.status.busy": "2025-10-07T12:19:25.624623Z",
     "iopub.status.idle": "2025-10-07T12:19:54.694393Z",
     "shell.execute_reply": "2025-10-07T12:19:54.693819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "WARNING:transformers.configuration_utils:`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-10-07 12:19:42] `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2025-10-07 12:19:44] MOE_RUNNER_BACKEND is not initialized, using triton backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-instruct \\\n",
    "    --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:54.696063Z",
     "iopub.status.busy": "2025-10-07T12:19:54.695910Z",
     "iopub.status.idle": "2025-10-07T12:19:54.718206Z",
     "shell.execute_reply": "2025-10-07T12:19:54.717681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Text embedding (first 10): [-0.00023102760314941406, -0.04986572265625, -0.0032711029052734375, 0.011077880859375, -0.0140533447265625, 0.0159912109375, -0.01441192626953125, 0.0059051513671875, -0.0228424072265625, 0.0272979736328125]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful encode for embedding model\n",
    "\n",
    "url = f\"http://localhost:{port}/encode\"\n",
    "data = {\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"text\": \"Once upon a time\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(f\"Text embedding (first 10): {response_json['embedding'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:54.719679Z",
     "iopub.status.busy": "2025-10-07T12:19:54.719527Z",
     "iopub.status.idle": "2025-10-07T12:19:54.750444Z",
     "shell.execute_reply": "2025-10-07T12:19:54.749775Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(embedding_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1/rerank (cross encoder rerank model)\n",
    "Rerank a list of documents given a query using a cross-encoder model. Note that this API is only available for cross encoder model like [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) with `attention-backend` `triton` and `torch_native`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:19:54.765751Z",
     "iopub.status.busy": "2025-10-07T12:19:54.765473Z",
     "iopub.status.idle": "2025-10-07T12:20:24.846135Z",
     "shell.execute_reply": "2025-10-07T12:20:24.845651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "WARNING:transformers.configuration_utils:`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-10-07 12:20:11] `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2025-10-07 12:20:14] MOE_RUNNER_BACKEND is not initialized, using triton backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reranker_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path BAAI/bge-reranker-v2-m3 \\\n",
    "    --host 0.0.0.0 --disable-radix-cache --chunked-prefill-size -1 --attention-backend triton --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:20:24.847644Z",
     "iopub.status.busy": "2025-10-07T12:20:24.847486Z",
     "iopub.status.idle": "2025-10-07T12:20:24.939064Z",
     "shell.execute_reply": "2025-10-07T12:20:24.938619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: 5.26 - Document: 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: -8.19 - Document: 'hi'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute rerank scores for query and documents\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/rerank\"\n",
    "data = {\n",
    "    \"model\": \"BAAI/bge-reranker-v2-m3\",\n",
    "    \"query\": \"what is panda?\",\n",
    "    \"documents\": [\n",
    "        \"hi\",\n",
    "        \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "for item in response_json:\n",
    "    print_highlight(f\"Score: {item['score']:.2f} - Document: '{item['document']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:20:24.940426Z",
     "iopub.status.busy": "2025-10-07T12:20:24.940276Z",
     "iopub.status.idle": "2025-10-07T12:20:24.985344Z",
     "shell.execute_reply": "2025-10-07T12:20:24.984591Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reranker_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify (reward model)\n",
    "\n",
    "SGLang Runtime also supports reward models. Here we use a reward model to classify the quality of pairwise generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:20:24.987814Z",
     "iopub.status.busy": "2025-10-07T12:20:24.987644Z",
     "iopub.status.idle": "2025-10-07T12:20:54.062086Z",
     "shell.execute_reply": "2025-10-07T12:20:54.061607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "WARNING:transformers.configuration_utils:`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-10-07 12:20:41] `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2025-10-07 12:20:43] MOE_RUNNER_BACKEND is not initialized, using triton backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.13it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.72it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.46it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that SGLang now treats embedding models and reward models as the same type of models.\n",
    "# This will be updated in the future.\n",
    "\n",
    "reward_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:20:54.063739Z",
     "iopub.status.busy": "2025-10-07T12:20:54.063584Z",
     "iopub.status.idle": "2025-10-07T12:20:54.668344Z",
     "shell.execute_reply": "2025-10-07T12:20:54.667873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: -24.125</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: 1.171875</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "PROMPT = (\n",
    "    \"What is the range of the numeric output of a sigmoid node in a neural network?\"\n",
    ")\n",
    "\n",
    "RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n",
    "RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n",
    "\n",
    "CONVS = [\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE1}],\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE2}],\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\")\n",
    "prompts = tokenizer.apply_chat_template(CONVS, tokenize=False)\n",
    "\n",
    "url = f\"http://localhost:{port}/classify\"\n",
    "data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": prompts}\n",
    "\n",
    "responses = requests.post(url, json=data).json()\n",
    "for response in responses:\n",
    "    print_highlight(f\"reward: {response['embedding'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:20:54.669877Z",
     "iopub.status.busy": "2025-10-07T12:20:54.669731Z",
     "iopub.status.idle": "2025-10-07T12:20:54.710754Z",
     "shell.execute_reply": "2025-10-07T12:20:54.710182Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reward_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture expert selection distribution in MoE models\n",
    "\n",
    "SGLang Runtime supports recording the number of times an expert is selected in a MoE model run for each expert in the model. This is useful when analyzing the throughput of the model and plan for optimization.\n",
    "\n",
    "*Note: We only print out the first 10 lines of the csv below for better readability. Please adjust accordingly if you want to analyze the results more deeply.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:20:54.713080Z",
     "iopub.status.busy": "2025-10-07T12:20:54.712924Z",
     "iopub.status.idle": "2025-10-07T12:21:32.806461Z",
     "shell.execute_reply": "2025-10-07T12:21:32.805770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "WARNING:transformers.configuration_utils:`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-10-07 12:21:11] `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:04,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:04,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:03,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:01,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:05<00:00,  1.16it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.53it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.31it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=30.22 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 12:21:23] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l1f-gpu-1/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=30.22 GB):  33%|███▎      | 1/3 [00:01<00:03,  1.61s/it]\r",
      "Capturing batches (bs=2 avail_mem=30.10 GB):  33%|███▎      | 1/3 [00:01<00:03,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=30.10 GB):  67%|██████▋   | 2/3 [00:02<00:01,  1.14s/it]\r",
      "Capturing batches (bs=1 avail_mem=47.15 GB):  67%|██████▋   | 2/3 [00:02<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=47.15 GB): 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\r",
      "Capturing batches (bs=1 avail_mem=47.15 GB): 100%|██████████| 3/3 [00:02<00:00,  1.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expert_record_server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:21:32.808271Z",
     "iopub.status.busy": "2025-10-07T12:21:32.808106Z",
     "iopub.status.idle": "2025-10-07T12:21:33.349673Z",
     "shell.execute_reply": "2025-10-07T12:21:33.349101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 12:21:32] ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/public_sglang_ci/runner-l1f-gpu-1/_work/sglang/sglang/python/sglang/srt/entrypoints/http_server.py\", line 657, in start_expert_distribution_record_async\n",
      "    await _global_state.tokenizer_manager.start_expert_distribution_record()\n",
      "  File \"/public_sglang_ci/runner-l1f-gpu-1/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_communicator_mixin.py\", line 339, in start_expert_distribution_record\n",
      "    req = ExpertDistributionReq(action=ExpertDistributionReqType.START_RECORD)\n",
      "TypeError: BaseReq.__init__() got an unexpected keyword argument 'action'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [500]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': \" Can you believe our cultured little podcast that we get these two lawyers who went to the local France office, they hired a guy who works in the area, and actually went down there to Mississippi to find someone — and then learn a whole bunch about how the French monarchy worked. But that’s what you get in the episode. To be exact, you will learn about how Louis XVI as'$.\\nHighlights\\n- The Severan Dynasty\\n- Wild Beasts\\n- Seeing to Men Who Needed Help\\n- The Gallick Dynasty\\n- Louis XV\\n- Louis XVI\\n- Count of Countryside\\n- Private Glamis\\n- Does Astro\", 'output_ids': [279, 6722, 315, 9625, 30, 2980, 498, 4411, 1039, 88848, 2632, 17711, 429, 582, 633, 1493, 1378, 21177, 879, 3937, 311, 279, 2205, 9625, 5163, 11, 807, 21446, 264, 7412, 879, 4278, 304, 279, 3082, 11, 323, 3520, 3937, 1495, 1052, 311, 28438, 311, 1477, 4325, 1959, 323, 1221, 3960, 264, 4361, 15493, 911, 1246, 279, 8585, 86049, 6439, 13, 1988, 429, 748, 1128, 498, 633, 304, 279, 9234, 13, 2014, 387, 4734, 11, 498, 686, 3960, 911, 1246, 11876, 93837, 438, 44701, 624, 96306, 198, 12, 576, 58216, 276, 73395, 198, 12, 13630, 2823, 11757, 198, 12, 55024, 311, 11012, 10479, 56561, 11479, 198, 12, 576, 10620, 1206, 73395, 198, 12, 11876, 52594, 198, 12, 11876, 93837, 198, 12, 4504, 315, 31254, 44255, 198, 12, 9679, 86713, 285, 198, 12, 12553, 64129], 'meta_info': {'id': '260fbb771b99446c8d9b4e5440382e0e', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.5179164409637451}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 12:21:33] ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/public_sglang_ci/runner-l1f-gpu-1/_work/sglang/sglang/python/sglang/srt/entrypoints/http_server.py\", line 667, in stop_expert_distribution_record_async\n",
      "    await _global_state.tokenizer_manager.stop_expert_distribution_record()\n",
      "  File \"/public_sglang_ci/runner-l1f-gpu-1/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_communicator_mixin.py\", line 344, in stop_expert_distribution_record\n",
      "    req = ExpertDistributionReq(action=ExpertDistributionReqType.STOP_RECORD)\n",
      "TypeError: BaseReq.__init__() got an unexpected keyword argument 'action'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [500]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 12:21:33] ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/public_sglang_ci/runner-l1f-gpu-1/_work/sglang/sglang/python/sglang/srt/entrypoints/http_server.py\", line 677, in dump_expert_distribution_record_async\n",
      "    await _global_state.tokenizer_manager.dump_expert_distribution_record()\n",
      "  File \"/public_sglang_ci/runner-l1f-gpu-1/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_communicator_mixin.py\", line 349, in dump_expert_distribution_record\n",
      "    req = ExpertDistributionReq(action=ExpertDistributionReqType.DUMP_RECORD)\n",
      "TypeError: BaseReq.__init__() got an unexpected keyword argument 'action'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [500]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = requests.post(f\"http://localhost:{port}/start_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/stop_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/dump_expert_distribution_record\")\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T12:21:33.351454Z",
     "iopub.status.busy": "2025-10-07T12:21:33.351303Z",
     "iopub.status.idle": "2025-10-07T12:21:33.396448Z",
     "shell.execute_reply": "2025-10-07T12:21:33.395852Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(expert_record_server_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
