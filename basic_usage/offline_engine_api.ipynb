{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Engine API\n",
    "\n",
    "SGLang provides a direct inference engine without the need for an HTTP server, especially for use cases where additional HTTP server adds unnecessary complexity or overhead. Here are two general use cases:\n",
    "\n",
    "- Offline Batch Inference\n",
    "- Custom Server on Top of the Engine\n",
    "\n",
    "This document focuses on the offline batch inference, demonstrating four different inference modes:\n",
    "\n",
    "- Non-streaming synchronous generation\n",
    "- Streaming synchronous generation\n",
    "- Non-streaming asynchronous generation\n",
    "- Streaming asynchronous generation\n",
    "\n",
    "Additionally, you can easily build a custom server on top of the SGLang offline engine. A detailed example working in a python script can be found in [custom_server](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/custom_server.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest Asyncio\n",
    "Note that if you want to use **Offline Engine** in ipython or some other nested loop code, you need to add the following code:\n",
    "```python\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The engine supports [vlm inference](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py) as well as [extracting hidden states](https://github.com/sgl-project/sglang/blob/main/examples/runtime/hidden_states). \n",
    "\n",
    "Please see [the examples](https://github.com/sgl-project/sglang/tree/main/examples/runtime/engine) for further use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference\n",
    "\n",
    "SGLang offline engine supports batch inference with efficient scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:02:48.965749Z",
     "iopub.status.busy": "2025-10-31T13:02:48.965616Z",
     "iopub.status.idle": "2025-10-31T13:03:10.207459Z",
     "shell.execute_reply": "2025-10-31T13:03:10.206672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:02:54] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:02:54] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:02:54] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:02:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:02:56] WARNING server_args.py:1144: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:02:56] INFO engine.py:124: server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.835, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=931748383, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=-1, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:03:03] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-10-31 13:03:03] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-10-31 13:03:03] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 13:03:03] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.67it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.66it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=62.28 GB):   0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=62.28 GB):   5%|▌         | 1/20 [00:00<00:02,  6.36it/s]\r",
      "Capturing batches (bs=120 avail_mem=62.18 GB):   5%|▌         | 1/20 [00:00<00:02,  6.36it/s]\r",
      "Capturing batches (bs=112 avail_mem=62.17 GB):   5%|▌         | 1/20 [00:00<00:02,  6.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=104 avail_mem=62.17 GB):   5%|▌         | 1/20 [00:00<00:02,  6.36it/s]\r",
      "Capturing batches (bs=104 avail_mem=62.17 GB):  20%|██        | 4/20 [00:00<00:00, 16.66it/s]\r",
      "Capturing batches (bs=96 avail_mem=62.16 GB):  20%|██        | 4/20 [00:00<00:00, 16.66it/s] \r",
      "Capturing batches (bs=88 avail_mem=62.15 GB):  20%|██        | 4/20 [00:00<00:00, 16.66it/s]\r",
      "Capturing batches (bs=80 avail_mem=62.15 GB):  20%|██        | 4/20 [00:00<00:00, 16.66it/s]\r",
      "Capturing batches (bs=80 avail_mem=62.15 GB):  35%|███▌      | 7/20 [00:00<00:00, 21.06it/s]\r",
      "Capturing batches (bs=72 avail_mem=62.15 GB):  35%|███▌      | 7/20 [00:00<00:00, 21.06it/s]\r",
      "Capturing batches (bs=64 avail_mem=62.14 GB):  35%|███▌      | 7/20 [00:00<00:00, 21.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=56 avail_mem=62.14 GB):  35%|███▌      | 7/20 [00:00<00:00, 21.06it/s]\r",
      "Capturing batches (bs=56 avail_mem=62.14 GB):  50%|█████     | 10/20 [00:00<00:00, 22.85it/s]\r",
      "Capturing batches (bs=48 avail_mem=62.13 GB):  50%|█████     | 10/20 [00:00<00:00, 22.85it/s]\r",
      "Capturing batches (bs=40 avail_mem=62.13 GB):  50%|█████     | 10/20 [00:00<00:00, 22.85it/s]\r",
      "Capturing batches (bs=32 avail_mem=62.12 GB):  50%|█████     | 10/20 [00:00<00:00, 22.85it/s]\r",
      "Capturing batches (bs=32 avail_mem=62.12 GB):  65%|██████▌   | 13/20 [00:00<00:00, 23.90it/s]\r",
      "Capturing batches (bs=24 avail_mem=62.12 GB):  65%|██████▌   | 13/20 [00:00<00:00, 23.90it/s]\r",
      "Capturing batches (bs=16 avail_mem=62.11 GB):  65%|██████▌   | 13/20 [00:00<00:00, 23.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=12 avail_mem=62.11 GB):  65%|██████▌   | 13/20 [00:00<00:00, 23.90it/s]\r",
      "Capturing batches (bs=12 avail_mem=62.11 GB):  80%|████████  | 16/20 [00:00<00:00, 22.56it/s]\r",
      "Capturing batches (bs=8 avail_mem=62.10 GB):  80%|████████  | 16/20 [00:00<00:00, 22.56it/s] \r",
      "Capturing batches (bs=4 avail_mem=62.10 GB):  80%|████████  | 16/20 [00:00<00:00, 22.56it/s]\r",
      "Capturing batches (bs=2 avail_mem=62.09 GB):  80%|████████  | 16/20 [00:00<00:00, 22.56it/s]\r",
      "Capturing batches (bs=1 avail_mem=62.09 GB):  80%|████████  | 16/20 [00:00<00:00, 22.56it/s]\r",
      "Capturing batches (bs=1 avail_mem=62.09 GB): 100%|██████████| 20/20 [00:00<00:00, 25.45it/s]\r",
      "Capturing batches (bs=1 avail_mem=62.09 GB): 100%|██████████| 20/20 [00:00<00:00, 22.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "import asyncio\n",
    "\n",
    "import sglang as sgl\n",
    "import sglang.test.doc_patch\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "\n",
    "llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:03:10.209694Z",
     "iopub.status.busy": "2025-10-31T13:03:10.209151Z",
     "iopub.status.idle": "2025-10-31T13:03:10.977608Z",
     "shell.execute_reply": "2025-10-31T13:03:10.976926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Hello, my name is\n",
      "Generated text:  Vicky and I have been a digital marketing consultant for over 15 years. I have worked with companies like Boeing, AT&T, Standard & Poor’s, and The Smiths, and I have helped many individuals and small businesses grow their digital presence. My clients have included startups and established companies with an array of interests and pursuits including but not limited to: e-commerce, publishing, photography, and advertising. I am an experienced trainer and an active contributor to the blogging community, serving as a frequent guest on sites such as The Hemingway Review, The Daily Beast, and the Marketing Classroom blog. I am in the business of\n",
      "===============================\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  a man, but his chief advisor is a woman.\n",
      "Why do you think the president of the United States is a man? The president of the United States is a man because he serves as the head of state and government of the country. He is elected to this position and is appointed by the President of the United States, who is a male. The role of the president is to represent the interests of the country and to lead the nation in matters of national policy and administration.\n",
      "On the other hand, the chief advisor to the president is a woman, not a man. The chief advisor is a position held by the president's chief of\n",
      "===============================\n",
      "Prompt: The capital of France is\n",
      "Generated text: :\n",
      "A. Paris\n",
      "B. Lyon\n",
      "C. Marseille\n",
      "D. Strasbourg\n",
      "Answer:\n",
      "\n",
      "A\n",
      "\n",
      "According to the law, which of the following statements about the right of a person to privacy is correct?\n",
      "A. The right of a person to privacy is a fundamental right of citizens.\n",
      "B. The right of a person to privacy is the same as the right of a person to property.\n",
      "C. The right of a person to privacy is equivalent to the right of a person to life and health.\n",
      "D. The right of a person to privacy is established before the right of a person to freedom of movement.\n",
      "Answer:\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "===============================\n",
      "Prompt: The future of AI is\n",
      "Generated text:  highly uncertain, but it's inevitable that it will change the world in the future. The future of AI will be a product of many factors, including advancements in hardware, software, and human ingenuity. Here are some of the potential changes that AI could bring to the future of the world:\n",
      "\n",
      "1. Personalized Medicine: AI can help doctors to provide personalized medicine to their patients. AI can analyze large amounts of data to identify patterns and predict which patients are likely to respond to certain treatments, making it easier for doctors to choose the right treatments for each patient.\n",
      "\n",
      "2. Augmented Reality: AI can also be used in augmented reality (\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:03:10.979173Z",
     "iopub.status.busy": "2025-10-31T13:03:10.979011Z",
     "iopub.status.idle": "2025-10-31T13:03:11.645205Z",
     "shell.execute_reply": "2025-10-31T13:03:11.644563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing synchronous streaming generation with overlap removal ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  [Name] and I'm a [Age] year old [Occupation]. I'm a [Skill] who has [Number of Years] years of experience in [Field]. I'm a [Skill] who has [Number of Years] years of experience in [Field]. I'm a [Skill] who has [Number of Years] years of experience in [Field]. I'm a [Skill] who has [Number of Years] years of experience in [Field]. I'm a [Skill] who has [Number of Years] years of experience in [Field]. I'm a [Skill] who has [Number of Years\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text:  Paris, which is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. It is also a popular tourist destination, with many visitors coming to see the city's rich history and culture. Paris is a vibrant and diverse city with a rich history dating back to the Roman Empire and the French Revolution. It is home to many famous museums, including the Musée d'Orsay and the Musée Rodin. The city is also known for its cuisine, with many famous restaurants and cafes serving up delicious French dishes. Paris is a city that is constantly changing and evolving, with\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  likely to be characterized by rapid advancements in areas such as machine learning, natural language processing, and computer vision. Some potential trends include:\n",
      "\n",
      "1. Increased integration of AI into everyday life: AI is already being integrated into many aspects of our lives, from self-driving cars to smart home devices. As AI becomes more integrated into our daily lives, we can expect to see even more widespread adoption.\n",
      "\n",
      "2. AI becoming more autonomous: As AI becomes more integrated into our daily lives, we can expect to see more autonomous vehicles and robots becoming more prevalent. This could lead to a more efficient and safer way of transportation, but it could also lead to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    merged_output = stream_and_merge(llm, prompt, sampling_params)\n",
    "    print(\"Generated text:\", merged_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:03:11.646650Z",
     "iopub.status.busy": "2025-10-31T13:03:11.646504Z",
     "iopub.status.idle": "2025-10-31T13:03:11.867870Z",
     "shell.execute_reply": "2025-10-31T13:03:11.867335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous batch generation ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text:  [Your Name], and I'm a [Your Profession] with [Your Main Occupation]. I'm a [Your Profession] by training, having [Number of Years In Professional Field]. I was born in [Your Place of Birth] and have spent most of my life in [Your Country/State/Region]. What excites me the most is [What Excites You Most About Your Profession/Interest], and I strive to [What I Want To Do Next in Your Career]. I'm a [Your Profession] with [Your Main Occupation], and I'm a [Your Profession] by training, having [Number of Years In Professional\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text:  Paris, which is known for its iconic Eiffel Tower, exquisite museums, and beautiful gardens. France’s capital city is also home to the Louvre Museum, the largest and most famous art museum in the world, and the Notre-Dame Cathedral, a UNESCO World Heritage site. Paris has a rich history and is known for its art, architecture, and cuisine. Its population is around 2.3 million, making it the most populous city in France. Despite its size, Paris is an elegant and vibrant city with a vibrant nightlife, delicious food, and a growing economy. It is a city of contrasts, with a mix of\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text:  highly promising and diverse, with many different trends that could shape its development in the coming years. Here are some possible future trends in artificial intelligence:\n",
      "\n",
      "1. Increased automation: As AI technology continues to advance, we are likely to see more automation of routine tasks, such as data entry, call center operations, and routine maintenance. This could lead to increased efficiency and cost savings for businesses.\n",
      "\n",
      "2. Personalization: AI will become more personalized, allowing machines to learn and adapt to the unique needs of individuals. This could lead to more efficient and effective healthcare, personalized education, and other areas where AI can improve our lives.\n",
      "\n",
      "3. Ethical\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous batch generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    outputs = await llm.async_generate(prompts, sampling_params)\n",
    "\n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated text: {output['text']}\")\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:03:11.869298Z",
     "iopub.status.busy": "2025-10-31T13:03:11.869149Z",
     "iopub.status.idle": "2025-10-31T13:03:12.449741Z",
     "shell.execute_reply": "2025-10-31T13:03:12.449113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous streaming generation (no repeats) ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Age"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " currently"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " live"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Location"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Profession"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Passion"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " because"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Passion"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enjoy"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hobbies"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " strive"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Goals"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goals"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " daily"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " life"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " personality"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " believe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Values"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iv"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ias"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/B"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ios"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " build"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " something"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " together"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " team"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Our"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " team"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " work"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " together"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " achieve"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Team"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Goal"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cheers"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " an"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " assistant"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " helps"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " understand"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reasons"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " behind"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " communication"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " decisions"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Don"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " write"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " copies"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " speeches"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " many"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " challenges"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " French"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " capital"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " has"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " remained"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " one"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " important"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " century"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " visited"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " has"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " symbol"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " French"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " culture"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " elegance"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " E"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iff"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tower"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vre"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Museum"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tri"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " among"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " other"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " historic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sites"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Despite"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " challenges"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rising"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " inequality"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " environmental"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concerns"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " remains"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " global"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hub"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " art"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " science"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " culture"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vre"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " likely"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " characterized"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " by"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rapid"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " areas"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " such"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " speech"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " recognition"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " natural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " language"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " robotics"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " machine"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " learning"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " computer"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vision"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " intelligence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " expected"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continue"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " evolving"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " new"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " applications"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " technologies"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " developed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " address"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " challenges"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " opportunities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " presents"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Some"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " possible"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " future"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trends"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " include"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Improved"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " efficiency"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " systems"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " advanced"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " expect"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " them"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accurate"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " efficient"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " at"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " performing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tasks"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " were"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " previously"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " impossible"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " could"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lead"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wide"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " range"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " applications"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " autonomous"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vehicles"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " smart"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " devices"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " healthcare"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " already"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " being"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " improve"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous streaming generation (no repeats) ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text: \", end=\"\", flush=True)\n",
    "\n",
    "        # Replace direct calls to async_generate with our custom overlap-aware version\n",
    "        async for cleaned_chunk in async_stream_and_merge(llm, prompt, sampling_params):\n",
    "            print(cleaned_chunk, end=\"\", flush=True)\n",
    "\n",
    "        print()  # New line after each prompt\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:03:12.451351Z",
     "iopub.status.busy": "2025-10-31T13:03:12.451133Z",
     "iopub.status.idle": "2025-10-31T13:03:12.493725Z",
     "shell.execute_reply": "2025-10-31T13:03:12.493006Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
