{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs For Reasoning Models\n",
    "\n",
    "When working with reasoning models that use special tokens like `<think>...</think>` to denote reasoning sections, you might want to allow free-form text within these sections while still enforcing grammar constraints on the rest of the output.\n",
    "\n",
    "SGLang provides a feature to disable grammar restrictions within reasoning sections. This is particularly useful for models that need to perform complex reasoning steps before providing a structured output.\n",
    "\n",
    "To enable this feature, use the `--reasoning-parser` flag which decide the think_end_token, such as `</think>`, when launching the server. You can also specify the reasoning parser using the `--reasoning-parser` flag.\n",
    "\n",
    "## Supported Models\n",
    "\n",
    "Currently, SGLang supports the following reasoning models:\n",
    "- [DeepSeek R1 series](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "- [QwQ](https://huggingface.co/Qwen/QwQ-32B): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "## OpenAI Compatible API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `--grammar-backend`, `--reasoning-parser` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:13:10.104682Z",
     "iopub.status.busy": "2025-10-31T02:13:10.104555Z",
     "iopub.status.idle": "2025-10-31T02:13:49.886063Z",
     "shell.execute_reply": "2025-10-31T02:13:49.885315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:13:15] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:13:15] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:13:15] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:13:21] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-10-31 02:13:21] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-10-31 02:13:21] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:13:23] WARNING server_args.py:1148: Attention backend not explicitly specified. Use fa3 backend by default.\n",
      "[2025-10-31 02:13:23] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:13:31] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-10-31 02:13:31] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-10-31 02:13:31] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-10-31 02:13:31] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-10-31 02:13:31] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-10-31 02:13:31] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:13:33] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n",
      "[2025-10-31 02:13:33] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.55s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=47.14 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=47.14 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.58it/s]\r",
      "Capturing batches (bs=2 avail_mem=47.08 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.58it/s]\r",
      "Capturing batches (bs=1 avail_mem=47.07 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.58it/s]\r",
      "Capturing batches (bs=1 avail_mem=47.07 GB): 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --reasoning-parser deepseek-r1 --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")\n",
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "you can directly define a JSON schema or use [Pydantic](https://docs.pydantic.dev/latest/) to define and validate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:13:49.889033Z",
     "iopub.status.busy": "2025-10-31T02:13:49.888638Z",
     "iopub.status.idle": "2025-10-31T02:13:54.789903Z",
     "shell.execute_reply": "2025-10-31T02:13:54.789133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user asked for the information and population of the capital of France in JSON format. I immediately thought about what the capital is—Paris. I know Paris is both the capital and the most populous city in France, so that's a given.<br><br>Next, I considered the population. I remember that Paris has a large population, but I'm not exactly sure of the current number. I think it's around 2 million, but I'm not 100% certain. I should double-check that to make sure I provide accurate information.<br><br>I also need to structure this in JSON. JSON requires key-value pairs, so I'll need to define the keys appropriately. Maybe \"city\" for the name, \"country\" for the capital, and \"population\" for the number. I should make sure the syntax is correct, with proper commas and quotation marks.<br><br>Wait, the user might be using this data for a project or a presentation. They probably need it in a structured format that's easy to parse. JSON is a good choice because it's widely used in web applications and data interchange. I should ensure the JSON is valid and well-formatted to avoid any issues when they use it.<br><br>I also wonder if they need more details, like the administrative region or some notable landmarks. But since they specifically asked for population, I'll stick to that. Maybe I should include a note about the population figure being approximate, just in case.<br><br>Putting it all together, I'll format the JSON with the city name, country, and population. I'll make sure the keys are in English to keep it clear. Double-checking the population number is crucial to maintain credibility. I think 2,147,000 is a commonly cited figure, so I'll go with that.<br><br>Finally, I'll present the JSON in a code block so it's easy to read and copy. I should also offer further assistance in case they need more data. That way, I cover all bases and ensure the user gets exactly what they're looking for.<br><br><br>content: {<br>  \"name\": \"Paris\",<br>  \"population\": 2147000<br>}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"foo\",\n",
    "            # convert the pydantic model to json schema\n",
    "            \"schema\": CapitalInfo.model_json_schema(),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:13:54.791761Z",
     "iopub.status.busy": "2025-10-31T02:13:54.791600Z",
     "iopub.status.idle": "2025-10-31T02:13:57.117449Z",
     "shell.execute_reply": "2025-10-31T02:13:57.116578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user asked for the information and population of the capital of France in JSON format. I immediately thought about what the capital is—Paris. Then, I considered the population. I know it's a big city, but I'm not exactly sure of the current number. I remember it's over 3 million, but I'm not certain if it's 3.5 or 3.6. I should double-check that.<br><br>Next, I thought about the structure. The user wants JSON, so I need to format it correctly with keys like \"city\", \"population\", and maybe \"country\". I should make sure the syntax is correct—no typos, proper commas, and brackets. <br><br>I also considered the user's possible needs. They might be doing a project or a presentation, so providing accurate data is crucial. Maybe they're a student learning about France's capitals or someone compiling demographic data. Either way, precision is key.<br><br>I decided to look up the latest population figure. A quick search shows that as of 2023, Paris has a population of around 3,600,000. That seems right, but I should note that populations can fluctuate due to births, deaths, and migration. <br><br>Putting it all together, I structured the JSON with the city name, population, and country. I made sure the numbers were accurate and the format was correct. I also added a comment to explain the population figure, just in case the user needed more context.<br><br>Finally, I sent the response, ensuring it was clear and met the user's request. I hope this helps them with whatever they're working on!<br><br><br>content: {<br>  \"name\": \"Paris\",<br>  \"population\": 3600000<br>}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\"name\": \"foo\", \"schema\": json.loads(json_schema)},\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:13:57.119205Z",
     "iopub.status.busy": "2025-10-31T02:13:57.119046Z",
     "iopub.status.idle": "2025-10-31T02:13:58.258472Z",
     "shell.execute_reply": "2025-10-31T02:13:58.257694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user asked for the information and population of the capital of France in JSON format. I know that the capital is Paris, so I need to provide accurate data. I should check the latest population numbers to make sure it's up to date. From what I remember, Paris has a population around 2 million, but I should verify that. Also, I should include other relevant details like the administrative region, area, and some notable landmarks. I need to structure this information neatly in JSON, making it easy to read and understand. I should also ensure that the population number is correct and up-to-date, maybe cross-check it with a reliable source to avoid any mistakes. Once I have all the details, I'll format them into a JSON structure with proper keys and values. I think that's all the user needs, so I'll present it clearly.<br><br><br>content: London is the capital of France</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ebnf_grammar = \"\"\"\n",
    "root ::= city | description\n",
    "city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\n",
    "description ::= city \" is \" status\n",
    "status ::= \"the capital of \" country\n",
    "country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful geography bot.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"ebnf\": ebnf_grammar},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:13:58.260421Z",
     "iopub.status.busy": "2025-10-31T02:13:58.260134Z",
     "iopub.status.idle": "2025-10-31T02:13:58.991699Z",
     "shell.execute_reply": "2025-10-31T02:13:58.990911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user just asked, \"What is the capital of France?\" Hmm, that's a pretty straightforward question. I should make sure I provide a clear and accurate answer. Let me think, Paris is definitely the capital. But wait, is there any chance I might have mixed it up with another country? No, I'm pretty sure France's capital is Paris. Maybe I should double-check that to be certain. Yeah, Paris is where the government is located, and that's the capital. Okay, I feel confident about that. I'll just state it directly.<br><br><br>content: Paris</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"assistant\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"regex\": \"(Paris|London)\"},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:13:58.993624Z",
     "iopub.status.busy": "2025-10-31T02:13:58.993460Z",
     "iopub.status.idle": "2025-10-31T02:14:03.604887Z",
     "shell.execute_reply": "2025-10-31T02:14:03.604130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out how to get the current date and time in New York and the weather there. Let me start by understanding the user's request. They're in New York and want both the date and time, plus the weather. <br><br>First, I remember that the user provided specific functions to use. There's 'get_current_date' and 'get_current_weather'. I need to use both. Let me check the parameters for each function.<br><br>For 'get_current_date', the parameter is 'timezone', which should be a string like 'America/New_York'. That makes sense because New York is in the Eastern Time Zone, so the correct parameter value is 'America/New_York'.<br><br>Next, for 'get_current_weather', the parameters are 'city', 'state', and 'unit'. The city is 'New York', but I need the state abbreviation. New York is NY, so the state is 'NY'. The unit should be Fahrenheit since the user didn't specify, but I can double-check if that's what they want. Wait, the user didn't mention the unit, so maybe I should assume Fahrenheit or Celsius? Hmm, the example in the instructions didn't specify, but since the user is in New York, perhaps Fahrenheit is more common for weather. I'll go with 'fahrenheit' as the unit.<br><br>Now, I need to structure the function calls correctly. I have to call 'get_current_date' first with the timezone parameter. Then, I need to call 'get_current_weather' with city, state, and unit. But wait, the user wants both in one message, so I should format it correctly.<br><br>I think I should write each function call separately, but the user might want them in one message. However, the instructions say to use one function at a time, so maybe I should call them one after the other. But looking back, the user's example was a single function call, so perhaps I can structure it as two separate function calls in the same message, each within their own tags.<br><br>Wait, no, the user example showed a single function call. So perhaps I should combine both into one message, but each function call is separate. Let me think about the syntax. Each function call is within its own <function> tags. So I can write:<br><br><function=get_current_date>{{\"timezone\": \"America/New_York\"}}</function><br><function=get_current_weather>{{\"city\": \"New York\", \"state\": \"NY\", \"unit\": \"fahrenheit\"}}</function><br><br>But wait, the user's example used a single function with a JSON object. So each function call should be separate, each within their own start and end tags. That makes sense because each function is called individually.<br><br>Let me double-check the parameters. For 'get_current_date', the timezone is correct. For 'get_current_weather', city is New York, state is NY, and unit is Fahrenheit. That should provide the user with both the date and time in New York and the weather in Fahrenheit.<br><br>I think that's the correct approach. Now, I'll format the response with both function calls, each within their own tags, and include the sources as per the instructions.<br><br><br>content: <function=get_current_date>{  <br>  \"timezone\": \"America/New_York\"  <br>}</function>  <br><function=get_current_weather>{  <br>  \"city\": \"New York\",  <br>  \"state\": \"NY\",  <br>  \"unit\": \"fahrenheit\"  <br>}</function></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_get_current_weather = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\",\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tool_get_current_date = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"Get the current date and time for a given timezone\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"timezone\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The timezone to fetch the current date and time for, e.g. 'America/New_York'\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"timezone\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "schema_get_current_weather = tool_get_current_weather[\"function\"][\"parameters\"]\n",
    "schema_get_current_date = tool_get_current_date[\"function\"][\"parameters\"]\n",
    "\n",
    "\n",
    "def get_messages():\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "# Tool Instructions\n",
    "- Always execute python code in messages that you share.\n",
    "- When looking for real time information use relevant functions if available else fallback to brave_search\n",
    "You have access to the following functions:\n",
    "Use the function 'get_current_weather' to: Get the current weather in a given location\n",
    "{tool_get_current_weather[\"function\"]}\n",
    "Use the function 'get_current_date' to: Get the current date and time for a given timezone\n",
    "{tool_get_current_date[\"function\"]}\n",
    "If a you choose to call a function ONLY reply in the following format:\n",
    "<{{start_tag}}={{function_name}}>{{parameters}}{{end_tag}}\n",
    "where\n",
    "start_tag => `<function`\n",
    "parameters => a JSON dict with the function argument name as key and function argument value as value.\n",
    "end_tag => `</function>`\n",
    "Here is an example,\n",
    "<function=example_function_name>{{\"example_name\": \"example_value\"}}</function>\n",
    "Reminder:\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "- Always add your sources when using search results to answer the user query\n",
    "You are a helpful assistant.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"You are in New York. Please get the current date and time, and the weather.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "messages = get_messages()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=messages,\n",
    "    response_format={\n",
    "        \"type\": \"structural_tag\",\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structures\": [\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_weather>\",\n",
    "                \"schema\": schema_get_current_weather,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_date>\",\n",
    "                \"schema\": schema_get_current_date,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "        ],\n",
    "        \"triggers\": [\"<function=\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native API and SGLang Runtime (SRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:03.606786Z",
     "iopub.status.busy": "2025-10-31T02:14:03.606626Z",
     "iopub.status.idle": "2025-10-31T02:14:09.394330Z",
     "shell.execute_reply": "2025-10-31T02:14:09.393431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down. First, I need to identify what the capital of France is. I know that Paris is the capital, so that\\'s straightforward. \\n\\nNext, I need to find the population. I remember that Paris is a major city, so its population is quite large. I think it\\'s over 3 million, but I\\'m not exactly sure of the exact number. Maybe I should double-check that. \\n\\nWait, I recall that the population figure can vary depending on the source and the year. The user didn\\'t specify a particular year, so I should probably go with the most recent estimate. I believe the population is around 3,500,000 as of 2023. \\n\\nNow, I need to structure this information into a JSON format. JSON typically uses key-value pairs, so I\\'ll create an object with keys like \"city\", \"population\", and maybe \"country\" since the user mentioned France. \\n\\nI should make sure the keys are in English to keep it clear. The city is Paris, the population is 3,500,000, and the country is France. I\\'ll format this into a JSON object. \\n\\nI also need to present this in a way that\\'s easy to read, so I\\'ll use proper syntax with quotation marks and commas in the right places. No trailing commas to avoid errors. \\n\\nPutting it all together, the JSON should look something like this: a dictionary with the keys and the corresponding values. I\\'ll make sure to test it to ensure it\\'s valid, but since I\\'m just writing it out, I\\'ll assume it\\'s correct based on my knowledge. \\n\\nI think that\\'s all. The user just needs the information in JSON, so this should satisfy their request.\\n</think>{\\n\\n\"name\": \"Paris\",\\n\"population\": 3500000}', 'output_ids': [13, 151643, 151645, 151648, 198, 32313, 11, 773, 279, 1196, 374, 10161, 369, 279, 1995, 323, 7042, 315, 279, 6722, 315, 9625, 304, 4718, 3561, 13, 6771, 752, 1438, 419, 1495, 13, 5512, 11, 358, 1184, 311, 10542, 1128, 279, 6722, 315, 9625, 374, 13, 358, 1414, 429, 12095, 374, 279, 6722, 11, 773, 429, 594, 30339, 13, 4710, 5847, 11, 358, 1184, 311, 1477, 279, 7042, 13, 358, 6099, 429, 12095, 374, 264, 3598, 3283, 11, 773, 1181, 7042, 374, 5008, 3460, 13, 358, 1744, 432, 594, 916, 220, 18, 3526, 11, 714, 358, 2776, 537, 6896, 2704, 315, 279, 4734, 1372, 13, 10696, 358, 1265, 1990, 15934, 429, 13, 4710, 14190, 11, 358, 19091, 429, 279, 7042, 7071, 646, 13289, 11649, 389, 279, 2530, 323, 279, 1042, 13, 576, 1196, 3207, 944, 13837, 264, 3953, 1042, 11, 773, 358, 1265, 4658, 728, 448, 279, 1429, 3213, 16045, 13, 358, 4411, 279, 7042, 374, 2163, 220, 18, 11, 20, 15, 15, 11, 15, 15, 15, 438, 315, 220, 17, 15, 17, 18, 13, 4710, 7039, 11, 358, 1184, 311, 5944, 419, 1995, 1119, 264, 4718, 3561, 13, 4718, 11136, 5711, 1376, 19083, 13530, 11, 773, 358, 3278, 1855, 458, 1633, 448, 6894, 1075, 330, 8926, 497, 330, 44441, 497, 323, 7196, 330, 11141, 1, 2474, 279, 1196, 9733, 9625, 13, 4710, 40, 1265, 1281, 2704, 279, 6894, 525, 304, 6364, 311, 2506, 432, 2797, 13, 576, 3283, 374, 12095, 11, 279, 7042, 374, 220, 18, 11, 20, 15, 15, 11, 15, 15, 15, 11, 323, 279, 3146, 374, 9625, 13, 358, 3278, 3561, 419, 1119, 264, 4718, 1633, 13, 4710, 40, 1083, 1184, 311, 3042, 419, 304, 264, 1616, 429, 594, 4135, 311, 1349, 11, 773, 358, 3278, 990, 6169, 19482, 448, 54231, 15423, 323, 76602, 304, 279, 1290, 7482, 13, 2308, 27748, 76602, 311, 5648, 5975, 13, 4710, 97904, 432, 678, 3786, 11, 279, 4718, 1265, 1401, 2494, 1075, 419, 25, 264, 10997, 448, 279, 6894, 323, 279, 12159, 2750, 13, 358, 3278, 1281, 2704, 311, 1273, 432, 311, 5978, 432, 594, 2697, 11, 714, 2474, 358, 2776, 1101, 4378, 432, 700, 11, 358, 3278, 9658, 432, 594, 4396, 3118, 389, 847, 6540, 13, 4710, 40, 1744, 429, 594, 678, 13, 576, 1196, 1101, 3880, 279, 1995, 304, 4718, 11, 773, 419, 1265, 26553, 862, 1681, 624, 151649, 4257, 1, 606, 788, 330, 59604, 756, 1, 44441, 788, 220, 18, 20, 15, 15, 15, 15, 15, 92, 151643], 'meta_info': {'id': '5a158aeead7b4e6fa544302076ce09d2', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 405, 'cached_tokens': 1, 'e2e_latency': 5.1427788734436035}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down. First, I need to identify what the capital of France is. I know that Paris is the capital, so that's straightforward. <br><br>Next, I need to find the population. I remember that Paris is a major city, so its population is quite large. I think it's over 3 million, but I'm not exactly sure of the exact number. Maybe I should double-check that. <br><br>Wait, I recall that the population figure can vary depending on the source and the year. The user didn't specify a particular year, so I should probably go with the most recent estimate. I believe the population is around 3,500,000 as of 2023. <br><br>Now, I need to structure this information into a JSON format. JSON typically uses key-value pairs, so I'll create an object with keys like \"city\", \"population\", and maybe \"country\" since the user mentioned France. <br><br>I should make sure the keys are in English to keep it clear. The city is Paris, the population is 3,500,000, and the country is France. I'll format this into a JSON object. <br><br>I also need to present this in a way that's easy to read, so I'll use proper syntax with quotation marks and commas in the right places. No trailing commas to avoid errors. <br><br>Putting it all together, the JSON should look something like this: a dictionary with the keys and the corresponding values. I'll make sure to test it to ensure it's valid, but since I'm just writing it out, I'll assume it's correct based on my knowledge. <br><br>I think that's all. The user just needs the information in JSON, so this should satisfy their request.<br><br><br>content: {<br><br>\"name\": \"Paris\",<br>\"population\": 3500000}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "    },\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# Make API request\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())\n",
    "\n",
    "\n",
    "reasoing_content = response.json()[\"text\"].split(\"</think>\")[0]\n",
    "content = response.json()[\"text\"].split(\"</think>\")[1]\n",
    "print_highlight(f\"reasoing_content: {reasoing_content}\\n\\ncontent: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:09.396293Z",
     "iopub.status.busy": "2025-10-31T02:14:09.396119Z",
     "iopub.status.idle": "2025-10-31T02:14:12.656089Z",
     "shell.execute_reply": "2025-10-31T02:14:12.655428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down.\\n\\nFirst, I need to identify the capital of France. I know that Paris is the capital, so that\\'s straightforward. Now, I should find the most recent population data. I remember that the population of Paris has been growing, but I\\'m not sure of the exact number. I think it\\'s around 2 million, but I should verify that.\\n\\nI\\'ll check a reliable source, maybe the official Paris Municipality website or a recent census. Let me see, according to the 2020 census, Paris had a population of about 2,174,300. That seems accurate. I should make sure to include this number in the JSON.\\n\\nNext, I need to structure this information into a JSON format. The user wants both the general information and the population. So, I\\'ll create an object with a \"name\" field for the capital, a \"general_information\" section that includes the administrative center, area, and government department, and a \"population\" section that includes the current population and a note about the data source.\\n\\nI should also add a \"source\" field to indicate where the population data comes from, which is the 2020 census. This makes the information more transparent and trustworthy.\\n\\nPutting it all together, I\\'ll format the JSON with proper syntax, using double quotes for strings and ensuring that the keys are clear and descriptive. I\\'ll make sure there are no typos and that the JSON is valid.\\n\\nFinally, I\\'ll present the JSON in a code block so the user can easily copy and use it. I should also offer further assistance in case they need more data or have any questions.\\n</think>{\\n  \"name\": \"Paris\",\\n  \"population\": 2174300\\n}', 'output_ids': [13, 151643, 151645, 151648, 198, 32313, 11, 773, 279, 1196, 374, 10161, 369, 279, 1995, 323, 7042, 315, 279, 6722, 315, 9625, 304, 4718, 3561, 13, 6771, 752, 1438, 419, 1495, 382, 5338, 11, 358, 1184, 311, 10542, 279, 6722, 315, 9625, 13, 358, 1414, 429, 12095, 374, 279, 6722, 11, 773, 429, 594, 30339, 13, 4695, 11, 358, 1265, 1477, 279, 1429, 3213, 7042, 821, 13, 358, 6099, 429, 279, 7042, 315, 12095, 702, 1012, 7826, 11, 714, 358, 2776, 537, 2704, 315, 279, 4734, 1372, 13, 358, 1744, 432, 594, 2163, 220, 17, 3526, 11, 714, 358, 1265, 10146, 429, 382, 40, 3278, 1779, 264, 14720, 2530, 11, 7196, 279, 3946, 12095, 35703, 2719, 3910, 476, 264, 3213, 43602, 13, 6771, 752, 1490, 11, 4092, 311, 279, 220, 17, 15, 17, 15, 43602, 11, 12095, 1030, 264, 7042, 315, 911, 220, 17, 11, 16, 22, 19, 11, 18, 15, 15, 13, 2938, 4977, 13382, 13, 358, 1265, 1281, 2704, 311, 2924, 419, 1372, 304, 279, 4718, 382, 5847, 11, 358, 1184, 311, 5944, 419, 1995, 1119, 264, 4718, 3561, 13, 576, 1196, 6801, 2176, 279, 4586, 1995, 323, 279, 7042, 13, 2055, 11, 358, 3278, 1855, 458, 1633, 448, 264, 330, 606, 1, 2070, 369, 279, 6722, 11, 264, 330, 24595, 35212, 1, 3772, 429, 5646, 279, 22707, 4126, 11, 3082, 11, 323, 3033, 9292, 11, 323, 264, 330, 44441, 1, 3772, 429, 5646, 279, 1482, 7042, 323, 264, 5185, 911, 279, 821, 2530, 382, 40, 1265, 1083, 912, 264, 330, 2427, 1, 2070, 311, 13216, 1380, 279, 7042, 821, 4041, 504, 11, 892, 374, 279, 220, 17, 15, 17, 15, 43602, 13, 1096, 3643, 279, 1995, 803, 17821, 323, 55942, 382, 97904, 432, 678, 3786, 11, 358, 3278, 3561, 279, 4718, 448, 6169, 19482, 11, 1667, 1990, 17194, 369, 9069, 323, 22573, 429, 279, 6894, 525, 2797, 323, 52844, 13, 358, 3278, 1281, 2704, 1052, 525, 902, 13580, 966, 323, 429, 279, 4718, 374, 2697, 382, 23949, 11, 358, 3278, 3042, 279, 4718, 304, 264, 2038, 2504, 773, 279, 1196, 646, 6707, 2975, 323, 990, 432, 13, 358, 1265, 1083, 3010, 4623, 12994, 304, 1142, 807, 1184, 803, 821, 476, 614, 894, 4755, 624, 151649, 515, 220, 330, 606, 788, 330, 59604, 756, 220, 330, 44441, 788, 220, 17, 16, 22, 19, 18, 15, 15, 198, 92, 151643], 'meta_info': {'id': 'ff261003578647129067b26c34f0f87c', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 386, 'cached_tokens': 22, 'e2e_latency': 3.249803066253662}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# JSON\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json_schema,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:12.657779Z",
     "iopub.status.busy": "2025-10-31T02:14:12.657613Z",
     "iopub.status.idle": "2025-10-31T02:14:14.338214Z",
     "shell.execute_reply": "2025-10-31T02:14:14.337256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"\\nThe capital of France is Paris.\\n\\nThe user has provided the correct information that the capital of France is Paris. However, the user's request was to provide the information of the capital of France, which is a bit ambiguous. It could be interpreted as wanting more detailed information about Paris, such as its history, culture, landmarks, or government. Alternatively, it might mean they want to confirm that Paris is indeed the capital, which they already know. \\n\\nTo better assist the user, it would be helpful to ask for clarification. If they want more details about Paris, I can provide a comprehensive overview. If they just need confirmation, then the current response suffices. \\n\\nAdditionally, I can offer to provide more information on specific aspects of Paris if they specify, such as its history, famous landmarks, or cultural significance. This way, I can tailor my response to better meet their needs.\\n</think>Paris is the capital of France\", 'output_ids': [279, 6722, 315, 9625, 13, 198, 785, 6722, 315, 9625, 374, 12095, 382, 785, 1196, 702, 3897, 279, 4396, 1995, 429, 279, 6722, 315, 9625, 374, 12095, 13, 4354, 11, 279, 1196, 594, 1681, 572, 311, 3410, 279, 1995, 315, 279, 6722, 315, 9625, 11, 892, 374, 264, 2699, 54761, 13, 1084, 1410, 387, 32298, 438, 19211, 803, 11682, 1995, 911, 12095, 11, 1741, 438, 1181, 3840, 11, 7674, 11, 59924, 11, 476, 3033, 13, 38478, 11, 432, 2578, 3076, 807, 1366, 311, 7683, 429, 12095, 374, 12824, 279, 6722, 11, 892, 807, 2669, 1414, 13, 4710, 1249, 2664, 7789, 279, 1196, 11, 432, 1035, 387, 10950, 311, 2548, 369, 63684, 13, 1416, 807, 1366, 803, 3565, 911, 12095, 11, 358, 646, 3410, 264, 15817, 23251, 13, 1416, 807, 1101, 1184, 19539, 11, 1221, 279, 1482, 2033, 8489, 1216, 13, 4710, 49574, 11, 358, 646, 3010, 311, 3410, 803, 1995, 389, 3151, 13566, 315, 12095, 421, 807, 13837, 11, 1741, 438, 1181, 3840, 11, 11245, 59924, 11, 476, 12752, 25361, 13, 1096, 1616, 11, 358, 646, 50956, 847, 2033, 311, 2664, 3367, 862, 3880, 624, 151649, 59604, 374, 279, 6722, 315, 9625, 151643], 'meta_info': {'id': '5fabf0980c9442f980b88a4500e334ae', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 188, 'cached_tokens': 10, 'e2e_latency': 1.2750983238220215}}, {'text': \"\\nThe capital of France is Paris.\\n\\nThe user has provided the correct information that the capital of France is Paris. However, the user's request was to provide the information of the capital of France, which is a bit ambiguous. It could be interpreted as wanting more detailed information about Paris, such as its history, culture, landmarks, or government. Alternatively, it might mean they want to confirm that Paris is indeed the capital, which they already know. \\n\\nTo better assist the user, it would be helpful to ask for clarification. If they want more details about Paris, I can provide a comprehensive overview. If they just need confirmation, then the current response suffices. \\n\\nAdditionally, I can offer to provide more information on specific aspects of Paris if they specify, such as its history, famous landmarks, or cultural significance. This way, I can tailor my response to better meet their needs.\\n</think>Paris is the capital of France\", 'output_ids': [279, 6722, 315, 9625, 13, 198, 785, 6722, 315, 9625, 374, 12095, 382, 785, 1196, 702, 3897, 279, 4396, 1995, 429, 279, 6722, 315, 9625, 374, 12095, 13, 4354, 11, 279, 1196, 594, 1681, 572, 311, 3410, 279, 1995, 315, 279, 6722, 315, 9625, 11, 892, 374, 264, 2699, 54761, 13, 1084, 1410, 387, 32298, 438, 19211, 803, 11682, 1995, 911, 12095, 11, 1741, 438, 1181, 3840, 11, 7674, 11, 59924, 11, 476, 3033, 13, 38478, 11, 432, 2578, 3076, 807, 1366, 311, 7683, 429, 12095, 374, 12824, 279, 6722, 11, 892, 807, 2669, 1414, 13, 4710, 1249, 2664, 7789, 279, 1196, 11, 432, 1035, 387, 10950, 311, 2548, 369, 63684, 13, 1416, 807, 1366, 803, 3565, 911, 12095, 11, 358, 646, 3410, 264, 15817, 23251, 13, 1416, 807, 1101, 1184, 19539, 11, 1221, 279, 1482, 2033, 8489, 1216, 13, 4710, 49574, 11, 358, 646, 3010, 311, 3410, 803, 1995, 389, 3151, 13566, 315, 12095, 421, 807, 13837, 11, 1741, 438, 1181, 3840, 11, 11245, 59924, 11, 476, 12752, 25361, 13, 1096, 1616, 11, 358, 646, 50956, 847, 2033, 311, 2664, 3367, 862, 3880, 624, 151649, 59604, 374, 279, 6722, 315, 9625, 151643], 'meta_info': {'id': 'c4f93caceaee49daa6db1438fad61c2f', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 188, 'cached_tokens': 10, 'e2e_latency': 1.2751185894012451}}, {'text': \"\\nThe capital of France is Paris. It is located in the northern part of the country, along the Seine River. Paris is known for its rich history, art, and landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. It is a major city in France and has a significant cultural and economic impact.\\n\\nThe capital of France is Paris. It is located in the northern part of the country, along the Seine River. Paris is known for its rich history, art, and landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Damme Cathedral. It is a major city in France and has a significant cultural and economic impact.\\n\\nPlease provide the information in a clear and concise manner, using bullet points for the location and key landmarks.\\n\\nSure, here's the information about the capital of France presented in a clear and concise manner with bullet points:\\n\\n- **Capital of France**: Paris\\n- **Location**: Northern part of France, along the Seine River\\n- **Key Landmarks**:\\n  - Eiffel Tower\\n  - Louvre Museum\\n  - Notre-Dame Cathedral\\n\\nThis format organizes the information neatly, making it easy to read and understand.\", 'output_ids': [279, 6722, 315, 9625, 13, 198, 785, 6722, 315, 9625, 374, 12095, 13, 1084, 374, 7407, 304, 279, 18172, 949, 315, 279, 3146, 11, 3156, 279, 1345, 482, 10948, 13, 12095, 374, 3881, 369, 1181, 9080, 3840, 11, 1947, 11, 323, 59924, 1741, 438, 279, 468, 3092, 301, 21938, 11, 279, 9729, 48506, 16328, 11, 323, 43464, 9420, 373, 56729, 13, 1084, 374, 264, 3598, 3283, 304, 9625, 323, 702, 264, 5089, 12752, 323, 6955, 5421, 382, 785, 6722, 315, 9625, 374, 12095, 13, 1084, 374, 7407, 304, 279, 18172, 949, 315, 279, 3146, 11, 3156, 279, 1345, 482, 10948, 13, 12095, 374, 3881, 369, 1181, 9080, 3840, 11, 1947, 11, 323, 59924, 1741, 438, 279, 468, 3092, 301, 21938, 11, 279, 9729, 48506, 16328, 11, 323, 43464, 9420, 309, 2660, 56729, 13, 1084, 374, 264, 3598, 3283, 304, 9625, 323, 702, 264, 5089, 12752, 323, 6955, 5421, 382, 5501, 3410, 279, 1995, 304, 264, 2797, 323, 63594, 11566, 11, 1667, 17432, 3501, 369, 279, 3728, 323, 1376, 59924, 382, 39814, 11, 1588, 594, 279, 1995, 911, 279, 6722, 315, 9625, 10449, 304, 264, 2797, 323, 63594, 11566, 448, 17432, 3501, 1447, 12, 3070, 63593, 315, 9625, 95518, 12095, 198, 12, 3070, 4707, 95518, 16926, 949, 315, 9625, 11, 3156, 279, 1345, 482, 10948, 198, 12, 3070, 1592, 11426, 15544, 334, 510, 220, 481, 468, 3092, 301, 21938, 198, 220, 481, 9729, 48506, 16328, 198, 220, 481, 43464, 9420, 373, 56729, 271, 1986, 3561, 2872, 4756, 279, 1995, 62166, 11, 3259, 432, 4135, 311, 1349, 323, 3535, 13, 151643], 'meta_info': {'id': '0dd6432cfa804d1993f5cea6ae8af9be', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 254, 'cached_tokens': 10, 'e2e_latency': 1.6719348430633545}}]\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Give me the information of the capital of France.\",\n",
    "        \"sampling_params\": {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"temperature\": 0,\n",
    "            \"n\": 3,\n",
    "            \"ebnf\": (\n",
    "                \"root ::= city | description\\n\"\n",
    "                'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "                'description ::= city \" is \" status\\n'\n",
    "                'status ::= \"the capital of \" country\\n'\n",
    "                'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "            ),\n",
    "        },\n",
    "        \"stream\": False,\n",
    "        \"return_logprob\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:14.340215Z",
     "iopub.status.busy": "2025-10-31T02:14:14.340037Z",
     "iopub.status.idle": "2025-10-31T02:14:26.728217Z",
     "shell.execute_reply": "2025-10-31T02:14:26.727297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' France, and the \\n\\\\( n \\\\)  \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\(', 'output_ids': [59604, 374, 279, 6722, 315, 9625, 11, 323, 279, 220, 198, 44292, 308, 1124, 8, 220, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767, 326, 1124, 8, 17767, 296, 1124, 8, 17767, 595, 1124, 8, 17767], 'meta_info': {'id': '1fb8698690a44319b088227fa6b80218', 'finish_reason': {'type': 'length', 'length': 2048}, 'prompt_tokens': 6, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 2048, 'cached_tokens': 1, 'e2e_latency': 12.378073930740356}}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Paris is the capital of\",\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"regex\": \"(France|England)\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:26.730177Z",
     "iopub.status.busy": "2025-10-31T02:14:26.730000Z",
     "iopub.status.idle": "2025-10-31T02:14:30.519028Z",
     "shell.execute_reply": "2025-10-31T02:14:30.518220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Okay, so I need to figure out the population of the capital of France. First, I know that the capital of France is Paris. Now, I\\'m trying to remember or find out the most recent population figure for Paris. I think it\\'s around 3 million? But I\\'m not entirely sure if that\\'s up to date. Maybe I should check some sources to confirm. \\n\\nHmm, I recall that major cities in Europe tend to have significant populations. Paris is one of the largest urban areas in the world, so having a population in the tens of millions sounds plausible. Wait, did I mean 3.5 million or 3 million? I\\'m a bit confused because online sources might differ. Let me try to recall a specific figure. I think it was approximately 2 million or maybe 3 million. To be safe, I should probably state both possibilities or find a reliable source to confirm. \\n\\nWait, maybe I should think about other European capitals. For example, Germany\\'s capital, Berlin, has a population around 3.7 million, and maybe Paris is a bit less than that. So, perhaps 2.2 million? But I\\'m not confident in that exact number. Maybe rounding it to 3 million would make it easier to understand, acknowledging that the precise figure can vary. Alternatively, being more accurate, I think it\\'s closer to 2.2 million. \\n\\nI guess the best approach is to look up the most recent data. If I were to write this in JSON format, I should include \"capital\" as \"Paris\" and \"population\" as the approximate number. But since I\\'m not 100% sure, I might also add a note that this figure might change slightly depending on the source or the year. However, the user didn\\'t ask for a note, just the information, so perhaps just providing the approximate figure in JSON is sufficient.\\n\\nWait, another thought: sometimes people refer to the urban area of Paris, which includes surrounding suburbs, and maybe that\\'s higher than the city proper. If the user is asking for the city\\'s population versus the metro area, it could differ. But I think the user is asking about the city, so maybe 2.2 million is accurate. \\n\\nTo sum up, I\\'m leaning towards Paris having a population of around 2.2 million and will represent that in the JSON structure. I hope that\\'s correct. If not, the figure could be adjusted, but for now, I think 2.2 million is a reasonable approximation.\\n</think>\\n\\n```json\\n{\\n  \"capital\": \"Paris\",\\n  \"population\": 2200000\\n}\\n```', 'output_ids': [13, 151643, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 7071, 700, 279, 7042, 315, 279, 6722, 315, 9625, 13, 5512, 11, 358, 1414, 429, 279, 6722, 315, 9625, 374, 12095, 13, 4695, 11, 358, 2776, 4460, 311, 6099, 476, 1477, 700, 279, 1429, 3213, 7042, 7071, 369, 12095, 13, 358, 1744, 432, 594, 2163, 220, 18, 3526, 30, 1988, 358, 2776, 537, 11368, 2704, 421, 429, 594, 705, 311, 2400, 13, 10696, 358, 1265, 1779, 1045, 8173, 311, 7683, 13, 4710, 80022, 11, 358, 19091, 429, 3598, 9720, 304, 4505, 8376, 311, 614, 5089, 21910, 13, 12095, 374, 825, 315, 279, 7772, 15662, 5671, 304, 279, 1879, 11, 773, 3432, 264, 7042, 304, 279, 22008, 315, 11728, 10362, 49334, 13, 13824, 11, 1521, 358, 3076, 220, 18, 13, 20, 3526, 476, 220, 18, 3526, 30, 358, 2776, 264, 2699, 21815, 1576, 2860, 8173, 2578, 1745, 13, 6771, 752, 1430, 311, 19091, 264, 3151, 7071, 13, 358, 1744, 432, 572, 13187, 220, 17, 3526, 476, 7196, 220, 18, 3526, 13, 2014, 387, 6092, 11, 358, 1265, 4658, 1584, 2176, 23607, 476, 1477, 264, 14720, 2530, 311, 7683, 13, 4710, 14190, 11, 7196, 358, 1265, 1744, 911, 1008, 7513, 92999, 13, 1752, 3110, 11, 9856, 594, 6722, 11, 19846, 11, 702, 264, 7042, 2163, 220, 18, 13, 22, 3526, 11, 323, 7196, 12095, 374, 264, 2699, 2686, 1091, 429, 13, 2055, 11, 8365, 220, 17, 13, 17, 3526, 30, 1988, 358, 2776, 537, 16506, 304, 429, 4734, 1372, 13, 10696, 51562, 432, 311, 220, 18, 3526, 1035, 1281, 432, 8661, 311, 3535, 11, 60608, 429, 279, 23560, 7071, 646, 13289, 13, 38478, 11, 1660, 803, 13382, 11, 358, 1744, 432, 594, 12128, 311, 220, 17, 13, 17, 3526, 13, 4710, 40, 7942, 279, 1850, 5486, 374, 311, 1401, 705, 279, 1429, 3213, 821, 13, 1416, 358, 1033, 311, 3270, 419, 304, 4718, 3561, 11, 358, 1265, 2924, 330, 65063, 1, 438, 330, 59604, 1, 323, 330, 44441, 1, 438, 279, 44868, 1372, 13, 1988, 2474, 358, 2776, 537, 220, 16, 15, 15, 4, 2704, 11, 358, 2578, 1083, 912, 264, 5185, 429, 419, 7071, 2578, 2297, 10078, 11649, 389, 279, 2530, 476, 279, 1042, 13, 4354, 11, 279, 1196, 3207, 944, 2548, 369, 264, 5185, 11, 1101, 279, 1995, 11, 773, 8365, 1101, 8241, 279, 44868, 7071, 304, 4718, 374, 14016, 382, 14190, 11, 2441, 3381, 25, 7025, 1251, 8300, 311, 279, 15662, 3082, 315, 12095, 11, 892, 5646, 14590, 46913, 11, 323, 7196, 429, 594, 5080, 1091, 279, 3283, 6169, 13, 1416, 279, 1196, 374, 10161, 369, 279, 3283, 594, 7042, 19041, 279, 33482, 3082, 11, 432, 1410, 1745, 13, 1988, 358, 1744, 279, 1196, 374, 10161, 911, 279, 3283, 11, 773, 7196, 220, 17, 13, 17, 3526, 374, 13382, 13, 4710, 1249, 2629, 705, 11, 358, 2776, 48348, 6974, 12095, 3432, 264, 7042, 315, 2163, 220, 17, 13, 17, 3526, 323, 686, 4009, 429, 304, 279, 4718, 5944, 13, 358, 3900, 429, 594, 4396, 13, 1416, 537, 11, 279, 7071, 1410, 387, 23368, 11, 714, 369, 1431, 11, 358, 1744, 220, 17, 13, 17, 3526, 374, 264, 13276, 56204, 624, 151649, 271, 73594, 2236, 198, 515, 220, 330, 65063, 788, 330, 59604, 756, 220, 330, 44441, 788, 220, 17, 17, 15, 15, 15, 15, 15, 198, 532, 73594, 151643], 'meta_info': {'id': '77edd526ab30497f8a9e871cac6507ed', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 546, 'cached_tokens': 22, 'e2e_latency': 3.777708053588867}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "payload = {\n",
    "    \"text\": text,\n",
    "    \"sampling_params\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structural_tag\": json.dumps(\n",
    "            {\n",
    "                \"type\": \"structural_tag\",\n",
    "                \"structures\": [\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_weather>\",\n",
    "                        \"schema\": schema_get_current_weather,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_date>\",\n",
    "                        \"schema\": schema_get_current_date,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                ],\n",
    "                \"triggers\": [\"<function=\"],\n",
    "            }\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "response = requests.post(f\"http://localhost:{port}/generate\", json=payload)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:30.520848Z",
     "iopub.status.busy": "2025-10-31T02:14:30.520685Z",
     "iopub.status.idle": "2025-10-31T02:14:30.569552Z",
     "shell.execute_reply": "2025-10-31T02:14:30.568769Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:30.571850Z",
     "iopub.status.busy": "2025-10-31T02:14:30.571675Z",
     "iopub.status.idle": "2025-10-31T02:14:51.145831Z",
     "shell.execute_reply": "2025-10-31T02:14:51.144932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:14:30] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:14:32] WARNING server_args.py:1148: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:14:32] INFO engine.py:124: server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.835, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=366328264, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='deepseek-r1', tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:14:40] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-10-31 02:14:40] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-10-31 02:14:40] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-31 02:14:41] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.63s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.48s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.51s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=44.14 GB):   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=128 avail_mem=44.14 GB):   5%|▌         | 1/20 [00:00<00:07,  2.50it/s]\r",
      "Capturing batches (bs=120 avail_mem=43.31 GB):   5%|▌         | 1/20 [00:00<00:07,  2.50it/s]\r",
      "Capturing batches (bs=120 avail_mem=43.31 GB):  10%|█         | 2/20 [00:00<00:04,  4.41it/s]\r",
      "Capturing batches (bs=112 avail_mem=43.31 GB):  10%|█         | 2/20 [00:00<00:04,  4.41it/s]\r",
      "Capturing batches (bs=104 avail_mem=43.30 GB):  10%|█         | 2/20 [00:00<00:04,  4.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=104 avail_mem=43.30 GB):  20%|██        | 4/20 [00:00<00:02,  7.85it/s]\r",
      "Capturing batches (bs=96 avail_mem=43.30 GB):  20%|██        | 4/20 [00:00<00:02,  7.85it/s] \r",
      "Capturing batches (bs=88 avail_mem=43.21 GB):  20%|██        | 4/20 [00:00<00:02,  7.85it/s]\r",
      "Capturing batches (bs=88 avail_mem=43.21 GB):  30%|███       | 6/20 [00:00<00:01,  9.91it/s]\r",
      "Capturing batches (bs=80 avail_mem=43.17 GB):  30%|███       | 6/20 [00:00<00:01,  9.91it/s]\r",
      "Capturing batches (bs=72 avail_mem=43.16 GB):  30%|███       | 6/20 [00:00<00:01,  9.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=72 avail_mem=43.16 GB):  40%|████      | 8/20 [00:00<00:00, 12.33it/s]\r",
      "Capturing batches (bs=64 avail_mem=42.99 GB):  40%|████      | 8/20 [00:00<00:00, 12.33it/s]\r",
      "Capturing batches (bs=56 avail_mem=42.94 GB):  40%|████      | 8/20 [00:00<00:00, 12.33it/s]\r",
      "Capturing batches (bs=48 avail_mem=42.87 GB):  40%|████      | 8/20 [00:00<00:00, 12.33it/s]\r",
      "Capturing batches (bs=48 avail_mem=42.87 GB):  55%|█████▌    | 11/20 [00:01<00:00, 15.03it/s]\r",
      "Capturing batches (bs=40 avail_mem=42.86 GB):  55%|█████▌    | 11/20 [00:01<00:00, 15.03it/s]\r",
      "Capturing batches (bs=32 avail_mem=42.85 GB):  55%|█████▌    | 11/20 [00:01<00:00, 15.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=24 avail_mem=42.85 GB):  55%|█████▌    | 11/20 [00:01<00:00, 15.03it/s]\r",
      "Capturing batches (bs=24 avail_mem=42.85 GB):  70%|███████   | 14/20 [00:01<00:00, 17.09it/s]\r",
      "Capturing batches (bs=16 avail_mem=42.84 GB):  70%|███████   | 14/20 [00:01<00:00, 17.09it/s]\r",
      "Capturing batches (bs=12 avail_mem=42.84 GB):  70%|███████   | 14/20 [00:01<00:00, 17.09it/s]\r",
      "Capturing batches (bs=12 avail_mem=42.84 GB):  80%|████████  | 16/20 [00:01<00:00, 16.85it/s]\r",
      "Capturing batches (bs=8 avail_mem=42.83 GB):  80%|████████  | 16/20 [00:01<00:00, 16.85it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=42.83 GB):  80%|████████  | 16/20 [00:01<00:00, 16.85it/s]\r",
      "Capturing batches (bs=2 avail_mem=42.83 GB):  80%|████████  | 16/20 [00:01<00:00, 16.85it/s]\r",
      "Capturing batches (bs=2 avail_mem=42.83 GB):  95%|█████████▌| 19/20 [00:01<00:00, 19.32it/s]\r",
      "Capturing batches (bs=1 avail_mem=42.82 GB):  95%|█████████▌| 19/20 [00:01<00:00, 19.32it/s]\r",
      "Capturing batches (bs=1 avail_mem=42.82 GB): 100%|██████████| 20/20 [00:01<00:00, 13.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import sglang as sgl\n",
    "\n",
    "llm = sgl.Engine(\n",
    "    model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    reasoning_parser=\"deepseek-r1\",\n",
    "    grammar_backend=\"xgrammar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:51.148464Z",
     "iopub.status.busy": "2025-10-31T02:14:51.147968Z",
     "iopub.status.idle": "2025-10-31T02:14:55.429997Z",
     "shell.execute_reply": "2025-10-31T02:14:55.429238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of China, Beijing, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Beijing\",\n",
      "  \"capital\": \"Yes\",\n",
      "  \"population\": \"Over 30 million\",\n",
      "  \"founded\": \"1248\",\n",
      "  \"Nickname\": \"The Heaven on Earth\",\n",
      "  \"Location\": \"Northern China\",\n",
      "  \"OfficialLanguages\": [\n",
      "    \"Mandarin Chinese\",\n",
      "    \"Bingyuan Chinese\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\",\n",
      "    \"Yugou\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\"\n",
      "  ],\n",
      "  \"KeySights\": [\n",
      "    \"The Great Wall of China\",\n",
      "    \"Forbidden City\",\n",
      "    \"Tiananmen Square\",\n",
      "    \"Beijing Museum\",\n",
      "    \"Yuanmingyuan\"\n",
      "  ],\n",
      "  \"Climate\": \"Temperate\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need anything else!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of France, Paris, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"country\": \"France\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 48.8566,\n",
      "    \"longitude\": 2.3522\n",
      "  },\n",
      "  \"founded\": \"1340\",\n",
      "  \"population\": \"9.7 million\",\n",
      "  \"area\": \"105.5 square kilometers\",\n",
      "  \"WX\": {\n",
      "    \"averageTemperature\": \"12°C\",\n",
      "    \"precipitation\": \"540 mm/year\"\n",
      "  },\n",
      "  \"landmarks\": [\n",
      "    \"Eiffel Tower\",\n",
      "    \"Notre-Dame Cathedral\",\n",
      "    \"Louvre Museum\",\n",
      "    \"Palace of Versailles\"\n",
      "  ],\n",
      "  \"features\": [\n",
      "    \"Seine River\",\n",
      "    \"Eiffel Tower\",\n",
      "    \"Le Marais District\",\n",
      "    \"Château de la Défense\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of Ireland in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": \"Dublin\",\n",
      "  \"official_name\": \"Dublin City\",\n",
      "  \"region\": \"Dublin\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 53.3489,\n",
      "    \"longitude\": -6.2009\n",
      "  },\n",
      "  \"founded\": \"1241\",\n",
      "  \"population\": 1,234,567,\n",
      "  \"area\": {\n",
      "    \"total\": 123.45,\n",
      "    \"land\": 112.34,\n",
      "    \"water\": 11.11\n",
      "  },\n",
      "  \"climate\": \" temperate\",\n",
      "  \"key_features\": [\n",
      "    \"City Walls\",\n",
      "    \"Trinity College\",\n",
      "    \"Leaving Certificate\",\n",
      "    \"St. Stephen's Cathedral\",\n",
      "    \"Glynn Bridge\"\n",
      "  ],\n",
      "  \"tourism\": [\n",
      "    \"The GAA\",\n",
      "    \"The National Library of Ireland\",\n",
      "    \"The University of Dublin\",\n",
      "    \"The Phoenix Park\",\n",
      "    \"The SSE St. Patrick's Cathedral Quarter\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any adjustments!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:55.432269Z",
     "iopub.status.busy": "2025-10-31T02:14:55.432089Z",
     "iopub.status.idle": "2025-10-31T02:14:56.906314Z",
     "shell.execute_reply": "2025-10-31T02:14:56.905533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of China, Beijing, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Beijing\",\n",
      "  \"capital\": \"Yes\",\n",
      "  \"population\": \"Over 30 million\",\n",
      "  \"founded\": \"1248\",\n",
      "  \"Nickname\": \"The Heaven on Earth\",\n",
      "  \"Location\": \"Northern China\",\n",
      "  \"OfficialLanguages\": [\n",
      "    \"Mandarin Chinese\",\n",
      "    \"Bingyuan Chinese\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\",\n",
      "    \"Yugou\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\"\n",
      "  ],\n",
      "  \"KeySights\": [\n",
      "    \"The Great Wall of China\",\n",
      "    \"Forbidden City\",\n",
      "    \"Tiananmen Square\",\n",
      "    \"Beijing Museum\",\n",
      "    \"Yuanmingyuan\"\n",
      "  ],\n",
      "  \"Climate\": \"Temperate\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need anything else!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of France, Paris, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"country\": \"France\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 48.8566,\n",
      "    \"longitude\": 2.3522\n",
      "  },\n",
      "  \"founded\": \"1340\",\n",
      "  \"population\": \"9.7 million\",\n",
      "  \"area\": \"105.5 square kilometers\",\n",
      "  \"WX\": {\n",
      "    \"averageTemperature\": \"12°C\",\n",
      "    \"precipitation\": \"590 mm/year\"\n",
      "  },\n",
      "  \"landmarks\": [\n",
      "    \"Eiffel Tower\",\n",
      "    \"Notre-Dame Cathedral\",\n",
      "    \"Louvre Museum\",\n",
      "    \"Palace of Versailles\",\n",
      "    \"Le Marais District\"\n",
      "  ],\n",
      "  \"features\": [\n",
      "    \"Seine River\",\n",
      "    \"Eiffel Tower\",\n",
      "    \"Le Grand Parc\",\n",
      "    \"Javel Market\",\n",
      "    \"Château de la Loire\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you'd like any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of Ireland in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": \"Dublin\",\n",
      "  \"official_name\": \"Dublin City\",\n",
      "  \"region\": \"Dublin\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 53.3489,\n",
      "    \"longitude\": -6.2009\n",
      "  },\n",
      "  \"founded\": \"1241\",\n",
      "  \"population\": 1,234,567,\n",
      "  \"area\": 123.45 km²,\n",
      "  \"elevation\": 10 meters\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other details!\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "sampling_params = {\"temperature\": 0, \"max_new_tokens\": 2048, \"json_schema\": json_schema}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:56.908855Z",
     "iopub.status.busy": "2025-10-31T02:14:56.908507Z",
     "iopub.status.idle": "2025-10-31T02:14:57.756256Z",
     "shell.execute_reply": "2025-10-31T02:14:57.755222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of France.\n",
      "Generated text: \n",
      "The capital of France is Paris.\n",
      "\n",
      "That's all.\n",
      "\n",
      "Wait, maybe I should elaborate a bit more about Paris to give a more comprehensive answer.\n",
      "\n",
      "Okay, Paris is the largest city in France and has been its capital since the early Middle Ages. It's located in the northern part of the country, along the Seine River. The city is known for its rich history, cultural landmarks, and serves as the heart of political, economic, and cultural activities in France. Paris is home to famous attractions like the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Bibliothèque nationale de France\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Germany.\n",
      "Generated text: \n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "So, in this sentence, the word \"capital\" is used twice. I want to replace both instances with a synonym. The first \"capital\" refers to the capital city, and the second \"capital\" refers to the capital letter. So, I need a word that can replace the first meaning (capital city) but not the second (capital letter). Is there such a word?\n",
      "\n",
      "Yes, the word \"aN\" can replace the first instance, referring to Berlin, and \"B\" can replace the second, as in the capital letter. But I want\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Italy.\n",
      "Generated text: \n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Step-by-step explanation:\n",
      "The capital of a country is its official seat of government and is where the president or prime minister is typically located. Italy's capital is Rome, known for its rich history, historical landmarks, and cultural significance. Rome is situated in central Italy and serves as the political, economic, and cultural heart of the country.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of France.\",\n",
    "    \"Give me the information of the capital of Germany.\",\n",
    "    \"Give me the information of the capital of Italy.\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"ebnf\": (\n",
    "        \"root ::= city | description\\n\"\n",
    "        'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "        'description ::= city \" is \" status\\n'\n",
    "        'status ::= \"the capital of \" country\\n'\n",
    "        'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "    ),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:57.758377Z",
     "iopub.status.busy": "2025-10-31T02:14:57.758192Z",
     "iopub.status.idle": "2025-10-31T02:14:58.604632Z",
     "shell.execute_reply": "2025-10-31T02:14:58.603764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Please provide information about London as a major global city:\n",
      "Generated text:  Its location, capital, major industries, and cultural significance.\n",
      "6-7 sentences.\n",
      "The information should be presented in a clear, organized manner.\n",
      "Alright, the user has asked for information about London as a major global city. They specified to include its location, capital, major industries, and cultural significance, all in 6-7 sentences. They also want it presented clearly and organized.\n",
      "\n",
      "First, I should start by identifying London's location. It's in England, United Kingdom, straddling the River Thames. That's straightforward.\n",
      "\n",
      "Next, capital part. London is the capital of the UK and is a global metropolis. It\n",
      "===============================\n",
      "Prompt: Please provide information about Paris as a major global city:\n",
      "Generated text:  its location, major attractions, famous landmarks, and the history that makes it a significant city.\n",
      "\n",
      "2. Discuss the role of Paris in the history of mathematics. Include information about notable mathematicians from Paris, the establishment and purpose of significant mathematical institutions in Paris, and the contributions of Parisian mathematicians to the field of mathematics.\n",
      "\n",
      "3. Focus on the role of mathematics in the Paris Agreement. Describe the mathematical models and calculations used in creating and implementing the Paris Agreement, as well as the role of mathematical research in addressing climate change.\n",
      "\n",
      "4. Analyze the future of mathematics in Paris. Discuss how Paris will continue to be a center for\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Please provide information about London as a major global city:\",\n",
    "    \"Please provide information about Paris as a major global city:\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"regex\": \"(France|England)\"}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:14:58.606842Z",
     "iopub.status.busy": "2025-10-31T02:14:58.606662Z",
     "iopub.status.idle": "2025-10-31T02:15:01.983435Z",
     "shell.execute_reply": "2025-10-31T02:15:01.982527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: <｜begin▁of▁sentence｜><｜Assistant｜>Give me the information and population of the capital of France in the JSON format.<｜end▁of▁sentence｜><｜Assistant｜><think>\n",
      "\n",
      "Generated text: Alright, the user is asking for the information and population of the capital of France in JSON format. Let me break this down.\n",
      "\n",
      "First, I need to determine what exactly they're looking for. They mentioned the capital, so I should identify which city that is. Paris is definitely the capital of France.\n",
      "\n",
      "Next, I should figure out the key details they want. They asked for information and population, so I'll include general information about the city and its population data. It's important to note that population numbers can change over time, so providing both the current figure and the approximate range might be helpful.\n",
      "\n",
      "I should also consider the date of the population count. As of 2023, Paris has a population around 2.1 million. Including the approximate range gives a sense of the possible variation.\n",
      "\n",
      "Now, structuring this into JSON format. I'll need to create an object with a \"name\" field for the city, a \"general_info\" section for broader details, and a \"population\" object that includes both the exact figure and an approximate range with a note about the data source.\n",
      "\n",
      "I should make sure the JSON is properly formatted, with correct syntax and indentation for readability. Using \"UTF-8\" encoding is standard, so I'll include that in the meta tag.\n",
      "\n",
      "I wonder if the user might need this for a project or a presentation. Providing clear and concise information in a structured format like JSON could be very useful for them. Maybe they'll use this data for mapping, research, or creating reports.\n",
      "\n",
      "Also, I should double-check the population numbers to ensure accuracy. Sometimes estimates can vary, so giving a range or noting that the figure might change could be beneficial.\n",
      "\n",
      "Overall, the response should be straightforward, informative, and neatly organized in JSON to meet the user's needs effectively.\n",
      "</think>\n",
      "\n",
      "Here is the information and population of the capital of France (Paris) in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"general_info\": {\n",
      "    \"capital\": \"Yes\",\n",
      "    \"country\": \"France\",\n",
      "    \"region\": \"Ile-de-France\",\n",
      "    \"area\": \"12.516 km²\",\n",
      "    \"population\": {\n",
      "      \"exact\": \"2,140,526\",\n",
      "      \"approximate\": {\n",
      "        \"min\": \"2,100,000\",\n",
      "        \"max\": \"2,200,000\",\n",
      "        \"year\": \"2023\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need further details!\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "prompts = [text]\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"structural_tag\": json.dumps(\n",
    "        {\n",
    "            \"type\": \"structural_tag\",\n",
    "            \"structures\": [\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_weather>\",\n",
    "                    \"schema\": schema_get_current_weather,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_date>\",\n",
    "                    \"schema\": schema_get_current_date,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "            ],\n",
    "            \"triggers\": [\"<function=\"],\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T02:15:01.985426Z",
     "iopub.status.busy": "2025-10-31T02:15:01.985249Z",
     "iopub.status.idle": "2025-10-31T02:15:02.026835Z",
     "shell.execute_reply": "2025-10-31T02:15:02.025795Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
