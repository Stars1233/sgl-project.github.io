
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>SGLang Frontend Language &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=68368d77"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'references/frontend/frontend_tutorial';</script>
    <link rel="icon" href="../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Choices Methods in SGLang" href="choices_methods.html" />
    <link rel="prev" title="Frontend Language" href="frontend_index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Aug 17, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../get_started/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic Usage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/openai_api.html">OpenAI-Compatible APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/deepseek.html">DeepSeek Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/gpt_oss.html">GPT OSS Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basic_usage/llama4.html">Llama4 Usage</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/function_calling.html">Tool and Function Calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/lora.html">LoRA Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/pd_disaggregation.html">PD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/vlm_query.html">Query Vision Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/router.html">SGLang Router</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/observability.html">Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_features/attention_backend.html">Attention Backend</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/generative_models.html">Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/multimodal_language_models.html">Multimodal Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/embedding_models.html">Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/reward_models.html">Reward Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/rerank_models.html">Rerank Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/support_new_models.html">How to Support New Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/transformers_fallback.html">Transformers fallback in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supported_models/modelscope.html">Use Models From ModelScope</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hardware Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../platforms/amd_gpu.html">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platforms/blackwell_gpu.html">Blackwell GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platforms/cpu_server.html">CPU Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platforms/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platforms/nvidia_jetson.html">NVIDIA Jetson Orin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platforms/ascend_npu.html">SGLang on Ascend NPUs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/development_guide_using_docker.html">Development Guide Using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/benchmark_and_profiling.html">Benchmark and Profiling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Troubleshooting and Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environment_variables.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../production_metrics.html">Production Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi_node_deployment/multi_node_index.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="frontend_index.html">Frontend Language</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">SGLang Frontend Language</a></li>
<li class="toctree-l2"><a class="reference internal" href="choices_methods.html">Choices Methods in SGLang</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../learn_more.html">Learn more</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/references/frontend/frontend_tutorial.ipynb?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/references/frontend/frontend_tutorial.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Freferences/frontend/frontend_tutorial.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/references/frontend/frontend_tutorial.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>SGLang Frontend Language</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Basic-Usage">Basic Usage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-turn-Dialog">Multi-turn Dialog</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Control-flow">Control flow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Parallelism">Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Constrained-Decoding">Constrained Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Batching">Batching</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Streaming">Streaming</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Complex-Prompts">Complex Prompts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-modal-Generation">Multi-modal Generation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <style>
    .output_area.stderr, .output_area.stdout {
        color: #d3d3d3 !important; /* light gray */
    }
</style><section id="SGLang-Frontend-Language">
<h1>SGLang Frontend Language<a class="headerlink" href="#SGLang-Frontend-Language" title="Link to this heading">#</a></h1>
<p>SGLang frontend language can be used to define simple and easy prompts in a convenient, structured way.</p>
<section id="Launch-A-Server">
<h2>Launch A Server<a class="headerlink" href="#Launch-A-Server" title="Link to this heading">#</a></h2>
<p>Launch the server in your terminal and wait for it to initialize.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">assistant_begin</span><span class="p">,</span> <span class="n">assistant_end</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">assistant</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">gen</span><span class="p">,</span> <span class="n">system</span><span class="p">,</span> <span class="n">user</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">RuntimeEndpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.lang.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_default_backend</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.test.doc_patch</span><span class="w"> </span><span class="kn">import</span> <span class="n">launch_server_cmd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">print_highlight</span><span class="p">,</span> <span class="n">terminate_process</span><span class="p">,</span> <span class="n">wait_for_server</span>

<span class="n">server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
    <span class="s2">&quot;python -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --host 0.0.0.0&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Server started on http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
W0817 06:51:56.349000 921164 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
W0817 06:51:56.349000 921164 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures.
[2025-08-17 06:51:57] server_args=ServerArgs(model_path=&#39;Qwen/Qwen2.5-7B-Instruct&#39;, tokenizer_path=&#39;Qwen/Qwen2.5-7B-Instruct&#39;, tokenizer_mode=&#39;auto&#39;, skip_tokenizer_init=False, load_format=&#39;auto&#39;, model_loader_extra_config=&#39;{}&#39;, trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl=&#39;auto&#39;, host=&#39;0.0.0.0&#39;, port=37621, skip_server_warmup=False, warmups=None, nccl_port=None, dtype=&#39;auto&#39;, quantization=None, quantization_param_path=None, kv_cache_dtype=&#39;auto&#39;, mem_fraction_static=0.874, max_running_requests=128, max_queued_requests=9223372036854775807, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy=&#39;fcfs&#39;, schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device=&#39;cuda&#39;, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=824498156, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level=&#39;info&#39;, log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, served_model_name=&#39;Qwen/Qwen2.5-7B-Instruct&#39;, weight_version=&#39;default&#39;, chat_template=None, completion_template=None, file_storage_path=&#39;sglang_storage&#39;, enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method=&#39;round_robin&#39;, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args=&#39;{}&#39;, preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend=&#39;triton&#39;, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend=&#39;flashinfer&#39;, grammar_backend=&#39;xgrammar&#39;, mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, moe_a2a_backend=&#39;none&#39;, moe_runner_backend=&#39;auto&#39;, enable_flashinfer_allreduce_fusion=False, deepep_mode=&#39;auto&#39;, ep_num_redundant_experts=0, ep_dispatch_algorithm=&#39;static&#39;, init_expert_location=&#39;trivial&#39;, enable_eplb=False, eplb_algorithm=&#39;auto&#39;, eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy=&#39;write_through_selective&#39;, hicache_io_backend=&#39;kernel&#39;, hicache_mem_layout=&#39;layer_first&#39;, hicache_storage_backend=None, hicache_storage_prefetch_policy=&#39;best_effort&#39;, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type=&#39;qk&#39;, ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config=&#39;&#39;, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode=&#39;null&#39;, disaggregation_transfer_backend=&#39;mooncake&#39;, disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)
[2025-08-17 06:51:58] Using default HuggingFace chat template with detected content format: string
W0817 06:52:05.095000 921380 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
W0817 06:52:05.095000 921380 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures.
W0817 06:52:05.683000 921379 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
W0817 06:52:05.683000 921379 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures.
[2025-08-17 06:52:07] Attention backend not explicitly specified. Use fa3 backend by default.
[2025-08-17 06:52:07] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-08-17 06:52:07] Init torch distributed ends. mem usage=0.00 GB
[2025-08-17 06:52:07] MOE_RUNNER_BACKEND is not initialized, using triton backend
[2025-08-17 06:52:08] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named &#39;transformers.models.glm4v_moe&#39;
[2025-08-17 06:52:08] Load weight begin. avail mem=78.58 GB
[2025-08-17 06:52:08] Using model weights format [&#39;*.safetensors&#39;]
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00&lt;00:01,  1.57it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01&lt;00:01,  1.46it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02&lt;00:00,  1.42it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02&lt;00:00,  1.45it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02&lt;00:00,  1.45it/s]

[2025-08-17 06:52:11] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=64.20 GB, mem usage=14.38 GB.
[2025-08-17 06:52:11] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB
[2025-08-17 06:52:11] Memory pool end. avail mem=62.91 GB
[2025-08-17 06:52:11] Capture cuda graph begin. This can take up to several minutes. avail mem=62.81 GB
[2025-08-17 06:52:12] Capture graph bs [1, 2, 4]
Capturing batches (bs=4 avail_mem=62.79 GB):   0%|          | 0/3 [00:00&lt;?, ?it/s][2025-08-17 06:52:12] IS_TBO_ENABLED is not initialized, using False
Capturing batches (bs=1 avail_mem=62.73 GB): 100%|██████████| 3/3 [00:00&lt;00:00, 11.05it/s]
[2025-08-17 06:52:12] Capture cuda graph end. Time elapsed: 0.77 s. mem usage=0.09 GB. avail mem=62.72 GB.
[2025-08-17 06:52:12] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=128, context_len=32768, available_gpu_mem=62.72 GB
[2025-08-17 06:52:13] INFO:     Started server process [921164]
[2025-08-17 06:52:13] INFO:     Waiting for application startup.
[2025-08-17 06:52:13] INFO:     Application startup complete.
[2025-08-17 06:52:13] INFO:     Uvicorn running on http://0.0.0.0:37621 (Press CTRL+C to quit)
[2025-08-17 06:52:14] INFO:     127.0.0.1:45210 - &#34;GET /v1/models HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:14] INFO:     127.0.0.1:45224 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:14] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:15] INFO:     127.0.0.1:45230 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:15] The server is fired up and ready to roll!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Server started on http://localhost:37621
</pre></div></div>
</div>
<p>Set the default backend. Note: Besides the local server, you may use also <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code> or other API endpoints.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_default_backend</span><span class="p">(</span><span class="n">RuntimeEndpoint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:19] INFO:     127.0.0.1:45236 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
</section>
<section id="Basic-Usage">
<h2>Basic Usage<a class="headerlink" href="#Basic-Usage" title="Link to this heading">#</a></h2>
<p>The most simple way of using SGLang frontend language is a simple question answer dialog between a user and an assistant.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">basic_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">system</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;You are a helpful assistant than can answer questions.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">basic_qa</span><span class="p">(</span><span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:19] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:19] Decode batch. #running-req: 1, #token: 64, token usage: 0.00, cuda graph: True, gen throughput (token/s): 5.68, #queue-req: 0,
[2025-08-17 06:52:19] INFO:     127.0.0.1:45252 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Sure! Here are three countries along with their capitals:<br><br>1. **France** - Paris<br>2. **Japan** - Tokyo<br>3. **Australia** - Canberra</strong></div>
</div>
</section>
<section id="Multi-turn-Dialog">
<h2>Multi-turn Dialog<a class="headerlink" href="#Multi-turn-Dialog" title="Link to this heading">#</a></h2>
<p>SGLang frontend language can also be used to define multi-turn dialogs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multi_turn_qa</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">system</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;You are a helpful assistant than can answer questions.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="s2">&quot;Please give me a list of 3 countries and their capitals.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;first_answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">))</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="s2">&quot;Please give me another list of 3 countries and their capitals.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;second_answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">s</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">multi_turn_qa</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;first_answer&quot;</span><span class="p">])</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;second_answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:19] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 18, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:20] Decode batch. #running-req: 1, #token: 74, token usage: 0.00, cuda graph: True, gen throughput (token/s): 147.14, #queue-req: 0,
[2025-08-17 06:52:20] INFO:     127.0.0.1:45258 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Sure, here's a list of 3 countries along with their capitals:<br><br>1. **France** - Paris<br>2. **Australia** - Canberra<br>3. **Japan** - Tokyo</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:20] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 74, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:20] Decode batch. #running-req: 1, #token: 136, token usage: 0.01, cuda graph: True, gen throughput (token/s): 154.05, #queue-req: 0,
[2025-08-17 06:52:20] INFO:     127.0.0.1:45264 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Of course! Here's another list of 3 countries and their capitals:<br><br>1. **Italy** - Rome<br>2. **Mexico** - Mexico City<br>3. **India** - New Delhi</strong></div>
</div>
</section>
<section id="Control-flow">
<h2>Control flow<a class="headerlink" href="#Control-flow" title="Link to this heading">#</a></h2>
<p>You may use any Python code within the function to define more complex control flows.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">tool_use</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="s2">&quot;To answer this question: &quot;</span>
        <span class="o">+</span> <span class="n">question</span>
        <span class="o">+</span> <span class="s2">&quot;. I need to use a &quot;</span>
        <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;calculator&quot;</span><span class="p">,</span> <span class="s2">&quot;search engine&quot;</span><span class="p">])</span>
        <span class="o">+</span> <span class="s2">&quot;. &quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="s2">&quot;tool&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;calculator&quot;</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;The math expression is: &quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;expression&quot;</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">s</span><span class="p">[</span><span class="s2">&quot;tool&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;search engine&quot;</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;The key word to search is: &quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;word&quot;</span><span class="p">))</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">tool_use</span><span class="p">(</span><span class="s2">&quot;What is 2 * 2?&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;tool&quot;</span><span class="p">])</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;expression&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:20] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 8, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:20] INFO:     127.0.0.1:45280 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:20] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:20] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:20] INFO:     127.0.0.1:45282 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>calculator</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:20] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 33, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:20] Decode batch. #running-req: 1, #token: 84, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.93, #queue-req: 0,
[2025-08-17 06:52:20] INFO:     127.0.0.1:45290 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>2 * 2. <br><br>To solve it, you don't really need a calculator for such a simple calculation, but I understand you might want to demonstrate or use a tool. Here's the calculation:<br><br>2 * 2 = 4<br><br>So, 2 multiplied by 2 equals 4.</strong></div>
</div>
</section>
<section id="Parallelism">
<h2>Parallelism<a class="headerlink" href="#Parallelism" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">fork</span></code> to launch parallel prompts. Because <code class="docutils literal notranslate"><span class="pre">sgl.gen</span></code> is non-blocking, the for loop below issues two generation calls in parallel.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">tip_suggestion</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="s2">&quot;Here are two tips for staying healthy: &quot;</span>
        <span class="s2">&quot;1. Balanced Diet. 2. Regular Exercise.</span><span class="se">\n\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="n">forks</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fork</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">forks</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Now, expand tip </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> into a paragraph:</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;detailed_tip&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;Tip 1:&quot;</span> <span class="o">+</span> <span class="n">forks</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;detailed_tip&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;Tip 2:&quot;</span> <span class="o">+</span> <span class="n">forks</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;detailed_tip&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="s2">&quot;To summarize the above two tips, I can say:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;summary&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="p">)</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">tip_suggestion</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;summary&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:20] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:20] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2025-08-17 06:52:21] Decode batch. #running-req: 2, #token: 86, token usage: 0.00, cuda graph: True, gen throughput (token/s): 161.73, #queue-req: 0,
[2025-08-17 06:52:21] Decode batch. #running-req: 2, #token: 166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 318.65, #queue-req: 0,
[2025-08-17 06:52:21] Decode batch. #running-req: 2, #token: 246, token usage: 0.01, cuda graph: True, gen throughput (token/s): 317.21, #queue-req: 0,
[2025-08-17 06:52:21] INFO:     127.0.0.1:45300 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:21] Decode batch. #running-req: 2, #token: 185, token usage: 0.01, cuda graph: True, gen throughput (token/s): 313.97, #queue-req: 0,
[2025-08-17 06:52:21] INFO:     127.0.0.1:45304 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:21] Prefill batch. #new-seq: 1, #new-token: 309, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:22] Decode batch. #running-req: 1, #token: 377, token usage: 0.02, cuda graph: True, gen throughput (token/s): 145.40, #queue-req: 0,
[2025-08-17 06:52:22] Decode batch. #running-req: 1, #token: 417, token usage: 0.02, cuda graph: True, gen throughput (token/s): 161.79, #queue-req: 0,
[2025-08-17 06:52:22] Decode batch. #running-req: 1, #token: 457, token usage: 0.02, cuda graph: True, gen throughput (token/s): 162.01, #queue-req: 0,
[2025-08-17 06:52:22] Decode batch. #running-req: 1, #token: 497, token usage: 0.02, cuda graph: True, gen throughput (token/s): 161.96, #queue-req: 0,
[2025-08-17 06:52:23] Decode batch. #running-req: 1, #token: 537, token usage: 0.03, cuda graph: True, gen throughput (token/s): 161.95, #queue-req: 0,
[2025-08-17 06:52:23] Decode batch. #running-req: 1, #token: 577, token usage: 0.03, cuda graph: True, gen throughput (token/s): 161.88, #queue-req: 0,
[2025-08-17 06:52:23] Decode batch. #running-req: 1, #token: 617, token usage: 0.03, cuda graph: True, gen throughput (token/s): 161.80, #queue-req: 0,
[2025-08-17 06:52:23] Decode batch. #running-req: 1, #token: 657, token usage: 0.03, cuda graph: True, gen throughput (token/s): 161.82, #queue-req: 0,
[2025-08-17 06:52:24] Decode batch. #running-req: 1, #token: 697, token usage: 0.03, cuda graph: True, gen throughput (token/s): 161.65, #queue-req: 0,
[2025-08-17 06:52:24] INFO:     127.0.0.1:45314 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>1. **Balanced Diet:**<br>   - **Importance:** A well-balanced diet is crucial for overall health and well-being.<br>   - **Key Components:**<br>     - **Fruits and Vegetables:** Rich in vitamins, minerals, and fiber.<br>     - **Whole Grains:** Provide sustained energy and essential nutrients.<br>     - **Lean Proteins:** Include sources like chicken, fish, legumes, and tofu.<br>     - **Healthy Fats:** Such as those found in avocados, nuts, seeds, and olive oil.<br>   - **Health Benefits:**<br>     - **Weight Management:** Helps regulate body weight.<br>     - **Immune Health:** Enhances the immune system.<br>     - **Energy Levels:** Provides consistent energy throughout the day.<br>     - **Prevention:** Reduces the risk of chronic diseases like heart disease, diabetes, and certain cancers.<br><br>2. **Regular Exercise:**<br>   - **Importance:** Regular physical activity is essential for maintaining overall health and well-being.<br>   - **Health Benefits:**<br>     - **Cardiovascular Health:** Improves heart function and circulation.<br>     - **Muscle Strength:** Strengthens muscles and bones.<br>     - **Flexibility and Endurance:** Enhances mobility and reduces the risk of injuries.<br>     - **Mood and Mental Health:** Boosts mood and reduces anxiety and depression.<br>   - **Frequency and Types:**<br>     - **Aim for:** At least 150 minutes of moderate aerobic activity or 75 minutes of vigorous activity per week.<br>     - **Incorporate:** Activities like walking, jogging, swimming, cycling, or strength training into your routine.<br>   - **Consistency:** Regularity is key; try to exercise most days of the week for the best results.<br><br>By combining a balanced diet with regular exercise, you can significantly enhance your overall health and quality of life.</strong></div>
</div>
</section>
<section id="Constrained-Decoding">
<h2>Constrained Decoding<a class="headerlink" href="#Constrained-Decoding" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">regex</span></code> to specify a regular expression as a decoding constraint. This is only supported for local models.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">regular_expression_gen</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="s2">&quot;What is the IP address of the Google DNS servers?&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="n">gen</span><span class="p">(</span>
            <span class="s2">&quot;answer&quot;</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">regex</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;((25[0-5]|2[0-4]\d|[01]?\d\d?).)</span><span class="si">{3}</span><span class="s2">(25[0-5]|2[0-4]\d|[01]?\d\d?)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">regular_expression_gen</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:24] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 12, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:25] Decode batch. #running-req: 1, #token: 36, token usage: 0.00, cuda graph: True, gen throughput (token/s): 30.44, #queue-req: 0,
[2025-08-17 06:52:25] INFO:     127.0.0.1:53006 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>208.67.222.222</strong></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">regex</span></code> to define a <code class="docutils literal notranslate"><span class="pre">JSON</span></code> decoding schema.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">character_regex</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;\{\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;name&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;house&quot;: &quot;(Gryffindor|Slytherin|Ravenclaw|Hufflepuff)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;blood status&quot;: &quot;(Pure-blood|Half-blood|Muggle-born)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;occupation&quot;: &quot;(student|teacher|auror|ministry of magic|death eater|order of the phoenix)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;wand&quot;: \{\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;        &quot;wood&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;        &quot;core&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;        &quot;length&quot;: [0-9]{1,2}\.[0-9]{0,2}\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    \},\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;alive&quot;: &quot;(Alive|Deceased)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;patronus&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;bogart&quot;: &quot;[\w\d\s]{1,16}&quot;\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;\}&quot;&quot;&quot;</span>
<span class="p">)</span>


<span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">character_gen</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is a character in Harry Potter. Please fill in the following information about this character.&quot;</span>
    <span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;json_output&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">regex</span><span class="o">=</span><span class="n">character_regex</span><span class="p">))</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">character_gen</span><span class="p">(</span><span class="s2">&quot;Harry Potter&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;json_output&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:25] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:25] Decode batch. #running-req: 1, #token: 69, token usage: 0.00, cuda graph: True, gen throughput (token/s): 85.13, #queue-req: 0,
[2025-08-17 06:52:26] Decode batch. #running-req: 1, #token: 109, token usage: 0.01, cuda graph: True, gen throughput (token/s): 162.61, #queue-req: 0,
[2025-08-17 06:52:26] INFO:     127.0.0.1:53020 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{<br>    "name": "Harry Potter",<br>    "house": "Gryffindor",<br>    "blood status": "Half-blood",<br>    "occupation": "student",<br>    "wand": {<br>        "wood": "Willow",<br>        "core": "Phoenix feather",<br>        "length": 10.5<br>    },<br>    "alive": "Alive",<br>    "patronus": " stag",<br>    "bogart": "Professor QuirrL"<br>}</strong></div>
</div>
</section>
<section id="Batching">
<h2>Batching<a class="headerlink" href="#Batching" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">run_batch</span></code> to run a batch of prompts.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">text_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">))</span>


<span class="n">states</span> <span class="o">=</span> <span class="n">text_qa</span><span class="o">.</span><span class="n">run_batch</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of the United Kingdom?&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of Japan?&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">states</span><span class="p">):</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Answer </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;answer&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:26] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 160.79, #queue-req: 0,
[2025-08-17 06:52:26] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:26] INFO:     127.0.0.1:53036 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 3/3 [00:00&lt;00:00, 29.16it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:26] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 17, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:26] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2025-08-17 06:52:26] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 19, token usage: 0.00, #running-req: 2, #queue-req: 0,
[2025-08-17 06:52:26] INFO:     127.0.0.1:53058 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:26] INFO:     127.0.0.1:53066 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:26] INFO:     127.0.0.1:53046 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Answer 1: The capital of the United Kingdom is London.</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Answer 2: The capital of France is Paris.</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Answer 3: The capital of Japan is Tokyo.</strong></div>
</div>
</section>
<section id="Streaming">
<h2>Streaming<a class="headerlink" href="#Streaming" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">stream</span></code> to stream the output to the user.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">text_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">))</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">text_qa</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">question</span><span class="o">=</span><span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">text_iter</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;|im_start|&gt;system
You are a helpful assistant.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
What is the capital of France?&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
[2025-08-17 06:52:26] INFO:     127.0.0.1:53078 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:26] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0,
The capital of France is Paris.&lt;|im_end|&gt;
</pre></div></div>
</div>
</section>
<section id="Complex-Prompts">
<h2>Complex Prompts<a class="headerlink" href="#Complex-Prompts" title="Link to this heading">#</a></h2>
<p>You may use <code class="docutils literal notranslate"><span class="pre">{system|user|assistant}_{begin|end}</span></code> to define complex prompts.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_example</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">system</span><span class="p">(</span><span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">)</span>
    <span class="c1"># Same as: s += s.system(&quot;You are a helpful assistant.&quot;)</span>

    <span class="k">with</span> <span class="n">s</span><span class="o">.</span><span class="n">user</span><span class="p">():</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;Question: What is the capital of France?&quot;</span>

    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant_begin</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;Answer: &quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant_end</span><span class="p">()</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">chat_example</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:26] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:26] INFO:     127.0.0.1:53094 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'> The capital of France is Paris.</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Multi-modal-Generation">
<h2>Multi-modal Generation<a class="headerlink" href="#Multi-modal-Generation" title="Link to this heading">#</a></h2>
<p>You may use SGLang frontend language to define multi-modal prompts. See <a class="reference external" href="https://docs.sglang.ai/supported_models/generative_models.html">here</a> for supported models.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
    <span class="s2">&quot;python -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct --host 0.0.0.0&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Server started on http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
W0817 06:52:33.135000 922219 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
W0817 06:52:33.135000 922219 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures.
[2025-08-17 06:52:34] server_args=ServerArgs(model_path=&#39;Qwen/Qwen2.5-VL-7B-Instruct&#39;, tokenizer_path=&#39;Qwen/Qwen2.5-VL-7B-Instruct&#39;, tokenizer_mode=&#39;auto&#39;, skip_tokenizer_init=False, load_format=&#39;auto&#39;, model_loader_extra_config=&#39;{}&#39;, trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl=&#39;auto&#39;, host=&#39;0.0.0.0&#39;, port=36533, skip_server_warmup=False, warmups=None, nccl_port=None, dtype=&#39;auto&#39;, quantization=None, quantization_param_path=None, kv_cache_dtype=&#39;auto&#39;, mem_fraction_static=0.7835956249999999, max_running_requests=128, max_queued_requests=9223372036854775807, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy=&#39;fcfs&#39;, schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device=&#39;cuda&#39;, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=592992778, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level=&#39;info&#39;, log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, served_model_name=&#39;Qwen/Qwen2.5-VL-7B-Instruct&#39;, weight_version=&#39;default&#39;, chat_template=None, completion_template=None, file_storage_path=&#39;sglang_storage&#39;, enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method=&#39;round_robin&#39;, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args=&#39;{}&#39;, preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend=&#39;triton&#39;, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend=&#39;flashinfer&#39;, grammar_backend=&#39;xgrammar&#39;, mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, moe_a2a_backend=&#39;none&#39;, moe_runner_backend=&#39;auto&#39;, enable_flashinfer_allreduce_fusion=False, deepep_mode=&#39;auto&#39;, ep_num_redundant_experts=0, ep_dispatch_algorithm=&#39;static&#39;, init_expert_location=&#39;trivial&#39;, enable_eplb=False, eplb_algorithm=&#39;auto&#39;, eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy=&#39;write_through_selective&#39;, hicache_io_backend=&#39;kernel&#39;, hicache_mem_layout=&#39;layer_first&#39;, hicache_storage_backend=None, hicache_storage_prefetch_policy=&#39;best_effort&#39;, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type=&#39;qk&#39;, ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config=&#39;&#39;, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode=&#39;null&#39;, disaggregation_transfer_backend=&#39;mooncake&#39;, disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)
[2025-08-17 06:52:34] MOE_RUNNER_BACKEND is not initialized, using triton backend
[2025-08-17 06:52:35] Ignore import error when loading sglang.srt.multimodal.processors.glm4v: No module named &#39;transformers.models.glm4v_moe&#39;
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[2025-08-17 06:52:36] You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[2025-08-17 06:52:36] Using default HuggingFace chat template with detected content format: openai
W0817 06:52:41.827000 922434 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
W0817 06:52:41.827000 922434 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures.
W0817 06:52:42.028000 922435 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
W0817 06:52:42.028000 922435 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[2025-08-17 06:52:43] You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[2025-08-17 06:52:44] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-08-17 06:52:44] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-08-17 06:52:45] Init torch distributed ends. mem usage=0.00 GB
[2025-08-17 06:52:45] MOE_RUNNER_BACKEND is not initialized, using triton backend
[2025-08-17 06:52:46] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named &#39;transformers.models.glm4v_moe&#39;
[2025-08-17 06:52:46] Load weight begin. avail mem=78.58 GB
[2025-08-17 06:52:46] Multimodal attention backend not set. Use triton_attn.
[2025-08-17 06:52:46] Using triton_attn as multimodal attention backend.
[2025-08-17 06:52:46] Using model weights format [&#39;*.safetensors&#39;]
Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00&lt;00:02,  1.45it/s]
Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01&lt;00:02,  1.38it/s]
Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02&lt;00:01,  1.36it/s]
Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02&lt;00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03&lt;00:00,  1.71it/s]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03&lt;00:00,  1.54it/s]

[2025-08-17 06:52:50] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=62.79 GB, mem usage=15.79 GB.
[2025-08-17 06:52:50] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB
[2025-08-17 06:52:50] Memory pool end. avail mem=61.46 GB
[2025-08-17 06:52:50] Capture cuda graph begin. This can take up to several minutes. avail mem=60.89 GB
[2025-08-17 06:52:50] Capture graph bs [1, 2, 4]
Capturing batches (bs=4 avail_mem=60.89 GB):   0%|          | 0/3 [00:00&lt;?, ?it/s][2025-08-17 06:52:51] IS_TBO_ENABLED is not initialized, using False
Capturing batches (bs=1 avail_mem=60.79 GB): 100%|██████████| 3/3 [00:01&lt;00:00,  2.98it/s]
[2025-08-17 06:52:52] Capture cuda graph end. Time elapsed: 1.84 s. mem usage=0.12 GB. avail mem=60.76 GB.
[2025-08-17 06:52:53] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=128, context_len=128000, available_gpu_mem=60.76 GB
[2025-08-17 06:52:54] INFO:     Started server process [922219]
[2025-08-17 06:52:54] INFO:     Waiting for application startup.
[2025-08-17 06:52:54] INFO:     Application startup complete.
[2025-08-17 06:52:54] INFO:     Uvicorn running on http://0.0.0.0:36533 (Press CTRL+C to quit)
[2025-08-17 06:52:54] INFO:     127.0.0.1:35604 - &#34;GET /v1/models HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:55] INFO:     127.0.0.1:35606 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:55] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:52:55] INFO:     127.0.0.1:35616 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-08-17 06:52:55] The server is fired up and ready to roll!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Server started on http://localhost:36533
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_default_backend</span><span class="p">(</span><span class="n">RuntimeEndpoint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:52:59] INFO:     127.0.0.1:35618 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<p>Ask a question about an image.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">image_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">image_file</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">image</span><span class="p">(</span><span class="n">image_file</span><span class="p">)</span> <span class="o">+</span> <span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">))</span>


<span class="n">image_url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true&quot;</span>
<span class="n">image_bytes</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_image</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">image_qa</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">,</span> <span class="s2">&quot;What is in the image?&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-08-17 06:53:00] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-08-17 06:53:01] Decode batch. #running-req: 1, #token: 340, token usage: 0.02, cuda graph: True, gen throughput (token/s): 5.46, #queue-req: 0,
[2025-08-17 06:53:01] Decode batch. #running-req: 1, #token: 380, token usage: 0.02, cuda graph: True, gen throughput (token/s): 127.29, #queue-req: 0,
[2025-08-17 06:53:01] INFO:     127.0.0.1:35620 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>The image depicts a man performing an unusual task on the back of a yellow SUV shared by Budget Taxis and U-Haul. He is standing to ensure his balance, holding an iron and two crutches. On the SUV's roof luggage rack, there is a clipped-in striped pole, known in shirt-ironing trade escapees tours, presumably being used as a makeshift magnetic lumber for ironing. The image is candid and possibly staged, showcasing creativity and resourcefulness as insurance for the unpredictable situations taxis provoke, which seemed irresistible opportunities.</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="frontend_index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Frontend Language</p>
      </div>
    </a>
    <a class="right-next"
       href="choices_methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Choices Methods in SGLang</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Basic-Usage">Basic Usage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-turn-Dialog">Multi-turn Dialog</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Control-flow">Control flow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Parallelism">Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Constrained-Decoding">Constrained Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Batching">Batching</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Streaming">Streaming</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Complex-Prompts">Complex Prompts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-modal-Generation">Multi-modal Generation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023-2025, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Aug 17, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>