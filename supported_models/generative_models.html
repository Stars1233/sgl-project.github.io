
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Large Language Models &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=c87b5680"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'supported_models/generative_models';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multimodal Language Models" href="multimodal_language_models.html" />
    <link rel="prev" title="Observability" href="../advanced_features/observability.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Oct 15, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic Usage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/openai_api.html">OpenAI-Compatible APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/deepseek.html">DeepSeek Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/gpt_oss.html">GPT OSS Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/llama4.html">Llama4 Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/qwen3.html">Qwen3-Next Usage</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/attention_backend.html">Attention Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/tool_parser.html">Tool Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/lora.html">LoRA Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pd_disaggregation.html">PD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hicache.html">Hierarchical KV Caching (HiCache)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pd_multiplexing.html">PD Multiplexing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/vlm_query.html">Query Vision Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/router.html">SGLang Model Gateway (formerly SGLang Router)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/observability.html">Observability</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal_language_models.html">Multimodal Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedding_models.html">Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="reward_models.html">Reward Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="rerank_models.html">Rerank Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="support_new_models.html">How to Support New Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers_fallback.html">Transformers fallback in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="modelscope.html">Use Models From ModelScope</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hardware Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../platforms/amd_gpu.html">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/cpu_server.html">CPU Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/nvidia_jetson.html">NVIDIA Jetson Orin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/ascend_npu.html">Ascend NPUs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/development_guide_using_docker.html">Development Guide Using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/benchmark_and_profiling.html">Benchmark and Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/bench_serving.html">Bench Serving Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/faq.html">Troubleshooting and Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/environment_variables.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_metrics.html">Production Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/multi_node_deployment/multi_node_index.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/frontend/frontend_index.html">Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/learn_more.html">Learn more</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/supported_models/generative_models.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/supported_models/generative_models.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fsupported_models/generative_models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/supported_models/generative_models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Large Language Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-launch-command">Example launch Command</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="large-language-models">
<h1>Large Language Models<a class="headerlink" href="#large-language-models" title="Link to this heading">#</a></h1>
<p>These models accept text input and produce text output (e.g., chat completions). They are primarily large language models (LLMs), some with mixture-of-experts (MoE) architectures for scaling.</p>
<section id="example-launch-command">
<h2>Example launch Command<a class="headerlink" href="#example-launch-command" title="Link to this heading">#</a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>meta-llama/Llama-3.2-1B-Instruct<span class="w"> </span><span class="se">\ </span><span class="w"> </span><span class="c1"># example HF/local path</span>
<span class="w">  </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="m">30000</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
</section>
<section id="supported-models">
<h2>Supported models<a class="headerlink" href="#supported-models" title="Link to this heading">#</a></h2>
<p>Below the supported models are summarized in a table.</p>
<p>If you are unsure if a specific architecture is implemented, you can search for it via GitHub. For example, to search for <code class="docutils literal notranslate"><span class="pre">Qwen3ForCausalLM</span></code>, use the expression:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">repo</span><span class="p">:</span><span class="n">sgl</span><span class="o">-</span><span class="n">project</span><span class="o">/</span><span class="n">sglang</span> <span class="n">path</span><span class="p">:</span><span class="o">/^</span><span class="n">python</span>\<span class="o">/</span><span class="n">sglang</span>\<span class="o">/</span><span class="n">srt</span>\<span class="o">/</span><span class="n">models</span>\<span class="o">//</span> <span class="n">Qwen3ForCausalLM</span>
</pre></div>
</div>
<p>in the GitHub search bar.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Family (Variants)</p></th>
<th class="head"><p>Example HuggingFace Identifier</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>DeepSeek</strong> (v1, v2, v3/R1)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">deepseek-ai/DeepSeek-R1</span></code></p></td>
<td><p>Series of advanced reasoning-optimized models (including a 671B MoE) trained with reinforcement learning; top performance on complex reasoning, math, and code tasks. <a class="reference internal" href="../basic_usage/deepseek.html"><span class="std std-doc">SGLang provides Deepseek v3/R1 model-specific optimizations</span></a> and <a class="reference internal" href="../advanced_features/separate_reasoning.html"><span class="std std-doc">Reasoning Parser</span></a></p></td>
</tr>
<tr class="row-odd"><td><p><strong>GPT-OSS</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">openai/gpt-oss-20b</span></code>, <code class="docutils literal notranslate"><span class="pre">openai/gpt-oss-120b</span></code></p></td>
<td><p>OpenAI’s latest GPT-OSS series for complex reasoning, agentic tasks, and versatile developer use cases.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Qwen</strong> (3, 3MoE, 3Next, 2.5, 2 series)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Qwen/Qwen3-0.6B</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen/Qwen3-30B-A3B</span></code> <code class="docutils literal notranslate"><span class="pre">Qwen/Qwen3-Next-80B-A3B-Instruct</span> </code></p></td>
<td><p>Alibaba’s latest Qwen3 series for complex reasoning, language understanding, and generation tasks; Support for MoE variants along with previous generation 2.5, 2, etc. <a class="reference internal" href="../advanced_features/separate_reasoning.html"><span class="std std-doc">SGLang provides Qwen3 specific reasoning parser</span></a></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Llama</strong> (2, 3.x, 4 series)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-4-Scout-17B-16E-Instruct</span></code></p></td>
<td><p>Meta’s open LLM series, spanning 7B to 400B parameters (Llama 2, 3, and new Llama 4) with well-recognized performance. <a class="reference internal" href="../basic_usage/llama4.html"><span class="std std-doc">SGLang provides Llama-4 model-specific optimizations</span></a></p></td>
</tr>
<tr class="row-even"><td><p><strong>Mistral</strong> (Mixtral, NeMo, Small3)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mistralai/Mistral-7B-Instruct-v0.2</span></code></p></td>
<td><p>Open 7B LLM by Mistral AI with strong performance; extended into MoE (“Mixtral”) and NeMo Megatron variants for larger scale.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Gemma</strong> (v1, v2, v3)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">google/gemma-3-1b-it</span></code></p></td>
<td><p>Google’s family of efficient multilingual models (1B–27B); Gemma 3 offers a 128K context window, and its larger (4B+) variants support vision input.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Phi</strong> (Phi-1.5, Phi-2, Phi-3, Phi-4, Phi-MoE series)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">microsoft/Phi-4-multimodal-instruct</span></code>, <code class="docutils literal notranslate"><span class="pre">microsoft/Phi-3.5-MoE-instruct</span></code></p></td>
<td><p>Microsoft’s Phi family of small models (1.3B–5.6B); Phi-4-multimodal (5.6B) processes text, images, and speech, Phi-4-mini is a high-accuracy text model and Phi-3.5-MoE is a mixture-of-experts model.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>MiniCPM</strong> (v3, 4B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">openbmb/MiniCPM3-4B</span></code></p></td>
<td><p>OpenBMB’s series of compact LLMs for edge devices; MiniCPM 3 (4B) achieves GPT-3.5-level results in text tasks.</p></td>
</tr>
<tr class="row-even"><td><p><strong>OLMoE</strong> (Open MoE)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">allenai/OLMoE-1B-7B-0924</span></code></p></td>
<td><p>Allen AI’s open Mixture-of-Experts model (7B total, 1B active parameters) delivering state-of-the-art results with sparse expert activation.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>StableLM</strong> (3B, 7B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">stabilityai/stablelm-tuned-alpha-7b</span></code></p></td>
<td><p>StabilityAI’s early open-source LLM (3B &amp; 7B) for general text generation; a demonstration model with basic instruction-following ability.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Command-R</strong> (Cohere)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CohereForAI/c4ai-command-r-v01</span></code></p></td>
<td><p>Cohere’s open conversational LLM (Command series) optimized for long context, retrieval-augmented generation, and tool use.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>DBRX</strong> (Databricks)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">databricks/dbrx-instruct</span></code></p></td>
<td><p>Databricks’ 132B-parameter MoE model (36B active) trained on 12T tokens; competes with GPT-3.5 quality as a fully open foundation model.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Grok</strong> (xAI)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">xai-org/grok-1</span></code></p></td>
<td><p>xAI’s grok-1 model known for vast size(314B parameters) and high quality; integrated in SGLang for high-performance inference.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ChatGLM</strong> (GLM-130B family)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">THUDM/chatglm2-6b</span></code></p></td>
<td><p>Zhipu AI’s bilingual chat model (6B) excelling at Chinese-English dialogue; fine-tuned for conversational quality and alignment.</p></td>
</tr>
<tr class="row-even"><td><p><strong>InternLM 2</strong> (7B, 20B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">internlm/internlm2-7b</span></code></p></td>
<td><p>Next-gen InternLM (7B and 20B) from SenseTime, offering strong reasoning and ultra-long context support (up to 200K tokens).</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ExaONE 3</strong> (Korean-English)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct</span></code></p></td>
<td><p>LG AI Research’s Korean-English model (7.8B) trained on 8T tokens; provides high-quality bilingual understanding and generation.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Baichuan 2</strong> (7B, 13B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">baichuan-inc/Baichuan2-13B-Chat</span></code></p></td>
<td><p>BaichuanAI’s second-generation Chinese-English LLM (7B/13B) with improved performance and an open commercial license.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>XVERSE</strong> (MoE)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">xverse/XVERSE-MoE-A36B</span></code></p></td>
<td><p>Yuanxiang’s open MoE LLM (XVERSE-MoE-A36B: 255B total, 36B active) supporting ~40 languages; delivers 100B+ dense-level performance via expert routing.</p></td>
</tr>
<tr class="row-even"><td><p><strong>SmolLM</strong> (135M–1.7B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HuggingFaceTB/SmolLM-1.7B</span></code></p></td>
<td><p>Hugging Face’s ultra-small LLM series (135M–1.7B params) offering surprisingly strong results, enabling advanced AI on mobile/edge devices.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>GLM-4</strong> (Multilingual 9B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ZhipuAI/glm-4-9b-chat</span></code></p></td>
<td><p>Zhipu’s GLM-4 series (up to 9B parameters) – open multilingual models with support for 1M-token context and even a 5.6B multimodal variant (Phi-4V).</p></td>
</tr>
<tr class="row-even"><td><p><strong>MiMo</strong> (7B series)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">XiaomiMiMo/MiMo-7B-RL</span></code></p></td>
<td><p>Xiaomi’s reasoning-optimized model series, leverages Multiple-Token Prediction for faster inference.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ERNIE-4.5</strong> (4.5, 4.5MoE series)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">baidu/ERNIE-4.5-21B-A3B-PT</span></code></p></td>
<td><p>Baidu’s ERNIE-4.5 series which consists of MoE with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Arcee AFM-4.5B</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">arcee-ai/AFM-4.5B-Base</span></code></p></td>
<td><p>Arcee’s foundational model series for real world reliability and edge deployments.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Persimmon</strong> (8B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">adept/persimmon-8b-chat</span></code></p></td>
<td><p>Adept’s open 8B model with a 16K context window and fast inference; trained for broad usability and licensed under Apache 2.0.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Solar</strong> (10.7B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">upstage/SOLAR-10.7B-Instruct-v1.0</span></code></p></td>
<td><p>Upstage’s 10.7B parameter model, optimized for instruction-following tasks. This architecture incorporates a depth-up scaling methodology, enhancing model performance.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Ling</strong> (16.8B–290B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inclusionAI/Ling-lite</span></code>, <code class="docutils literal notranslate"><span class="pre">inclusionAI/Ling-plus</span></code></p></td>
<td><p>InclusionAI’s open MoE models. Ling-Lite has 16.8B total / 2.75B active parameters, and Ling-Plus has 290B total / 28.8B active parameters. They are designed for high performance on NLP and complex reasoning tasks.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Granite 3.0, 3.1</strong> (IBM)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ibm-granite/granite-3.1-8b-instruct</span></code></p></td>
<td><p>IBM’s open dense foundation models optimized for reasoning, code, and business AI use cases. Integrated with Red Hat and watsonx systems.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Granite 3.0 MoE</strong> (IBM)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ibm-granite/granite-3.0-3b-a800m-instruct</span></code></p></td>
<td><p>IBM’s Mixture-of-Experts models offering strong performance with cost-efficiency. MoE expert routing designed for enterprise deployment at scale.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Llama Nemotron Super</strong> (v1, v1.5, NVIDIA)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nvidia/Llama-3_3-Nemotron-Super-49B-v1</span></code>, <code class="docutils literal notranslate"><span class="pre">nvidia/Llama-3_3-Nemotron-Super-49B-v1_5</span></code></p></td>
<td><p>The <a class="reference external" href="https://www.nvidia.com/en-us/ai-data-science/foundation-models/nemotron/">NVIDIA Nemotron</a> family of multimodal models provides state-of-the-art reasoning models specifically designed for enterprise-ready AI agents.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Llama Nemotron Ultra</strong> (v1, NVIDIA)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nvidia/Llama-3_1-Nemotron-Ultra-253B-v1</span></code></p></td>
<td><p>The <a class="reference external" href="https://www.nvidia.com/en-us/ai-data-science/foundation-models/nemotron/">NVIDIA Nemotron</a> family of multimodal models provides state-of-the-art reasoning models specifically designed for enterprise-ready AI agents.</p></td>
</tr>
<tr class="row-even"><td><p><strong>NVIDIA Nemotron Nano 2.0</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nvidia/NVIDIA-Nemotron-Nano-9B-v2</span></code></p></td>
<td><p>The <a class="reference external" href="https://www.nvidia.com/en-us/ai-data-science/foundation-models/nemotron/">NVIDIA Nemotron</a> family of multimodal models provides state-of-the-art reasoning models specifically designed for enterprise-ready AI agents. <code class="docutils literal notranslate"><span class="pre">Nemotron-Nano-9B-v2</span></code> is a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>StarCoder2</strong> (3B-15B)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bigcode/starcoder2-7b</span></code></p></td>
<td><p>StarCoder2 is a family of open large language models (LLMs) specialized for code generation and understanding. It is the successor to StarCoder, jointly developed by the BigCode project (a collaboration between Hugging Face, ServiceNow Research, and other contributors).</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../advanced_features/observability.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Observability</p>
      </div>
    </a>
    <a class="right-next"
       href="multimodal_language_models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Multimodal Language Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-launch-command">Example launch Command</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-models">Supported models</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023-2025, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Oct 15, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>