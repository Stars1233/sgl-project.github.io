{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs For Reasoning Models\n",
    "\n",
    "When working with reasoning models that use special tokens like `<think>...</think>` to denote reasoning sections, you might want to allow free-form text within these sections while still enforcing grammar constraints on the rest of the output.\n",
    "\n",
    "SGLang provides a feature to disable grammar restrictions within reasoning sections. This is particularly useful for models that need to perform complex reasoning steps before providing a structured output.\n",
    "\n",
    "To enable this feature, use the `--reasoning-parser` flag which decide the think_end_token, such as `</think>`, when launching the server. You can also specify the reasoning parser using the `--reasoning-parser` flag.\n",
    "\n",
    "## Supported Models\n",
    "\n",
    "Currently, SGLang supports the following reasoning models:\n",
    "- [DeepSeek R1 series](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "- [QwQ](https://huggingface.co/Qwen/QwQ-32B): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "## OpenAI Compatible API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `--grammar-backend`, `--reasoning-parser` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:36:35.271794Z",
     "iopub.status.busy": "2025-05-15T22:36:35.271317Z",
     "iopub.status.idle": "2025-05-15T22:37:11.084117Z",
     "shell.execute_reply": "2025-05-15T22:37:11.083519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:36:46] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, host='0.0.0.0', port=30120, mem_fraction_static=0.88, max_running_requests=200, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=412130716, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='deepseek-r1', dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None, pdlb_url=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:36:53] Attention backend not set. Use fa3 backend by default.\n",
      "[2025-05-15 22:36:53] Init torch distributed begin.\n",
      "[2025-05-15 22:36:54] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-05-15 22:36:54] Load weight begin. avail mem=60.59 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:36:56] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.75s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.62s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:04] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=43.18 GB, mem usage=17.41 GB.\n",
      "[2025-05-15 22:37:04] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB\n",
      "[2025-05-15 22:37:04] Memory pool end. avail mem=41.81 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:04] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=200, context_len=131072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:05] INFO:     Started server process [70984]\n",
      "[2025-05-15 22:37:05] INFO:     Waiting for application startup.\n",
      "[2025-05-15 22:37:05] INFO:     Application startup complete.\n",
      "[2025-05-15 22:37:05] INFO:     Uvicorn running on http://0.0.0.0:30120 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:06] INFO:     127.0.0.1:40760 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:06] INFO:     127.0.0.1:40766 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-05-15 22:37:06] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:07] INFO:     127.0.0.1:40778 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-05-15 22:37:07] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --reasoning-parser deepseek-r1\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")\n",
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "you can directly define a JSON schema or use [Pydantic](https://docs.pydantic.dev/latest/) to define and validate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:37:11.086622Z",
     "iopub.status.busy": "2025-05-15T22:37:11.086120Z",
     "iopub.status.idle": "2025-05-15T22:37:19.324067Z",
     "shell.execute_reply": "2025-05-15T22:37:19.323497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:11] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:12] Decode batch. #running-req: 1, #token: 55, token usage: 0.00, cuda graph: False, gen throughput (token/s): 4.93, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:13] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, cuda graph: False, gen throughput (token/s): 66.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:14] Decode batch. #running-req: 1, #token: 135, token usage: 0.01, cuda graph: False, gen throughput (token/s): 58.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:14] Decode batch. #running-req: 1, #token: 175, token usage: 0.01, cuda graph: False, gen throughput (token/s): 62.42, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:15] Decode batch. #running-req: 1, #token: 215, token usage: 0.01, cuda graph: False, gen throughput (token/s): 66.42, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:16] Decode batch. #running-req: 1, #token: 255, token usage: 0.01, cuda graph: False, gen throughput (token/s): 66.55, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:16] Decode batch. #running-req: 1, #token: 295, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.27, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:17] Decode batch. #running-req: 1, #token: 335, token usage: 0.02, cuda graph: False, gen throughput (token/s): 65.49, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:17] Decode batch. #running-req: 1, #token: 375, token usage: 0.02, cuda graph: False, gen throughput (token/s): 65.43, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:18] Decode batch. #running-req: 1, #token: 415, token usage: 0.02, cuda graph: False, gen throughput (token/s): 62.24, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:19] Decode batch. #running-req: 1, #token: 455, token usage: 0.02, cuda graph: False, gen throughput (token/s): 65.42, #queue-req: 0\n",
      "[2025-05-15 22:37:19] INFO:     127.0.0.1:48690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user asked for the information and population of the capital of France in JSON format. I immediately thought about what the capital is—Paris. Then, I considered the population. I know it's a big city, but I'm not exactly sure of the exact number. I remember it's over 3 million, but I'm not certain if it's 3.5 or 3.6. I should double-check that.<br><br>Wait, maybe I should think about recent data. I recall that Paris has been growing a bit, so perhaps it's around 3.6 million. But I'm not 100% sure. I should look up the latest statistics to confirm. Okay, checking a reliable source, and it says the population is approximately 3,600,000. That seems right.<br><br>Now, the user wants this in JSON format. I need to structure it properly. The key should be \"capital\" with the name \"Paris,\" and another key for \"population.\" I'll make sure the population number is correctly formatted as an integer in the JSON.<br><br>I also need to present this clearly. Maybe I should write it out in the response, showing the JSON structure so the user can see exactly what they'll get. That way, they can easily copy it or use it in their application.<br><br>I wonder if the user is a developer working on a project that requires population data. They might need it in JSON for integration or analysis. So providing the exact format they asked for is important. I should also make sure the response is concise and to the point, without any extra fluff.<br><br>Is there anything else I should consider? Maybe the date of the population count, but the user didn't specify that. I'll stick to the most recent data available. Also, I should ensure that the JSON is valid, so using proper syntax with quotes and commas.<br><br>In summary, I'll provide the JSON with the correct capital name and population number, making sure it's accurate and in the format the user requested. That should meet their needs effectively.<br><br><br>content: {\"name\": \"Paris\", \"population\": 3600000}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"foo\",\n",
    "            # convert the pydantic model to json schema\n",
    "            \"schema\": CapitalInfo.model_json_schema(),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:37:19.325905Z",
     "iopub.status.busy": "2025-05-15T22:37:19.325722Z",
     "iopub.status.idle": "2025-05-15T22:37:24.605741Z",
     "shell.execute_reply": "2025-05-15T22:37:24.605168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:19] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:19] Decode batch. #running-req: 1, #token: 52, token usage: 0.00, cuda graph: False, gen throughput (token/s): 58.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:20] Decode batch. #running-req: 1, #token: 92, token usage: 0.00, cuda graph: False, gen throughput (token/s): 66.22, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:21] Decode batch. #running-req: 1, #token: 132, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:21] Decode batch. #running-req: 1, #token: 172, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.62, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:22] Decode batch. #running-req: 1, #token: 212, token usage: 0.01, cuda graph: False, gen throughput (token/s): 61.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:22] Decode batch. #running-req: 1, #token: 252, token usage: 0.01, cuda graph: False, gen throughput (token/s): 62.59, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:23] Decode batch. #running-req: 1, #token: 292, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.38, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:24] Decode batch. #running-req: 1, #token: 332, token usage: 0.02, cuda graph: False, gen throughput (token/s): 66.19, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:24] INFO:     127.0.0.1:48690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user asked for the information and population of the capital of France in JSON format. I immediately thought about what the capital is. Paris is the capital, no doubt there. Now, I need to recall or look up the population. I remember it's a big city, so the population is in the millions. I think it's around 2 million, but I'm not 100% sure. Maybe I should double-check that. <br><br>Wait, I should consider the exact figure. I believe the population is approximately 2,175,000 as of 2023. That seems right, but I'm not certain. I should make sure to present it accurately. Also, the user wants it in JSON format, so I need to structure it properly with the key \"capital\" and the value as a string. <br><br>I wonder if the user is a student working on a project or maybe a developer needing data for an app. Either way, providing the correct and up-to-date information is crucial. Maybe they also need this for a presentation or a report, so accuracy is key. <br><br>I should also consider if there are any other details they might need, like the area or the time zone, but the user specifically asked for population. So I'll stick to that. I'll format the JSON correctly, making sure the syntax is right to avoid any errors. <br><br>In summary, I'll provide the JSON with the correct population number for Paris, the capital of France, ensuring it's accurate and formatted as requested.<br><br><br>content: {\"name\": \"Paris\", \"population\": 2175000}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\"name\": \"foo\", \"schema\": json.loads(json_schema)},\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:37:24.607871Z",
     "iopub.status.busy": "2025-05-15T22:37:24.607561Z",
     "iopub.status.idle": "2025-05-15T22:37:30.696147Z",
     "shell.execute_reply": "2025-05-15T22:37:30.695438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:24] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:24] Decode batch. #running-req: 1, #token: 42, token usage: 0.00, cuda graph: False, gen throughput (token/s): 58.82, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:25] Decode batch. #running-req: 1, #token: 82, token usage: 0.00, cuda graph: False, gen throughput (token/s): 66.13, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:26] Decode batch. #running-req: 1, #token: 122, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:26] Decode batch. #running-req: 1, #token: 162, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:27] Decode batch. #running-req: 1, #token: 202, token usage: 0.01, cuda graph: False, gen throughput (token/s): 62.18, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:28] Decode batch. #running-req: 1, #token: 242, token usage: 0.01, cuda graph: False, gen throughput (token/s): 60.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:28] Decode batch. #running-req: 1, #token: 282, token usage: 0.01, cuda graph: False, gen throughput (token/s): 59.30, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:29] Decode batch. #running-req: 1, #token: 322, token usage: 0.02, cuda graph: False, gen throughput (token/s): 65.64, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:29] Decode batch. #running-req: 1, #token: 362, token usage: 0.02, cuda graph: False, gen throughput (token/s): 66.86, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:30] Decode batch. #running-req: 1, #token: 402, token usage: 0.02, cuda graph: False, gen throughput (token/s): 64.34, #queue-req: 0\n",
      "[2025-05-15 22:37:30] INFO:     127.0.0.1:48690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so the user asked for the information and population of the capital of France in JSON format. I responded with Paris, its population of around 2.1 million, and included some key facts. Now, the user is following up with another query about the population of the United States. They want the data in JSON again, but this time for the entire country.<br><br>Hmm, I need to make sure I provide accurate and up-to-date information. The population of the US is a big number, so I should check the latest estimates. I remember it's over 300 million, maybe around 332 million as of 2023. I should include that. Also, adding some key facts about the US population would be helpful, like the diversity of states, the fastest-growing state, and the slowest-growing state. That gives a bit more context and makes the information more useful.<br><br>I should structure the JSON properly, making sure the keys are clear and the data is easy to read. Maybe include the country name, population, and the key facts as an array. I need to ensure the JSON syntax is correct, with proper commas and brackets. Also, I should avoid any markdown formatting as per the instructions, keeping it plain text.<br><br>Wait, should I mention the growth rate? The user didn't ask for it, but it's a related fact. Maybe just stick to what they asked for unless they specify more details. I'll focus on the population and the key facts they provided. <br><br>Double-checking the population number is crucial. I think it's 332.8 million as of the latest data, so I'll use that. For the key facts, I'll list the most populous and least populous states. That should cover the user's needs and provide a comprehensive answer in the JSON format they requested.<br><br><br>content: Rome is the capital of Italy</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ebnf_grammar = \"\"\"\n",
    "root ::= city | description\n",
    "city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\n",
    "description ::= city \" is \" status\n",
    "status ::= \"the capital of \" country\n",
    "country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful geography bot.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"ebnf\": ebnf_grammar},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:37:30.698232Z",
     "iopub.status.busy": "2025-05-15T22:37:30.697868Z",
     "iopub.status.idle": "2025-05-15T22:37:32.282888Z",
     "shell.execute_reply": "2025-05-15T22:37:32.282143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:30] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:31] Decode batch. #running-req: 1, #token: 41, token usage: 0.00, cuda graph: False, gen throughput (token/s): 61.07, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:31] Decode batch. #running-req: 1, #token: 81, token usage: 0.00, cuda graph: False, gen throughput (token/s): 66.64, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:32] INFO:     127.0.0.1:48690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user just asked, \"What is the capital of France?\" Hmm, that's a pretty straightforward question. I should make sure I provide a clear and accurate answer. Let me think, Paris is definitely the capital. But wait, is there any chance I might have mixed it up with another country? No, I'm pretty sure France's capital is Paris. Maybe I should double-check that to be safe. Yeah, Paris is correct. I'll go with that.<br><br><br>content: London</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"assistant\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"regex\": \"(Paris|London)\"},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:37:32.285268Z",
     "iopub.status.busy": "2025-05-15T22:37:32.284913Z",
     "iopub.status.idle": "2025-05-15T22:41:42.856749Z",
     "shell.execute_reply": "2025-05-15T22:41:42.856126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:32] Prefill batch. #new-seq: 1, #new-token: 472, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:32] Decode batch. #running-req: 1, #token: 480, token usage: 0.02, cuda graph: False, gen throughput (token/s): 39.63, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:33] Decode batch. #running-req: 1, #token: 520, token usage: 0.03, cuda graph: False, gen throughput (token/s): 61.92, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:34] Decode batch. #running-req: 1, #token: 560, token usage: 0.03, cuda graph: False, gen throughput (token/s): 61.83, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:34] Decode batch. #running-req: 1, #token: 600, token usage: 0.03, cuda graph: False, gen throughput (token/s): 64.14, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:35] Decode batch. #running-req: 1, #token: 640, token usage: 0.03, cuda graph: False, gen throughput (token/s): 62.85, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:35] Decode batch. #running-req: 1, #token: 680, token usage: 0.03, cuda graph: False, gen throughput (token/s): 62.35, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:36] Decode batch. #running-req: 1, #token: 720, token usage: 0.04, cuda graph: False, gen throughput (token/s): 64.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:37] Decode batch. #running-req: 1, #token: 760, token usage: 0.04, cuda graph: False, gen throughput (token/s): 65.10, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:37] Decode batch. #running-req: 1, #token: 800, token usage: 0.04, cuda graph: False, gen throughput (token/s): 65.26, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:38] Decode batch. #running-req: 1, #token: 840, token usage: 0.04, cuda graph: False, gen throughput (token/s): 65.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:39] Decode batch. #running-req: 1, #token: 880, token usage: 0.04, cuda graph: False, gen throughput (token/s): 62.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:39] Decode batch. #running-req: 1, #token: 920, token usage: 0.04, cuda graph: False, gen throughput (token/s): 60.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:40] Decode batch. #running-req: 1, #token: 960, token usage: 0.05, cuda graph: False, gen throughput (token/s): 60.92, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:41] Decode batch. #running-req: 1, #token: 1000, token usage: 0.05, cuda graph: False, gen throughput (token/s): 60.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:41] Decode batch. #running-req: 1, #token: 1040, token usage: 0.05, cuda graph: False, gen throughput (token/s): 57.51, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:42] Decode batch. #running-req: 1, #token: 1080, token usage: 0.05, cuda graph: False, gen throughput (token/s): 62.59, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:43] Decode batch. #running-req: 1, #token: 1120, token usage: 0.05, cuda graph: False, gen throughput (token/s): 62.59, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:43] Decode batch. #running-req: 1, #token: 1160, token usage: 0.06, cuda graph: False, gen throughput (token/s): 62.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:44] Decode batch. #running-req: 1, #token: 1200, token usage: 0.06, cuda graph: False, gen throughput (token/s): 61.68, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:44] Decode batch. #running-req: 1, #token: 1240, token usage: 0.06, cuda graph: False, gen throughput (token/s): 62.09, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:45] Decode batch. #running-req: 1, #token: 1280, token usage: 0.06, cuda graph: False, gen throughput (token/s): 62.48, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:46] Decode batch. #running-req: 1, #token: 1320, token usage: 0.06, cuda graph: False, gen throughput (token/s): 61.07, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:46] Decode batch. #running-req: 1, #token: 1360, token usage: 0.07, cuda graph: False, gen throughput (token/s): 62.14, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:47] Decode batch. #running-req: 1, #token: 1400, token usage: 0.07, cuda graph: False, gen throughput (token/s): 62.25, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:48] Decode batch. #running-req: 1, #token: 1440, token usage: 0.07, cuda graph: False, gen throughput (token/s): 60.22, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:48] Decode batch. #running-req: 1, #token: 1480, token usage: 0.07, cuda graph: False, gen throughput (token/s): 58.55, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:49] Decode batch. #running-req: 1, #token: 1520, token usage: 0.07, cuda graph: False, gen throughput (token/s): 56.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:50] Decode batch. #running-req: 1, #token: 1560, token usage: 0.08, cuda graph: False, gen throughput (token/s): 61.89, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:50] Decode batch. #running-req: 1, #token: 1600, token usage: 0.08, cuda graph: False, gen throughput (token/s): 61.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:51] Decode batch. #running-req: 1, #token: 1640, token usage: 0.08, cuda graph: False, gen throughput (token/s): 61.94, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:52] Decode batch. #running-req: 1, #token: 1680, token usage: 0.08, cuda graph: False, gen throughput (token/s): 63.69, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:52] Decode batch. #running-req: 1, #token: 1720, token usage: 0.08, cuda graph: False, gen throughput (token/s): 63.81, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:53] Decode batch. #running-req: 1, #token: 1760, token usage: 0.09, cuda graph: False, gen throughput (token/s): 63.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:54] Decode batch. #running-req: 1, #token: 1800, token usage: 0.09, cuda graph: False, gen throughput (token/s): 63.62, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:54] Decode batch. #running-req: 1, #token: 1840, token usage: 0.09, cuda graph: False, gen throughput (token/s): 63.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:55] Decode batch. #running-req: 1, #token: 1880, token usage: 0.09, cuda graph: False, gen throughput (token/s): 62.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:55] Decode batch. #running-req: 1, #token: 1920, token usage: 0.09, cuda graph: False, gen throughput (token/s): 62.71, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:56] Decode batch. #running-req: 1, #token: 1960, token usage: 0.10, cuda graph: False, gen throughput (token/s): 63.76, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:57] Decode batch. #running-req: 1, #token: 2000, token usage: 0.10, cuda graph: False, gen throughput (token/s): 63.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:57] Decode batch. #running-req: 1, #token: 2040, token usage: 0.10, cuda graph: False, gen throughput (token/s): 63.34, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:58] Decode batch. #running-req: 1, #token: 2080, token usage: 0.10, cuda graph: False, gen throughput (token/s): 63.67, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:59] Decode batch. #running-req: 1, #token: 2120, token usage: 0.10, cuda graph: False, gen throughput (token/s): 63.34, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:37:59] Decode batch. #running-req: 1, #token: 2160, token usage: 0.11, cuda graph: False, gen throughput (token/s): 63.25, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:00] Decode batch. #running-req: 1, #token: 2200, token usage: 0.11, cuda graph: False, gen throughput (token/s): 63.50, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:00] Decode batch. #running-req: 1, #token: 2240, token usage: 0.11, cuda graph: False, gen throughput (token/s): 63.08, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:01] Decode batch. #running-req: 1, #token: 2280, token usage: 0.11, cuda graph: False, gen throughput (token/s): 63.54, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:02] Decode batch. #running-req: 1, #token: 2320, token usage: 0.11, cuda graph: False, gen throughput (token/s): 61.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:02] Decode batch. #running-req: 1, #token: 2360, token usage: 0.12, cuda graph: False, gen throughput (token/s): 62.20, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:03] Decode batch. #running-req: 1, #token: 2400, token usage: 0.12, cuda graph: False, gen throughput (token/s): 61.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:04] Decode batch. #running-req: 1, #token: 2440, token usage: 0.12, cuda graph: False, gen throughput (token/s): 59.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:04] Decode batch. #running-req: 1, #token: 2480, token usage: 0.12, cuda graph: False, gen throughput (token/s): 61.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:05] Decode batch. #running-req: 1, #token: 2520, token usage: 0.12, cuda graph: False, gen throughput (token/s): 62.08, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:06] Decode batch. #running-req: 1, #token: 2560, token usage: 0.12, cuda graph: False, gen throughput (token/s): 62.20, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:06] Decode batch. #running-req: 1, #token: 2600, token usage: 0.13, cuda graph: False, gen throughput (token/s): 62.08, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:07] Decode batch. #running-req: 1, #token: 2640, token usage: 0.13, cuda graph: False, gen throughput (token/s): 62.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:08] Decode batch. #running-req: 1, #token: 2680, token usage: 0.13, cuda graph: False, gen throughput (token/s): 64.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:08] Decode batch. #running-req: 1, #token: 2720, token usage: 0.13, cuda graph: False, gen throughput (token/s): 64.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:09] Decode batch. #running-req: 1, #token: 2760, token usage: 0.13, cuda graph: False, gen throughput (token/s): 64.16, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:09] Decode batch. #running-req: 1, #token: 2800, token usage: 0.14, cuda graph: False, gen throughput (token/s): 63.53, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:10] Decode batch. #running-req: 1, #token: 2840, token usage: 0.14, cuda graph: False, gen throughput (token/s): 63.38, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:11] Decode batch. #running-req: 1, #token: 2880, token usage: 0.14, cuda graph: False, gen throughput (token/s): 63.45, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:11] Decode batch. #running-req: 1, #token: 2920, token usage: 0.14, cuda graph: False, gen throughput (token/s): 63.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:12] Decode batch. #running-req: 1, #token: 2960, token usage: 0.14, cuda graph: False, gen throughput (token/s): 63.45, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:13] Decode batch. #running-req: 1, #token: 3000, token usage: 0.15, cuda graph: False, gen throughput (token/s): 61.08, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:13] Decode batch. #running-req: 1, #token: 3040, token usage: 0.15, cuda graph: False, gen throughput (token/s): 59.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:14] Decode batch. #running-req: 1, #token: 3080, token usage: 0.15, cuda graph: False, gen throughput (token/s): 58.98, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:15] Decode batch. #running-req: 1, #token: 3120, token usage: 0.15, cuda graph: False, gen throughput (token/s): 62.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:15] Decode batch. #running-req: 1, #token: 3160, token usage: 0.15, cuda graph: False, gen throughput (token/s): 57.02, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:16] Decode batch. #running-req: 1, #token: 3200, token usage: 0.16, cuda graph: False, gen throughput (token/s): 61.86, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:17] Decode batch. #running-req: 1, #token: 3240, token usage: 0.16, cuda graph: False, gen throughput (token/s): 61.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:17] Decode batch. #running-req: 1, #token: 3280, token usage: 0.16, cuda graph: False, gen throughput (token/s): 58.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:18] Decode batch. #running-req: 1, #token: 3320, token usage: 0.16, cuda graph: False, gen throughput (token/s): 62.30, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:19] Decode batch. #running-req: 1, #token: 3360, token usage: 0.16, cuda graph: False, gen throughput (token/s): 62.70, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:19] Decode batch. #running-req: 1, #token: 3400, token usage: 0.17, cuda graph: False, gen throughput (token/s): 60.11, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:20] Decode batch. #running-req: 1, #token: 3440, token usage: 0.17, cuda graph: False, gen throughput (token/s): 58.64, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:21] Decode batch. #running-req: 1, #token: 3480, token usage: 0.17, cuda graph: False, gen throughput (token/s): 56.92, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:21] Decode batch. #running-req: 1, #token: 3520, token usage: 0.17, cuda graph: False, gen throughput (token/s): 61.27, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:22] Decode batch. #running-req: 1, #token: 3560, token usage: 0.17, cuda graph: False, gen throughput (token/s): 61.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:23] Decode batch. #running-req: 1, #token: 3600, token usage: 0.18, cuda graph: False, gen throughput (token/s): 62.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:23] Decode batch. #running-req: 1, #token: 3640, token usage: 0.18, cuda graph: False, gen throughput (token/s): 62.63, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:24] Decode batch. #running-req: 1, #token: 3680, token usage: 0.18, cuda graph: False, gen throughput (token/s): 61.94, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:24] Decode batch. #running-req: 1, #token: 3720, token usage: 0.18, cuda graph: False, gen throughput (token/s): 62.79, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:25] Decode batch. #running-req: 1, #token: 3760, token usage: 0.18, cuda graph: False, gen throughput (token/s): 62.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:26] Decode batch. #running-req: 1, #token: 3800, token usage: 0.19, cuda graph: False, gen throughput (token/s): 61.35, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:26] Decode batch. #running-req: 1, #token: 3840, token usage: 0.19, cuda graph: False, gen throughput (token/s): 62.24, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:27] Decode batch. #running-req: 1, #token: 3880, token usage: 0.19, cuda graph: False, gen throughput (token/s): 61.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:28] Decode batch. #running-req: 1, #token: 3920, token usage: 0.19, cuda graph: False, gen throughput (token/s): 62.43, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:28] Decode batch. #running-req: 1, #token: 3960, token usage: 0.19, cuda graph: False, gen throughput (token/s): 62.13, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:29] Decode batch. #running-req: 1, #token: 4000, token usage: 0.20, cuda graph: False, gen throughput (token/s): 62.12, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:30] Decode batch. #running-req: 1, #token: 4040, token usage: 0.20, cuda graph: False, gen throughput (token/s): 62.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:30] Decode batch. #running-req: 1, #token: 4080, token usage: 0.20, cuda graph: False, gen throughput (token/s): 62.11, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:31] Decode batch. #running-req: 1, #token: 4120, token usage: 0.20, cuda graph: False, gen throughput (token/s): 60.05, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:32] Decode batch. #running-req: 1, #token: 4160, token usage: 0.20, cuda graph: False, gen throughput (token/s): 63.98, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:32] Decode batch. #running-req: 1, #token: 4200, token usage: 0.21, cuda graph: False, gen throughput (token/s): 64.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:33] Decode batch. #running-req: 1, #token: 4240, token usage: 0.21, cuda graph: False, gen throughput (token/s): 62.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:33] Decode batch. #running-req: 1, #token: 4280, token usage: 0.21, cuda graph: False, gen throughput (token/s): 61.35, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:34] Decode batch. #running-req: 1, #token: 4320, token usage: 0.21, cuda graph: False, gen throughput (token/s): 62.81, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:35] Decode batch. #running-req: 1, #token: 4360, token usage: 0.21, cuda graph: False, gen throughput (token/s): 64.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:35] Decode batch. #running-req: 1, #token: 4400, token usage: 0.21, cuda graph: False, gen throughput (token/s): 60.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:36] Decode batch. #running-req: 1, #token: 4440, token usage: 0.22, cuda graph: False, gen throughput (token/s): 55.69, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:37] Decode batch. #running-req: 1, #token: 4480, token usage: 0.22, cuda graph: False, gen throughput (token/s): 58.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:37] Decode batch. #running-req: 1, #token: 4520, token usage: 0.22, cuda graph: False, gen throughput (token/s): 58.85, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:38] Decode batch. #running-req: 1, #token: 4560, token usage: 0.22, cuda graph: False, gen throughput (token/s): 58.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:39] Decode batch. #running-req: 1, #token: 4600, token usage: 0.22, cuda graph: False, gen throughput (token/s): 61.17, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:39] Decode batch. #running-req: 1, #token: 4640, token usage: 0.23, cuda graph: False, gen throughput (token/s): 61.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:40] Decode batch. #running-req: 1, #token: 4680, token usage: 0.23, cuda graph: False, gen throughput (token/s): 62.00, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:41] Decode batch. #running-req: 1, #token: 4720, token usage: 0.23, cuda graph: False, gen throughput (token/s): 59.27, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:41] Decode batch. #running-req: 1, #token: 4760, token usage: 0.23, cuda graph: False, gen throughput (token/s): 60.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:42] Decode batch. #running-req: 1, #token: 4800, token usage: 0.23, cuda graph: False, gen throughput (token/s): 63.58, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:43] Decode batch. #running-req: 1, #token: 4840, token usage: 0.24, cuda graph: False, gen throughput (token/s): 62.35, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:43] Decode batch. #running-req: 1, #token: 4880, token usage: 0.24, cuda graph: False, gen throughput (token/s): 62.38, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:44] Decode batch. #running-req: 1, #token: 4920, token usage: 0.24, cuda graph: False, gen throughput (token/s): 62.11, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:45] Decode batch. #running-req: 1, #token: 4960, token usage: 0.24, cuda graph: False, gen throughput (token/s): 62.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:45] Decode batch. #running-req: 1, #token: 5000, token usage: 0.24, cuda graph: False, gen throughput (token/s): 61.05, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:46] Decode batch. #running-req: 1, #token: 5040, token usage: 0.25, cuda graph: False, gen throughput (token/s): 62.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:47] Decode batch. #running-req: 1, #token: 5080, token usage: 0.25, cuda graph: False, gen throughput (token/s): 61.24, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:47] Decode batch. #running-req: 1, #token: 5120, token usage: 0.25, cuda graph: False, gen throughput (token/s): 61.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:48] Decode batch. #running-req: 1, #token: 5160, token usage: 0.25, cuda graph: False, gen throughput (token/s): 64.19, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:49] Decode batch. #running-req: 1, #token: 5200, token usage: 0.25, cuda graph: False, gen throughput (token/s): 58.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:49] Decode batch. #running-req: 1, #token: 5240, token usage: 0.26, cuda graph: False, gen throughput (token/s): 63.47, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:50] Decode batch. #running-req: 1, #token: 5280, token usage: 0.26, cuda graph: False, gen throughput (token/s): 63.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:50] Decode batch. #running-req: 1, #token: 5320, token usage: 0.26, cuda graph: False, gen throughput (token/s): 62.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:51] Decode batch. #running-req: 1, #token: 5360, token usage: 0.26, cuda graph: False, gen throughput (token/s): 63.28, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:52] Decode batch. #running-req: 1, #token: 5400, token usage: 0.26, cuda graph: False, gen throughput (token/s): 63.71, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:52] Decode batch. #running-req: 1, #token: 5440, token usage: 0.27, cuda graph: False, gen throughput (token/s): 59.91, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:53] Decode batch. #running-req: 1, #token: 5480, token usage: 0.27, cuda graph: False, gen throughput (token/s): 63.24, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:54] Decode batch. #running-req: 1, #token: 5520, token usage: 0.27, cuda graph: False, gen throughput (token/s): 62.50, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:54] Decode batch. #running-req: 1, #token: 5560, token usage: 0.27, cuda graph: False, gen throughput (token/s): 57.74, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:55] Decode batch. #running-req: 1, #token: 5600, token usage: 0.27, cuda graph: False, gen throughput (token/s): 60.44, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:56] Decode batch. #running-req: 1, #token: 5640, token usage: 0.28, cuda graph: False, gen throughput (token/s): 63.60, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:56] Decode batch. #running-req: 1, #token: 5680, token usage: 0.28, cuda graph: False, gen throughput (token/s): 61.74, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:57] Decode batch. #running-req: 1, #token: 5720, token usage: 0.28, cuda graph: False, gen throughput (token/s): 65.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:57] Decode batch. #running-req: 1, #token: 5760, token usage: 0.28, cuda graph: False, gen throughput (token/s): 64.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:58] Decode batch. #running-req: 1, #token: 5800, token usage: 0.28, cuda graph: False, gen throughput (token/s): 64.17, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:59] Decode batch. #running-req: 1, #token: 5840, token usage: 0.29, cuda graph: False, gen throughput (token/s): 64.45, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:38:59] Decode batch. #running-req: 1, #token: 5880, token usage: 0.29, cuda graph: False, gen throughput (token/s): 65.34, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:00] Decode batch. #running-req: 1, #token: 5920, token usage: 0.29, cuda graph: False, gen throughput (token/s): 64.04, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:01] Decode batch. #running-req: 1, #token: 5960, token usage: 0.29, cuda graph: False, gen throughput (token/s): 60.99, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:01] Decode batch. #running-req: 1, #token: 6000, token usage: 0.29, cuda graph: False, gen throughput (token/s): 63.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:02] Decode batch. #running-req: 1, #token: 6040, token usage: 0.29, cuda graph: False, gen throughput (token/s): 61.55, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:03] Decode batch. #running-req: 1, #token: 6080, token usage: 0.30, cuda graph: False, gen throughput (token/s): 61.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:03] Decode batch. #running-req: 1, #token: 6120, token usage: 0.30, cuda graph: False, gen throughput (token/s): 62.99, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:04] Decode batch. #running-req: 1, #token: 6160, token usage: 0.30, cuda graph: False, gen throughput (token/s): 64.23, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:04] Decode batch. #running-req: 1, #token: 6200, token usage: 0.30, cuda graph: False, gen throughput (token/s): 59.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:05] Decode batch. #running-req: 1, #token: 6240, token usage: 0.30, cuda graph: False, gen throughput (token/s): 63.33, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:06] Decode batch. #running-req: 1, #token: 6280, token usage: 0.31, cuda graph: False, gen throughput (token/s): 52.55, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:07] Decode batch. #running-req: 1, #token: 6320, token usage: 0.31, cuda graph: False, gen throughput (token/s): 49.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:07] Decode batch. #running-req: 1, #token: 6360, token usage: 0.31, cuda graph: False, gen throughput (token/s): 49.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:08] Decode batch. #running-req: 1, #token: 6400, token usage: 0.31, cuda graph: False, gen throughput (token/s): 54.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:09] Decode batch. #running-req: 1, #token: 6440, token usage: 0.31, cuda graph: False, gen throughput (token/s): 62.54, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:10] Decode batch. #running-req: 1, #token: 6480, token usage: 0.32, cuda graph: False, gen throughput (token/s): 61.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:10] Decode batch. #running-req: 1, #token: 6520, token usage: 0.32, cuda graph: False, gen throughput (token/s): 60.93, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:11] Decode batch. #running-req: 1, #token: 6560, token usage: 0.32, cuda graph: False, gen throughput (token/s): 62.53, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:11] Decode batch. #running-req: 1, #token: 6600, token usage: 0.32, cuda graph: False, gen throughput (token/s): 61.68, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:12] Decode batch. #running-req: 1, #token: 6640, token usage: 0.32, cuda graph: False, gen throughput (token/s): 61.70, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:13] Decode batch. #running-req: 1, #token: 6680, token usage: 0.33, cuda graph: False, gen throughput (token/s): 63.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:13] Decode batch. #running-req: 1, #token: 6720, token usage: 0.33, cuda graph: False, gen throughput (token/s): 63.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:14] Decode batch. #running-req: 1, #token: 6760, token usage: 0.33, cuda graph: False, gen throughput (token/s): 63.76, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:15] Decode batch. #running-req: 1, #token: 6800, token usage: 0.33, cuda graph: False, gen throughput (token/s): 62.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:15] Decode batch. #running-req: 1, #token: 6840, token usage: 0.33, cuda graph: False, gen throughput (token/s): 63.17, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:16] Decode batch. #running-req: 1, #token: 6880, token usage: 0.34, cuda graph: False, gen throughput (token/s): 63.93, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:17] Decode batch. #running-req: 1, #token: 6920, token usage: 0.34, cuda graph: False, gen throughput (token/s): 63.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:17] Decode batch. #running-req: 1, #token: 6960, token usage: 0.34, cuda graph: False, gen throughput (token/s): 60.91, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:18] Decode batch. #running-req: 1, #token: 7000, token usage: 0.34, cuda graph: False, gen throughput (token/s): 63.72, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:18] Decode batch. #running-req: 1, #token: 7040, token usage: 0.34, cuda graph: False, gen throughput (token/s): 63.66, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:19] Decode batch. #running-req: 1, #token: 7080, token usage: 0.35, cuda graph: False, gen throughput (token/s): 62.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:20] Decode batch. #running-req: 1, #token: 7120, token usage: 0.35, cuda graph: False, gen throughput (token/s): 62.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:20] Decode batch. #running-req: 1, #token: 7160, token usage: 0.35, cuda graph: False, gen throughput (token/s): 58.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:21] Decode batch. #running-req: 1, #token: 7200, token usage: 0.35, cuda graph: False, gen throughput (token/s): 63.79, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:22] Decode batch. #running-req: 1, #token: 7240, token usage: 0.35, cuda graph: False, gen throughput (token/s): 59.92, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:22] Decode batch. #running-req: 1, #token: 7280, token usage: 0.36, cuda graph: False, gen throughput (token/s): 63.82, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:23] Decode batch. #running-req: 1, #token: 7320, token usage: 0.36, cuda graph: False, gen throughput (token/s): 63.70, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:24] Decode batch. #running-req: 1, #token: 7360, token usage: 0.36, cuda graph: False, gen throughput (token/s): 63.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:24] Decode batch. #running-req: 1, #token: 7400, token usage: 0.36, cuda graph: False, gen throughput (token/s): 63.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:25] Decode batch. #running-req: 1, #token: 7440, token usage: 0.36, cuda graph: False, gen throughput (token/s): 63.73, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:25] Decode batch. #running-req: 1, #token: 7480, token usage: 0.37, cuda graph: False, gen throughput (token/s): 63.85, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:26] Decode batch. #running-req: 1, #token: 7520, token usage: 0.37, cuda graph: False, gen throughput (token/s): 59.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:27] Decode batch. #running-req: 1, #token: 7560, token usage: 0.37, cuda graph: False, gen throughput (token/s): 62.35, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:27] Decode batch. #running-req: 1, #token: 7600, token usage: 0.37, cuda graph: False, gen throughput (token/s): 60.82, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:28] Decode batch. #running-req: 1, #token: 7640, token usage: 0.37, cuda graph: False, gen throughput (token/s): 61.51, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:29] Decode batch. #running-req: 1, #token: 7680, token usage: 0.38, cuda graph: False, gen throughput (token/s): 62.27, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:29] Decode batch. #running-req: 1, #token: 7720, token usage: 0.38, cuda graph: False, gen throughput (token/s): 65.05, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:30] Decode batch. #running-req: 1, #token: 7760, token usage: 0.38, cuda graph: False, gen throughput (token/s): 65.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:31] Decode batch. #running-req: 1, #token: 7800, token usage: 0.38, cuda graph: False, gen throughput (token/s): 65.45, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:31] Decode batch. #running-req: 1, #token: 7840, token usage: 0.38, cuda graph: False, gen throughput (token/s): 65.48, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:32] Decode batch. #running-req: 1, #token: 7880, token usage: 0.38, cuda graph: False, gen throughput (token/s): 63.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:32] Decode batch. #running-req: 1, #token: 7920, token usage: 0.39, cuda graph: False, gen throughput (token/s): 65.26, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:33] Decode batch. #running-req: 1, #token: 7960, token usage: 0.39, cuda graph: False, gen throughput (token/s): 64.38, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:34] Decode batch. #running-req: 1, #token: 8000, token usage: 0.39, cuda graph: False, gen throughput (token/s): 63.33, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:34] Decode batch. #running-req: 1, #token: 8040, token usage: 0.39, cuda graph: False, gen throughput (token/s): 61.92, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:35] Decode batch. #running-req: 1, #token: 8080, token usage: 0.39, cuda graph: False, gen throughput (token/s): 64.02, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:36] Decode batch. #running-req: 1, #token: 8120, token usage: 0.40, cuda graph: False, gen throughput (token/s): 63.48, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:36] Decode batch. #running-req: 1, #token: 8160, token usage: 0.40, cuda graph: False, gen throughput (token/s): 63.11, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:37] Decode batch. #running-req: 1, #token: 8200, token usage: 0.40, cuda graph: False, gen throughput (token/s): 63.82, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:37] Decode batch. #running-req: 1, #token: 8240, token usage: 0.40, cuda graph: False, gen throughput (token/s): 64.54, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:38] Decode batch. #running-req: 1, #token: 8280, token usage: 0.40, cuda graph: False, gen throughput (token/s): 64.13, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:39] Decode batch. #running-req: 1, #token: 8320, token usage: 0.41, cuda graph: False, gen throughput (token/s): 61.11, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:39] Decode batch. #running-req: 1, #token: 8360, token usage: 0.41, cuda graph: False, gen throughput (token/s): 62.64, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:40] Decode batch. #running-req: 1, #token: 8400, token usage: 0.41, cuda graph: False, gen throughput (token/s): 63.54, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:41] Decode batch. #running-req: 1, #token: 8440, token usage: 0.41, cuda graph: False, gen throughput (token/s): 63.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:41] Decode batch. #running-req: 1, #token: 8480, token usage: 0.41, cuda graph: False, gen throughput (token/s): 63.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:42] Decode batch. #running-req: 1, #token: 8520, token usage: 0.42, cuda graph: False, gen throughput (token/s): 64.11, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:42] Decode batch. #running-req: 1, #token: 8560, token usage: 0.42, cuda graph: False, gen throughput (token/s): 64.43, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:43] Decode batch. #running-req: 1, #token: 8600, token usage: 0.42, cuda graph: False, gen throughput (token/s): 64.37, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:44] Decode batch. #running-req: 1, #token: 8640, token usage: 0.42, cuda graph: False, gen throughput (token/s): 62.59, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:44] Decode batch. #running-req: 1, #token: 8680, token usage: 0.42, cuda graph: False, gen throughput (token/s): 59.85, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:45] Decode batch. #running-req: 1, #token: 8720, token usage: 0.43, cuda graph: False, gen throughput (token/s): 62.89, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:46] Decode batch. #running-req: 1, #token: 8760, token usage: 0.43, cuda graph: False, gen throughput (token/s): 62.83, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:46] Decode batch. #running-req: 1, #token: 8800, token usage: 0.43, cuda graph: False, gen throughput (token/s): 61.33, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:47] Decode batch. #running-req: 1, #token: 8840, token usage: 0.43, cuda graph: False, gen throughput (token/s): 64.63, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:48] Decode batch. #running-req: 1, #token: 8880, token usage: 0.43, cuda graph: False, gen throughput (token/s): 64.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:48] Decode batch. #running-req: 1, #token: 8920, token usage: 0.44, cuda graph: False, gen throughput (token/s): 64.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:49] Decode batch. #running-req: 1, #token: 8960, token usage: 0.44, cuda graph: False, gen throughput (token/s): 65.42, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:49] Decode batch. #running-req: 1, #token: 9000, token usage: 0.44, cuda graph: False, gen throughput (token/s): 64.92, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:50] Decode batch. #running-req: 1, #token: 9040, token usage: 0.44, cuda graph: False, gen throughput (token/s): 59.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:51] Decode batch. #running-req: 1, #token: 9080, token usage: 0.44, cuda graph: False, gen throughput (token/s): 63.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:51] Decode batch. #running-req: 1, #token: 9120, token usage: 0.45, cuda graph: False, gen throughput (token/s): 65.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:52] Decode batch. #running-req: 1, #token: 9160, token usage: 0.45, cuda graph: False, gen throughput (token/s): 63.68, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:53] Decode batch. #running-req: 1, #token: 9200, token usage: 0.45, cuda graph: False, gen throughput (token/s): 64.79, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:53] Decode batch. #running-req: 1, #token: 9240, token usage: 0.45, cuda graph: False, gen throughput (token/s): 64.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:54] Decode batch. #running-req: 1, #token: 9280, token usage: 0.45, cuda graph: False, gen throughput (token/s): 63.07, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:54] Decode batch. #running-req: 1, #token: 9320, token usage: 0.46, cuda graph: False, gen throughput (token/s): 62.36, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:55] Decode batch. #running-req: 1, #token: 9360, token usage: 0.46, cuda graph: False, gen throughput (token/s): 62.61, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:56] Decode batch. #running-req: 1, #token: 9400, token usage: 0.46, cuda graph: False, gen throughput (token/s): 64.25, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:56] Decode batch. #running-req: 1, #token: 9440, token usage: 0.46, cuda graph: False, gen throughput (token/s): 64.86, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:57] Decode batch. #running-req: 1, #token: 9480, token usage: 0.46, cuda graph: False, gen throughput (token/s): 64.92, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:58] Decode batch. #running-req: 1, #token: 9520, token usage: 0.46, cuda graph: False, gen throughput (token/s): 65.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:58] Decode batch. #running-req: 1, #token: 9560, token usage: 0.47, cuda graph: False, gen throughput (token/s): 65.00, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:59] Decode batch. #running-req: 1, #token: 9600, token usage: 0.47, cuda graph: False, gen throughput (token/s): 65.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:39:59] Decode batch. #running-req: 1, #token: 9640, token usage: 0.47, cuda graph: False, gen throughput (token/s): 65.03, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:00] Decode batch. #running-req: 1, #token: 9680, token usage: 0.47, cuda graph: False, gen throughput (token/s): 65.11, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:01] Decode batch. #running-req: 1, #token: 9720, token usage: 0.47, cuda graph: False, gen throughput (token/s): 64.36, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:01] Decode batch. #running-req: 1, #token: 9760, token usage: 0.48, cuda graph: False, gen throughput (token/s): 64.69, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:02] Decode batch. #running-req: 1, #token: 9800, token usage: 0.48, cuda graph: False, gen throughput (token/s): 64.47, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:03] Decode batch. #running-req: 1, #token: 9840, token usage: 0.48, cuda graph: False, gen throughput (token/s): 64.28, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:03] Decode batch. #running-req: 1, #token: 9880, token usage: 0.48, cuda graph: False, gen throughput (token/s): 64.20, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:04] Decode batch. #running-req: 1, #token: 9920, token usage: 0.48, cuda graph: False, gen throughput (token/s): 64.36, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:04] Decode batch. #running-req: 1, #token: 9960, token usage: 0.49, cuda graph: False, gen throughput (token/s): 64.37, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:05] Decode batch. #running-req: 1, #token: 10000, token usage: 0.49, cuda graph: False, gen throughput (token/s): 64.43, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:06] Decode batch. #running-req: 1, #token: 10040, token usage: 0.49, cuda graph: False, gen throughput (token/s): 64.31, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:06] Decode batch. #running-req: 1, #token: 10080, token usage: 0.49, cuda graph: False, gen throughput (token/s): 64.03, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:07] Decode batch. #running-req: 1, #token: 10120, token usage: 0.49, cuda graph: False, gen throughput (token/s): 64.12, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:08] Decode batch. #running-req: 1, #token: 10160, token usage: 0.50, cuda graph: False, gen throughput (token/s): 64.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:08] Decode batch. #running-req: 1, #token: 10200, token usage: 0.50, cuda graph: False, gen throughput (token/s): 62.70, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:09] Decode batch. #running-req: 1, #token: 10240, token usage: 0.50, cuda graph: False, gen throughput (token/s): 61.07, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:09] Decode batch. #running-req: 1, #token: 10280, token usage: 0.50, cuda graph: False, gen throughput (token/s): 64.67, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:10] Decode batch. #running-req: 1, #token: 10320, token usage: 0.50, cuda graph: False, gen throughput (token/s): 64.68, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:11] Decode batch. #running-req: 1, #token: 10360, token usage: 0.51, cuda graph: False, gen throughput (token/s): 57.94, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:11] Decode batch. #running-req: 1, #token: 10400, token usage: 0.51, cuda graph: False, gen throughput (token/s): 61.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:12] Decode batch. #running-req: 1, #token: 10440, token usage: 0.51, cuda graph: False, gen throughput (token/s): 61.74, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:13] Decode batch. #running-req: 1, #token: 10480, token usage: 0.51, cuda graph: False, gen throughput (token/s): 61.83, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:13] Decode batch. #running-req: 1, #token: 10520, token usage: 0.51, cuda graph: False, gen throughput (token/s): 63.00, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:14] Decode batch. #running-req: 1, #token: 10560, token usage: 0.52, cuda graph: False, gen throughput (token/s): 63.59, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:15] Decode batch. #running-req: 1, #token: 10600, token usage: 0.52, cuda graph: False, gen throughput (token/s): 62.70, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:15] Decode batch. #running-req: 1, #token: 10640, token usage: 0.52, cuda graph: False, gen throughput (token/s): 64.73, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:16] Decode batch. #running-req: 1, #token: 10680, token usage: 0.52, cuda graph: False, gen throughput (token/s): 64.62, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:16] Decode batch. #running-req: 1, #token: 10720, token usage: 0.52, cuda graph: False, gen throughput (token/s): 64.68, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:17] Decode batch. #running-req: 1, #token: 10760, token usage: 0.53, cuda graph: False, gen throughput (token/s): 64.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:18] Decode batch. #running-req: 1, #token: 10800, token usage: 0.53, cuda graph: False, gen throughput (token/s): 64.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:18] Decode batch. #running-req: 1, #token: 10840, token usage: 0.53, cuda graph: False, gen throughput (token/s): 64.74, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:19] Decode batch. #running-req: 1, #token: 10880, token usage: 0.53, cuda graph: False, gen throughput (token/s): 64.84, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:20] Decode batch. #running-req: 1, #token: 10920, token usage: 0.53, cuda graph: False, gen throughput (token/s): 64.86, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:20] Decode batch. #running-req: 1, #token: 10960, token usage: 0.54, cuda graph: False, gen throughput (token/s): 64.93, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:21] Decode batch. #running-req: 1, #token: 11000, token usage: 0.54, cuda graph: False, gen throughput (token/s): 64.76, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:21] Decode batch. #running-req: 1, #token: 11040, token usage: 0.54, cuda graph: False, gen throughput (token/s): 64.55, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:22] Decode batch. #running-req: 1, #token: 11080, token usage: 0.54, cuda graph: False, gen throughput (token/s): 64.57, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:23] Decode batch. #running-req: 1, #token: 11120, token usage: 0.54, cuda graph: False, gen throughput (token/s): 64.76, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:23] Decode batch. #running-req: 1, #token: 11160, token usage: 0.54, cuda graph: False, gen throughput (token/s): 64.79, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:24] Decode batch. #running-req: 1, #token: 11200, token usage: 0.55, cuda graph: False, gen throughput (token/s): 64.80, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:24] Decode batch. #running-req: 1, #token: 11240, token usage: 0.55, cuda graph: False, gen throughput (token/s): 64.71, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:25] Decode batch. #running-req: 1, #token: 11280, token usage: 0.55, cuda graph: False, gen throughput (token/s): 64.63, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:26] Decode batch. #running-req: 1, #token: 11320, token usage: 0.55, cuda graph: False, gen throughput (token/s): 64.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:26] Decode batch. #running-req: 1, #token: 11360, token usage: 0.55, cuda graph: False, gen throughput (token/s): 62.64, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:27] Decode batch. #running-req: 1, #token: 11400, token usage: 0.56, cuda graph: False, gen throughput (token/s): 64.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:28] Decode batch. #running-req: 1, #token: 11440, token usage: 0.56, cuda graph: False, gen throughput (token/s): 64.48, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:28] Decode batch. #running-req: 1, #token: 11480, token usage: 0.56, cuda graph: False, gen throughput (token/s): 61.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:29] Decode batch. #running-req: 1, #token: 11520, token usage: 0.56, cuda graph: False, gen throughput (token/s): 63.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:29] Decode batch. #running-req: 1, #token: 11560, token usage: 0.56, cuda graph: False, gen throughput (token/s): 63.98, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:30] Decode batch. #running-req: 1, #token: 11600, token usage: 0.57, cuda graph: False, gen throughput (token/s): 62.74, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:31] Decode batch. #running-req: 1, #token: 11640, token usage: 0.57, cuda graph: False, gen throughput (token/s): 66.53, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:31] Decode batch. #running-req: 1, #token: 11680, token usage: 0.57, cuda graph: False, gen throughput (token/s): 66.50, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:32] Decode batch. #running-req: 1, #token: 11720, token usage: 0.57, cuda graph: False, gen throughput (token/s): 66.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:33] Decode batch. #running-req: 1, #token: 11760, token usage: 0.57, cuda graph: False, gen throughput (token/s): 66.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:33] Decode batch. #running-req: 1, #token: 11800, token usage: 0.58, cuda graph: False, gen throughput (token/s): 58.73, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:34] Decode batch. #running-req: 1, #token: 11840, token usage: 0.58, cuda graph: False, gen throughput (token/s): 59.38, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:34] Decode batch. #running-req: 1, #token: 11880, token usage: 0.58, cuda graph: False, gen throughput (token/s): 64.45, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:35] Decode batch. #running-req: 1, #token: 11920, token usage: 0.58, cuda graph: False, gen throughput (token/s): 65.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:36] Decode batch. #running-req: 1, #token: 11960, token usage: 0.58, cuda graph: False, gen throughput (token/s): 65.07, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:36] Decode batch. #running-req: 1, #token: 12000, token usage: 0.59, cuda graph: False, gen throughput (token/s): 61.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:37] Decode batch. #running-req: 1, #token: 12040, token usage: 0.59, cuda graph: False, gen throughput (token/s): 63.08, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:38] Decode batch. #running-req: 1, #token: 12080, token usage: 0.59, cuda graph: False, gen throughput (token/s): 61.04, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:38] Decode batch. #running-req: 1, #token: 12120, token usage: 0.59, cuda graph: False, gen throughput (token/s): 63.65, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:39] Decode batch. #running-req: 1, #token: 12160, token usage: 0.59, cuda graph: False, gen throughput (token/s): 63.04, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:40] Decode batch. #running-req: 1, #token: 12200, token usage: 0.60, cuda graph: False, gen throughput (token/s): 62.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:40] Decode batch. #running-req: 1, #token: 12240, token usage: 0.60, cuda graph: False, gen throughput (token/s): 63.31, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:41] Decode batch. #running-req: 1, #token: 12280, token usage: 0.60, cuda graph: False, gen throughput (token/s): 63.61, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:41] Decode batch. #running-req: 1, #token: 12320, token usage: 0.60, cuda graph: False, gen throughput (token/s): 64.76, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:42] Decode batch. #running-req: 1, #token: 12360, token usage: 0.60, cuda graph: False, gen throughput (token/s): 64.94, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:43] Decode batch. #running-req: 1, #token: 12400, token usage: 0.61, cuda graph: False, gen throughput (token/s): 64.80, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:43] Decode batch. #running-req: 1, #token: 12440, token usage: 0.61, cuda graph: False, gen throughput (token/s): 64.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:44] Decode batch. #running-req: 1, #token: 12480, token usage: 0.61, cuda graph: False, gen throughput (token/s): 64.73, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:45] Decode batch. #running-req: 1, #token: 12520, token usage: 0.61, cuda graph: False, gen throughput (token/s): 60.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:45] Decode batch. #running-req: 1, #token: 12560, token usage: 0.61, cuda graph: False, gen throughput (token/s): 62.30, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:46] Decode batch. #running-req: 1, #token: 12600, token usage: 0.62, cuda graph: False, gen throughput (token/s): 64.09, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:46] Decode batch. #running-req: 1, #token: 12640, token usage: 0.62, cuda graph: False, gen throughput (token/s): 65.05, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:47] Decode batch. #running-req: 1, #token: 12680, token usage: 0.62, cuda graph: False, gen throughput (token/s): 63.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:48] Decode batch. #running-req: 1, #token: 12720, token usage: 0.62, cuda graph: False, gen throughput (token/s): 65.53, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:48] Decode batch. #running-req: 1, #token: 12760, token usage: 0.62, cuda graph: False, gen throughput (token/s): 61.31, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:49] Decode batch. #running-req: 1, #token: 12800, token usage: 0.62, cuda graph: False, gen throughput (token/s): 64.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:50] Decode batch. #running-req: 1, #token: 12840, token usage: 0.63, cuda graph: False, gen throughput (token/s): 62.25, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:50] Decode batch. #running-req: 1, #token: 12880, token usage: 0.63, cuda graph: False, gen throughput (token/s): 57.49, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:51] Decode batch. #running-req: 1, #token: 12920, token usage: 0.63, cuda graph: False, gen throughput (token/s): 46.29, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:52] Decode batch. #running-req: 1, #token: 12960, token usage: 0.63, cuda graph: False, gen throughput (token/s): 49.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:53] Decode batch. #running-req: 1, #token: 13000, token usage: 0.63, cuda graph: False, gen throughput (token/s): 64.74, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:53] Decode batch. #running-req: 1, #token: 13040, token usage: 0.64, cuda graph: False, gen throughput (token/s): 64.54, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:54] Decode batch. #running-req: 1, #token: 13080, token usage: 0.64, cuda graph: False, gen throughput (token/s): 64.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:54] Decode batch. #running-req: 1, #token: 13120, token usage: 0.64, cuda graph: False, gen throughput (token/s): 64.25, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:55] Decode batch. #running-req: 1, #token: 13160, token usage: 0.64, cuda graph: False, gen throughput (token/s): 64.35, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:56] Decode batch. #running-req: 1, #token: 13200, token usage: 0.64, cuda graph: False, gen throughput (token/s): 63.79, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:56] Decode batch. #running-req: 1, #token: 13240, token usage: 0.65, cuda graph: False, gen throughput (token/s): 64.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:57] Decode batch. #running-req: 1, #token: 13280, token usage: 0.65, cuda graph: False, gen throughput (token/s): 64.88, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:58] Decode batch. #running-req: 1, #token: 13320, token usage: 0.65, cuda graph: False, gen throughput (token/s): 64.88, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:58] Decode batch. #running-req: 1, #token: 13360, token usage: 0.65, cuda graph: False, gen throughput (token/s): 61.67, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:59] Decode batch. #running-req: 1, #token: 13400, token usage: 0.65, cuda graph: False, gen throughput (token/s): 64.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:40:59] Decode batch. #running-req: 1, #token: 13440, token usage: 0.66, cuda graph: False, gen throughput (token/s): 64.62, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:00] Decode batch. #running-req: 1, #token: 13480, token usage: 0.66, cuda graph: False, gen throughput (token/s): 54.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:01] Decode batch. #running-req: 1, #token: 13520, token usage: 0.66, cuda graph: False, gen throughput (token/s): 52.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:02] Decode batch. #running-req: 1, #token: 13560, token usage: 0.66, cuda graph: False, gen throughput (token/s): 33.52, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:03] Decode batch. #running-req: 1, #token: 13600, token usage: 0.66, cuda graph: False, gen throughput (token/s): 34.54, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:04] Decode batch. #running-req: 1, #token: 13640, token usage: 0.67, cuda graph: False, gen throughput (token/s): 51.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:05] Decode batch. #running-req: 1, #token: 13680, token usage: 0.67, cuda graph: False, gen throughput (token/s): 59.70, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:05] Decode batch. #running-req: 1, #token: 13720, token usage: 0.67, cuda graph: False, gen throughput (token/s): 64.00, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:06] Decode batch. #running-req: 1, #token: 13760, token usage: 0.67, cuda graph: False, gen throughput (token/s): 59.10, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:07] Decode batch. #running-req: 1, #token: 13800, token usage: 0.67, cuda graph: False, gen throughput (token/s): 60.86, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:07] Decode batch. #running-req: 1, #token: 13840, token usage: 0.68, cuda graph: False, gen throughput (token/s): 63.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:08] Decode batch. #running-req: 1, #token: 13880, token usage: 0.68, cuda graph: False, gen throughput (token/s): 63.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:09] Decode batch. #running-req: 1, #token: 13920, token usage: 0.68, cuda graph: False, gen throughput (token/s): 63.53, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:09] Decode batch. #running-req: 1, #token: 13960, token usage: 0.68, cuda graph: False, gen throughput (token/s): 61.55, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:10] Decode batch. #running-req: 1, #token: 14000, token usage: 0.68, cuda graph: False, gen throughput (token/s): 59.82, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:11] Decode batch. #running-req: 1, #token: 14040, token usage: 0.69, cuda graph: False, gen throughput (token/s): 60.60, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:11] Decode batch. #running-req: 1, #token: 14080, token usage: 0.69, cuda graph: False, gen throughput (token/s): 62.64, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:12] Decode batch. #running-req: 1, #token: 14120, token usage: 0.69, cuda graph: False, gen throughput (token/s): 62.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:12] Decode batch. #running-req: 1, #token: 14160, token usage: 0.69, cuda graph: False, gen throughput (token/s): 62.67, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:13] Decode batch. #running-req: 1, #token: 14200, token usage: 0.69, cuda graph: False, gen throughput (token/s): 61.57, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:14] Decode batch. #running-req: 1, #token: 14240, token usage: 0.70, cuda graph: False, gen throughput (token/s): 59.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:15] Decode batch. #running-req: 1, #token: 14280, token usage: 0.70, cuda graph: False, gen throughput (token/s): 56.66, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:15] Decode batch. #running-req: 1, #token: 14320, token usage: 0.70, cuda graph: False, gen throughput (token/s): 60.51, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:16] Decode batch. #running-req: 1, #token: 14360, token usage: 0.70, cuda graph: False, gen throughput (token/s): 58.28, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:17] Decode batch. #running-req: 1, #token: 14400, token usage: 0.70, cuda graph: False, gen throughput (token/s): 58.36, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:17] Decode batch. #running-req: 1, #token: 14440, token usage: 0.71, cuda graph: False, gen throughput (token/s): 59.62, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:18] Decode batch. #running-req: 1, #token: 14480, token usage: 0.71, cuda graph: False, gen throughput (token/s): 60.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:19] Decode batch. #running-req: 1, #token: 14520, token usage: 0.71, cuda graph: False, gen throughput (token/s): 62.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:19] Decode batch. #running-req: 1, #token: 14560, token usage: 0.71, cuda graph: False, gen throughput (token/s): 62.71, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:20] Decode batch. #running-req: 1, #token: 14600, token usage: 0.71, cuda graph: False, gen throughput (token/s): 61.88, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:20] Decode batch. #running-req: 1, #token: 14640, token usage: 0.71, cuda graph: False, gen throughput (token/s): 63.01, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:21] Decode batch. #running-req: 1, #token: 14680, token usage: 0.72, cuda graph: False, gen throughput (token/s): 62.97, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:22] Decode batch. #running-req: 1, #token: 14720, token usage: 0.72, cuda graph: False, gen throughput (token/s): 58.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:23] Decode batch. #running-req: 1, #token: 14760, token usage: 0.72, cuda graph: False, gen throughput (token/s): 50.84, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:23] Decode batch. #running-req: 1, #token: 14800, token usage: 0.72, cuda graph: False, gen throughput (token/s): 50.09, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:24] Decode batch. #running-req: 1, #token: 14840, token usage: 0.72, cuda graph: False, gen throughput (token/s): 49.86, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:25] Decode batch. #running-req: 1, #token: 14880, token usage: 0.73, cuda graph: False, gen throughput (token/s): 49.81, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:26] Decode batch. #running-req: 1, #token: 14920, token usage: 0.73, cuda graph: False, gen throughput (token/s): 49.48, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:26] Decode batch. #running-req: 1, #token: 14960, token usage: 0.73, cuda graph: False, gen throughput (token/s): 55.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:27] Decode batch. #running-req: 1, #token: 15000, token usage: 0.73, cuda graph: False, gen throughput (token/s): 60.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:28] Decode batch. #running-req: 1, #token: 15040, token usage: 0.73, cuda graph: False, gen throughput (token/s): 63.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:28] Decode batch. #running-req: 1, #token: 15080, token usage: 0.74, cuda graph: False, gen throughput (token/s): 61.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:29] Decode batch. #running-req: 1, #token: 15120, token usage: 0.74, cuda graph: False, gen throughput (token/s): 63.36, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:30] Decode batch. #running-req: 1, #token: 15160, token usage: 0.74, cuda graph: False, gen throughput (token/s): 62.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:30] Decode batch. #running-req: 1, #token: 15200, token usage: 0.74, cuda graph: False, gen throughput (token/s): 64.72, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:31] Decode batch. #running-req: 1, #token: 15240, token usage: 0.74, cuda graph: False, gen throughput (token/s): 64.62, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:32] Decode batch. #running-req: 1, #token: 15280, token usage: 0.75, cuda graph: False, gen throughput (token/s): 60.20, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:32] Decode batch. #running-req: 1, #token: 15320, token usage: 0.75, cuda graph: False, gen throughput (token/s): 63.49, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:33] Decode batch. #running-req: 1, #token: 15360, token usage: 0.75, cuda graph: False, gen throughput (token/s): 64.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:33] Decode batch. #running-req: 1, #token: 15400, token usage: 0.75, cuda graph: False, gen throughput (token/s): 61.59, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:34] Decode batch. #running-req: 1, #token: 15440, token usage: 0.75, cuda graph: False, gen throughput (token/s): 63.42, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:35] Decode batch. #running-req: 1, #token: 15480, token usage: 0.76, cuda graph: False, gen throughput (token/s): 61.87, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:35] Decode batch. #running-req: 1, #token: 15520, token usage: 0.76, cuda graph: False, gen throughput (token/s): 62.80, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:36] Decode batch. #running-req: 1, #token: 15560, token usage: 0.76, cuda graph: False, gen throughput (token/s): 62.91, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:37] Decode batch. #running-req: 1, #token: 15600, token usage: 0.76, cuda graph: False, gen throughput (token/s): 61.84, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:37] Decode batch. #running-req: 1, #token: 15640, token usage: 0.76, cuda graph: False, gen throughput (token/s): 62.28, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:38] Decode batch. #running-req: 1, #token: 15680, token usage: 0.77, cuda graph: False, gen throughput (token/s): 64.19, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:39] Decode batch. #running-req: 1, #token: 15720, token usage: 0.77, cuda graph: False, gen throughput (token/s): 64.36, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:39] Decode batch. #running-req: 1, #token: 15760, token usage: 0.77, cuda graph: False, gen throughput (token/s): 64.34, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:40] Decode batch. #running-req: 1, #token: 15800, token usage: 0.77, cuda graph: False, gen throughput (token/s): 64.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:40] Decode batch. #running-req: 1, #token: 15840, token usage: 0.77, cuda graph: False, gen throughput (token/s): 64.45, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:41] Decode batch. #running-req: 1, #token: 15880, token usage: 0.78, cuda graph: False, gen throughput (token/s): 64.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:42] Decode batch. #running-req: 1, #token: 15920, token usage: 0.78, cuda graph: False, gen throughput (token/s): 64.52, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:42] Decode batch. #running-req: 1, #token: 15960, token usage: 0.78, cuda graph: False, gen throughput (token/s): 64.59, #queue-req: 0\n",
      "[2025-05-15 22:41:42] INFO:     127.0.0.1:48690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Alright, so the user is in New York and wants to know the current date and time along with the weather. They've previously been told about two functions: get_current_weather and get_current_date. I need to figure out how to call these functions correctly.<br><br>First, I should determine the necessary parameters for each function. For get_current_date, the parameter is timezone, and the default is 'America/New_York'. Since the user is in New York, I can just use the default value without specifying it.<br><br>Next, for get_current_weather, the required parameters are city, state, and unit. The city is 'New York', the state is 'NY', and the unit should be 'fahrenheit' since the user might prefer that, but it's optional. However, to be helpful, specifying the unit could make the response more useful.<br><br>I need to structure each function call correctly. Each should be on its own line with the correct syntax. The function calls should be enclosed within the <function> tags, with the parameters as a JSON object.<br><br>So, I'll first call get_current_date with timezone as 'America/New_York'. Then, I'll call get_current_weather with the specified city, state, and unit. I'll make sure to format the JSON correctly, using double quotes and proper syntax to avoid errors.<br><br>I should also remember to include the sources where I got the information about the functions. Since I used the provided function definitions, I'll reference those.<br><br>Putting it all together, I'll write two separate function calls, each on its own line, ensuring clarity and correctness.<br><br><br>content: <function=get_current_date>{\"timezone\": \"America/New_York>}</function> <function=get_current_weather>{\"  <br>  \t<br>    <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br><br>    <br>  <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br> <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br><br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br><br>    <br>    <br><br><br><br><br>    <br>   <br>    <br>   <br>    <br>    <br>   <br>    <br>   <br>    <br> <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br> <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br> <br>    <br>   <br>    <br>   <br>    <br> <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>  <br><br><br><br>    <br>   <br>    <br>   <br>    <br>            <br>   <br>    <br>  <br>    <br>   <br>    <br>                        <br>   <br>    <br>    <br>   <br>    <br> <br>    <br>   <br>    <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>   <br>    <br>       <br>    <br> <br>    <br>   <br>    <br>   <br>    <br>    <br><br><br><br>    <br>   <br><br><br><br><br>    <br>   <br>    <br> }</function><br></think><br><br></think><br><br></think><br><br></think><br><br></think><br><br></think><br><br></think><br><br></think><br><br>好的，我将根据你的指示，我需要分析你提供的函数的实现步骤。首先，我会检查函数的结构和功能，确保函数符合要求。然后，我会逐步分析为什么函数不能通过，以及为什么它通过。以下是详细分析：<br><br>1. **函数分析**<br><br>函数的目的是根据输入的\\( x \\)、\\( y \\)和\\( z \\)计算和验证以下等式：<br><br>\\[<br>\\frac{\\sqrt{x^2 + y + z}}{\\sqrt{x^2 + y + z}} = 1<br>\\]<br><br>这是一个恒等式，无论\\( x \\)、\\( y \\)和\\( z \\)如何变化，等式都成立。这意味着这个函数的输出总是1，无论输入是什么。<br><br>2. **函数分析**<br><br>由于函数总是返回1，函数的实现很简单，因为无论输入是什么，结果都相同。不过，我需要确保函数在实现过程中没有潜在的逻辑错误，比如没有错误，所以函数可以是：<br><br>```python<br>def    <br>    def f(x, y, z):<br>        return 1<br>    <br>    return 1<br>```<br><br>3. **改进后的函数**<br><br>为了提高函数的可读性，我建议做一些微调：<br><br>```python<br>def<br>    <br>    def f(x, y, z):<br>        return 1<br>    <br>    return 1<br>```<br><br>4. **测试函数**<br><br>为了验证函数是否正确，我可以写一些测试用例：<br><br>- 当\\( x = 1 \\)，\\( y = 2 \\)，\\( z = 3 \\)时：<br>  \\[<br>  \\frac{\\sqrt{1 + 2 + 3}}{\\sqrt{1 + 2 + 3}} = 1<br>  \\]<br>  函数返回1，正确。<br><br>- 当\\( x = 100 \\)，\\( y = 200 \\)，\\( z = 400 \\)时：<br>  \\[<br>  \\frac{\\sqrt{100^2 + 200 + 400}}{\\sqrt{100^2 + 200 + 400}} = 1<br>  \\]<br>  函数返回1，正确。<br><br>5. **潜在的问题**<br><br>- 如果函数没有输入验证，可能会遇到性能问题，但在这个案例中，函数没有处理输入，因为它总是返回1。如果没有其他要求，这可能是一个空虚的函数。如果需要确保函数仅返回1，可以添加一个参数有效性检查，例如：<br><br>```python<br>def<br>    <br>    def f(x, y, z):<br>        if x is None or y is None or z is None:<br>            return 1<br>        return 1<br>    <br>    return 1<br>```<br><br>总结：<br><br>这个函数的实现很简单，因为等式恒成立。确保函数没有逻辑错误，并且所有输入都是可选的。</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_get_current_weather = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\",\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tool_get_current_date = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"Get the current date and time for a given timezone\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"timezone\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The timezone to fetch the current date and time for, e.g. 'America/New_York'\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"timezone\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "schema_get_current_weather = tool_get_current_weather[\"function\"][\"parameters\"]\n",
    "schema_get_current_date = tool_get_current_date[\"function\"][\"parameters\"]\n",
    "\n",
    "\n",
    "def get_messages():\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "# Tool Instructions\n",
    "- Always execute python code in messages that you share.\n",
    "- When looking for real time information use relevant functions if available else fallback to brave_search\n",
    "You have access to the following functions:\n",
    "Use the function 'get_current_weather' to: Get the current weather in a given location\n",
    "{tool_get_current_weather[\"function\"]}\n",
    "Use the function 'get_current_date' to: Get the current date and time for a given timezone\n",
    "{tool_get_current_date[\"function\"]}\n",
    "If a you choose to call a function ONLY reply in the following format:\n",
    "<{{start_tag}}={{function_name}}>{{parameters}}{{end_tag}}\n",
    "where\n",
    "start_tag => `<function`\n",
    "parameters => a JSON dict with the function argument name as key and function argument value as value.\n",
    "end_tag => `</function>`\n",
    "Here is an example,\n",
    "<function=example_function_name>{{\"example_name\": \"example_value\"}}</function>\n",
    "Reminder:\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "- Always add your sources when using search results to answer the user query\n",
    "You are a helpful assistant.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"You are in New York. Please get the current date and time, and the weather.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "messages = get_messages()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=messages,\n",
    "    response_format={\n",
    "        \"type\": \"structural_tag\",\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structures\": [\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_weather>\",\n",
    "                \"schema\": schema_get_current_weather,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_date>\",\n",
    "                \"schema\": schema_get_current_date,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "        ],\n",
    "        \"triggers\": [\"<function=\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native API and SGLang Runtime (SRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:41:42.858916Z",
     "iopub.status.busy": "2025-05-15T22:41:42.858621Z",
     "iopub.status.idle": "2025-05-15T22:41:49.178914Z",
     "shell.execute_reply": "2025-05-15T22:41:49.178208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:43] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:43] Decode batch. #running-req: 1, #token: 59, token usage: 0.00, cuda graph: False, gen throughput (token/s): 33.00, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:44] Decode batch. #running-req: 1, #token: 99, token usage: 0.00, cuda graph: False, gen throughput (token/s): 62.77, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:45] Decode batch. #running-req: 1, #token: 139, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.61, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:45] Decode batch. #running-req: 1, #token: 179, token usage: 0.01, cuda graph: False, gen throughput (token/s): 63.54, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:46] Decode batch. #running-req: 1, #token: 219, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.71, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:47] Decode batch. #running-req: 1, #token: 259, token usage: 0.01, cuda graph: False, gen throughput (token/s): 66.37, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:47] Decode batch. #running-req: 1, #token: 299, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.12, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:48] Decode batch. #running-req: 1, #token: 339, token usage: 0.02, cuda graph: False, gen throughput (token/s): 67.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:48] Decode batch. #running-req: 1, #token: 379, token usage: 0.02, cuda graph: False, gen throughput (token/s): 67.30, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:49] INFO:     127.0.0.1:33958 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "{'text': 'Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down.\\n\\nFirst, I need to identify the capital of France. I know that Paris is the capital, so that\\'s straightforward. Now, I should find the most recent population data. I remember that the population of Paris has been growing, but I\\'m not sure of the exact number. I think it\\'s around 2 million, but I should verify that.\\n\\nWait, I should check the latest statistics to be accurate. Maybe I can recall that as of 2023, the population was approximately 2,150,000. That seems about right. I should make sure to include this number in the JSON.\\n\\nNext, I need to structure this information into a JSON format. JSON typically uses key-value pairs, so I\\'ll create an object with keys like \"city\", \"population\", and \"country\". The city is Paris, the population is 2,150,000, and the country is France.\\n\\nI should also consider the format. The user wants it in JSON, so I\\'ll make sure to use proper syntax with quotes and commas. I\\'ll avoid any markdown since they specified that, so just plain JSON.\\n\\nPutting it all together, the JSON object will have the city, population, and country. I\\'ll double-check the numbers to ensure accuracy. I think 2,150,000 is correct, but if I\\'m unsure, I might mention that the data is approximate.\\n\\nFinally, I\\'ll present the JSON without any additional text, just the code, as per the user\\'s request. That should fulfill their query effectively.\\n</think>{\\n  \"name\": \"Paris\",\\n  \"population\": 2150000\\n}', 'meta_info': {'id': 'f76169ee5b8642bdaa42b845f62243ad', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'completion_tokens': 374, 'cached_tokens': 1, 'e2e_latency': 5.739806413650513}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down.<br><br>First, I need to identify the capital of France. I know that Paris is the capital, so that's straightforward. Now, I should find the most recent population data. I remember that the population of Paris has been growing, but I'm not sure of the exact number. I think it's around 2 million, but I should verify that.<br><br>Wait, I should check the latest statistics to be accurate. Maybe I can recall that as of 2023, the population was approximately 2,150,000. That seems about right. I should make sure to include this number in the JSON.<br><br>Next, I need to structure this information into a JSON format. JSON typically uses key-value pairs, so I'll create an object with keys like \"city\", \"population\", and \"country\". The city is Paris, the population is 2,150,000, and the country is France.<br><br>I should also consider the format. The user wants it in JSON, so I'll make sure to use proper syntax with quotes and commas. I'll avoid any markdown since they specified that, so just plain JSON.<br><br>Putting it all together, the JSON object will have the city, population, and country. I'll double-check the numbers to ensure accuracy. I think 2,150,000 is correct, but if I'm unsure, I might mention that the data is approximate.<br><br>Finally, I'll present the JSON without any additional text, just the code, as per the user's request. That should fulfill their query effectively.<br><br><br>content: {<br>  \"name\": \"Paris\",<br>  \"population\": 2150000<br>}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "    },\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# Make API request\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())\n",
    "\n",
    "\n",
    "reasoing_content = response.json()[\"text\"].split(\"</think>\")[0]\n",
    "content = response.json()[\"text\"].split(\"</think>\")[1]\n",
    "print_highlight(f\"reasoing_content: {reasoing_content}\\n\\ncontent: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:41:49.180901Z",
     "iopub.status.busy": "2025-05-15T22:41:49.180667Z",
     "iopub.status.idle": "2025-05-15T22:41:56.601437Z",
     "shell.execute_reply": "2025-05-15T22:41:56.600793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:49] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:49] Decode batch. #running-req: 1, #token: 45, token usage: 0.00, cuda graph: False, gen throughput (token/s): 64.18, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:50] Decode batch. #running-req: 1, #token: 85, token usage: 0.00, cuda graph: False, gen throughput (token/s): 67.30, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:50] Decode batch. #running-req: 1, #token: 125, token usage: 0.01, cuda graph: False, gen throughput (token/s): 66.57, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:51] Decode batch. #running-req: 1, #token: 165, token usage: 0.01, cuda graph: False, gen throughput (token/s): 66.90, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:51] Decode batch. #running-req: 1, #token: 205, token usage: 0.01, cuda graph: False, gen throughput (token/s): 66.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:52] Decode batch. #running-req: 1, #token: 245, token usage: 0.01, cuda graph: False, gen throughput (token/s): 67.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:53] Decode batch. #running-req: 1, #token: 285, token usage: 0.01, cuda graph: False, gen throughput (token/s): 66.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:53] Decode batch. #running-req: 1, #token: 325, token usage: 0.02, cuda graph: False, gen throughput (token/s): 67.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:54] Decode batch. #running-req: 1, #token: 365, token usage: 0.02, cuda graph: False, gen throughput (token/s): 67.61, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:54] Decode batch. #running-req: 1, #token: 405, token usage: 0.02, cuda graph: False, gen throughput (token/s): 67.41, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:55] Decode batch. #running-req: 1, #token: 445, token usage: 0.02, cuda graph: False, gen throughput (token/s): 67.36, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:56] Decode batch. #running-req: 1, #token: 485, token usage: 0.02, cuda graph: False, gen throughput (token/s): 67.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:56] INFO:     127.0.0.1:33970 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down. First, I need to identify what the capital of France is. I know that Paris is the capital, so that\\'s the starting point.\\n\\nNext, I need to find the population of Paris. I remember that Paris is a major city with a large population, but I\\'m not exactly sure of the current number. I think it\\'s around 2 million, but I should double-check that. Maybe I can recall that it\\'s approximately 2,150,000 as of recent estimates.\\n\\nNow, the user wants this information in JSON format. JSON stands for JavaScript Object Notation, which is a way to structure data. I need to create a JSON object that includes the key \"capital\" with the value \"Paris\" and another key \"population\" with the number I just thought of.\\n\\nI should make sure the JSON syntax is correct. That means using double quotes for keys and string values, and commas appropriately between key-value pairs. Also, the numbers should be in quotes if they\\'re strings, but population is a number, so it should be without quotes.\\n\\nPutting it all together, the JSON object should look like this: {\"capital\": \"Paris\", \"population\": 2150000}. I should present this clearly so the user can easily understand and use the information.\\n\\nI wonder if the user needs more details, like the population figure\\'s source or the exact year it was recorded. But since they didn\\'t ask for that, I\\'ll stick to the information requested. Maybe they just need a straightforward data structure for a program or a report.\\n\\nAlso, considering the user\\'s request, they might be a student working on a project, or perhaps someone developing an app that requires capital cities and their populations. Either way, providing accurate and concise data is key.\\n\\nI should ensure that the JSON is correctly formatted to avoid any errors when the user tries to use it. No markdown, just plain JSON. That way, it\\'s easy to copy and paste into their code or document.\\n\\nIn summary, I identified the capital, found the population number, structured it into a JSON object, and made sure the syntax is correct. I think this should meet the user\\'s needs effectively.\\n</think>{\\n  \"name\": \"Paris\",\\n  \"population\": 2150000\\n}', 'meta_info': {'id': 'b961373736b741f2940b8f4308a3a00b', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'completion_tokens': 497, 'cached_tokens': 22, 'e2e_latency': 7.411503314971924}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# JSON\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json_schema,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:41:56.603407Z",
     "iopub.status.busy": "2025-05-15T22:41:56.603194Z",
     "iopub.status.idle": "2025-05-15T22:42:00.099110Z",
     "shell.execute_reply": "2025-05-15T22:42:00.098390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:56] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-15 22:41:56] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:56] Decode batch. #running-req: 3, #token: 23, token usage: 0.00, cuda graph: False, gen throughput (token/s): 52.02, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:57] Decode batch. #running-req: 3, #token: 143, token usage: 0.01, cuda graph: False, gen throughput (token/s): 190.61, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:58] Decode batch. #running-req: 3, #token: 263, token usage: 0.01, cuda graph: False, gen throughput (token/s): 181.07, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:58] Decode batch. #running-req: 3, #token: 383, token usage: 0.02, cuda graph: False, gen throughput (token/s): 190.45, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:41:59] Decode batch. #running-req: 3, #token: 503, token usage: 0.02, cuda graph: False, gen throughput (token/s): 190.60, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:00] INFO:     127.0.0.1:40908 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[{'text': \"\\nThe capital of France is Paris.\\n\\nThat's all the information I have.\\n\\nOkay, so I need to figure out the capital of France. I know that Paris is the capital, but I'm not entirely sure. Let me think about why I think that. I've heard it mentioned a lot, especially in movies and TV shows. People often go there for business or tourism. Also, I remember learning in school that Paris is a major city in France, known for landmarks like the Eiffel Tower and the Louvre Museum. Those places are famous worldwide, which makes me think that Paris is indeed the capital. Maybe I can cross-check this with some other sources or my notes. Wait, I don't have any other information right now, but based on what I know, Paris is the capital of France. I don't recall any other major city in France being referred to as the capital. So, I'm pretty confident that Paris is correct.\\n</think>Paris is the capital of France\", 'meta_info': {'id': '6ebb8cbc74e24330905adebb0a2b5c78', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'completion_tokens': 201, 'cached_tokens': 10, 'e2e_latency': 3.487443447113037}}, {'text': \"\\nThe capital of France is Paris.\\n\\nThat's all the information I have.\\n\\nOkay, so I need to figure out the capital of France. I know that Paris is the capital, but I'm not entirely sure. Let me think about why I think that. I've heard it mentioned a lot, especially in movies and TV shows. People often go there for business or tourism. Also, I remember learning in school that Paris is a major city in France, known for landmarks like the Eiffel Tower and the Louvre Museum. Those places are famous worldwide, which makes me think that Paris is indeed the capital. Maybe I can cross-check this with some other sources or my notes. Wait, I don't have any other information right now, but based on what I know, Paris is the capital of France. I don't recall any other major city in France being referred to as the capital. So, I'm pretty confident that Paris is correct.\\n</think>Paris is the capital of France\", 'meta_info': {'id': '530d3859db864058aeeb6d9040b05b98', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'completion_tokens': 201, 'cached_tokens': 10, 'e2e_latency': 3.487457036972046}}, {'text': \"\\nThe capital of France is Paris.\\n\\nThat's all the information I have.\\n\\nOkay, so I need to figure out the capital of France. I know that Paris is the capital, but I'm not entirely sure. Let me think about why I think that. I've heard it mentioned a lot, especially in movies and TV shows. People often go there for business or tourism. Also, I remember learning in school that Paris is a major city in France, known for landmarks like the Eiffel Tower and the Louvre Museum. Those places are famous worldwide, which makes me think that Paris is indeed the capital. Maybe I can cross-check this with some other sources or my notes. Wait, I don't have any other information right now, but based on what I know, Paris is the capital of France. I don't recall any other major city in France being referred to as the capital. So, I'm pretty confident that Paris is correct.\\n</think>Paris is the capital of France\", 'meta_info': {'id': 'c1ee4902a6f241c9b1e8742bb352723d', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'completion_tokens': 201, 'cached_tokens': 10, 'e2e_latency': 3.48746395111084}}]\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Give me the information of the capital of France.\",\n",
    "        \"sampling_params\": {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"temperature\": 0,\n",
    "            \"n\": 3,\n",
    "            \"ebnf\": (\n",
    "                \"root ::= city | description\\n\"\n",
    "                'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "                'description ::= city \" is \" status\\n'\n",
    "                'status ::= \"the capital of \" country\\n'\n",
    "                'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "            ),\n",
    "        },\n",
    "        \"stream\": False,\n",
    "        \"return_logprob\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:42:00.101097Z",
     "iopub.status.busy": "2025-05-15T22:42:00.100872Z",
     "iopub.status.idle": "2025-05-15T22:42:32.329886Z",
     "shell.execute_reply": "2025-05-15T22:42:32.329213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:00] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-05-15 22:42:00] Decode batch. #running-req: 1, #token: 9, token usage: 0.00, cuda graph: False, gen throughput (token/s): 172.56, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:00] Decode batch. #running-req: 1, #token: 49, token usage: 0.00, cuda graph: False, gen throughput (token/s): 66.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:01] Decode batch. #running-req: 1, #token: 89, token usage: 0.00, cuda graph: False, gen throughput (token/s): 67.02, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:02] Decode batch. #running-req: 1, #token: 129, token usage: 0.01, cuda graph: False, gen throughput (token/s): 60.40, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:02] Decode batch. #running-req: 1, #token: 169, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.07, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:03] Decode batch. #running-req: 1, #token: 209, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.17, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:03] Decode batch. #running-req: 1, #token: 249, token usage: 0.01, cuda graph: False, gen throughput (token/s): 65.48, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:04] Decode batch. #running-req: 1, #token: 289, token usage: 0.01, cuda graph: False, gen throughput (token/s): 61.21, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:05] Decode batch. #running-req: 1, #token: 329, token usage: 0.02, cuda graph: False, gen throughput (token/s): 64.23, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:05] Decode batch. #running-req: 1, #token: 369, token usage: 0.02, cuda graph: False, gen throughput (token/s): 63.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:06] Decode batch. #running-req: 1, #token: 409, token usage: 0.02, cuda graph: False, gen throughput (token/s): 63.68, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:07] Decode batch. #running-req: 1, #token: 449, token usage: 0.02, cuda graph: False, gen throughput (token/s): 65.00, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:07] Decode batch. #running-req: 1, #token: 489, token usage: 0.02, cuda graph: False, gen throughput (token/s): 64.20, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:08] Decode batch. #running-req: 1, #token: 529, token usage: 0.03, cuda graph: False, gen throughput (token/s): 64.06, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:08] Decode batch. #running-req: 1, #token: 569, token usage: 0.03, cuda graph: False, gen throughput (token/s): 63.65, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:09] Decode batch. #running-req: 1, #token: 609, token usage: 0.03, cuda graph: False, gen throughput (token/s): 63.35, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:10] Decode batch. #running-req: 1, #token: 649, token usage: 0.03, cuda graph: False, gen throughput (token/s): 64.75, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:10] Decode batch. #running-req: 1, #token: 689, token usage: 0.03, cuda graph: False, gen throughput (token/s): 64.42, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:11] Decode batch. #running-req: 1, #token: 729, token usage: 0.04, cuda graph: False, gen throughput (token/s): 64.25, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:12] Decode batch. #running-req: 1, #token: 769, token usage: 0.04, cuda graph: False, gen throughput (token/s): 64.96, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:12] Decode batch. #running-req: 1, #token: 809, token usage: 0.04, cuda graph: False, gen throughput (token/s): 64.91, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:13] Decode batch. #running-req: 1, #token: 849, token usage: 0.04, cuda graph: False, gen throughput (token/s): 65.08, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:13] Decode batch. #running-req: 1, #token: 889, token usage: 0.04, cuda graph: False, gen throughput (token/s): 64.99, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:14] Decode batch. #running-req: 1, #token: 929, token usage: 0.05, cuda graph: False, gen throughput (token/s): 64.98, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:15] Decode batch. #running-req: 1, #token: 969, token usage: 0.05, cuda graph: False, gen throughput (token/s): 64.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:15] Decode batch. #running-req: 1, #token: 1009, token usage: 0.05, cuda graph: False, gen throughput (token/s): 65.00, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:16] Decode batch. #running-req: 1, #token: 1049, token usage: 0.05, cuda graph: False, gen throughput (token/s): 63.44, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:16] Decode batch. #running-req: 1, #token: 1089, token usage: 0.05, cuda graph: False, gen throughput (token/s): 64.84, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:17] Decode batch. #running-req: 1, #token: 1129, token usage: 0.06, cuda graph: False, gen throughput (token/s): 64.33, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:18] Decode batch. #running-req: 1, #token: 1169, token usage: 0.06, cuda graph: False, gen throughput (token/s): 59.48, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:18] Decode batch. #running-req: 1, #token: 1209, token usage: 0.06, cuda graph: False, gen throughput (token/s): 63.88, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:19] Decode batch. #running-req: 1, #token: 1249, token usage: 0.06, cuda graph: False, gen throughput (token/s): 60.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:20] Decode batch. #running-req: 1, #token: 1289, token usage: 0.06, cuda graph: False, gen throughput (token/s): 63.94, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:20] Decode batch. #running-req: 1, #token: 1329, token usage: 0.06, cuda graph: False, gen throughput (token/s): 64.15, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:21] Decode batch. #running-req: 1, #token: 1369, token usage: 0.07, cuda graph: False, gen throughput (token/s): 57.81, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:22] Decode batch. #running-req: 1, #token: 1409, token usage: 0.07, cuda graph: False, gen throughput (token/s): 62.32, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:22] Decode batch. #running-req: 1, #token: 1449, token usage: 0.07, cuda graph: False, gen throughput (token/s): 60.08, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:23] Decode batch. #running-req: 1, #token: 1489, token usage: 0.07, cuda graph: False, gen throughput (token/s): 62.03, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:24] Decode batch. #running-req: 1, #token: 1529, token usage: 0.07, cuda graph: False, gen throughput (token/s): 61.26, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:24] Decode batch. #running-req: 1, #token: 1569, token usage: 0.08, cuda graph: False, gen throughput (token/s): 60.78, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:25] Decode batch. #running-req: 1, #token: 1609, token usage: 0.08, cuda graph: False, gen throughput (token/s): 62.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:25] Decode batch. #running-req: 1, #token: 1649, token usage: 0.08, cuda graph: False, gen throughput (token/s): 63.57, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:26] Decode batch. #running-req: 1, #token: 1689, token usage: 0.08, cuda graph: False, gen throughput (token/s): 64.42, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:27] Decode batch. #running-req: 1, #token: 1729, token usage: 0.08, cuda graph: False, gen throughput (token/s): 64.24, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:27] Decode batch. #running-req: 1, #token: 1769, token usage: 0.09, cuda graph: False, gen throughput (token/s): 64.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:28] Decode batch. #running-req: 1, #token: 1809, token usage: 0.09, cuda graph: False, gen throughput (token/s): 64.32, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:29] Decode batch. #running-req: 1, #token: 1849, token usage: 0.09, cuda graph: False, gen throughput (token/s): 60.52, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:29] Decode batch. #running-req: 1, #token: 1889, token usage: 0.09, cuda graph: False, gen throughput (token/s): 64.39, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:30] Decode batch. #running-req: 1, #token: 1929, token usage: 0.09, cuda graph: False, gen throughput (token/s): 64.46, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:31] Decode batch. #running-req: 1, #token: 1969, token usage: 0.10, cuda graph: False, gen throughput (token/s): 64.49, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:31] Decode batch. #running-req: 1, #token: 2009, token usage: 0.10, cuda graph: False, gen throughput (token/s): 64.41, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:32] Decode batch. #running-req: 1, #token: 2049, token usage: 0.10, cuda graph: False, gen throughput (token/s): 64.47, #queue-req: 0\n",
      "[2025-05-15 22:42:32] INFO:     127.0.0.1:40910 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "{'text': ' France, and the \\n\\\\( n \\\\)  \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\( l \\\\) \\\\( m \\\\) \\\\( k \\\\) \\\\(', 'meta_info': {'id': '1b836e438fb849f3b420592c2f153644', 'finish_reason': {'type': 'length', 'length': 2048}, 'prompt_tokens': 6, 'completion_tokens': 2048, 'cached_tokens': 1, 'e2e_latency': 32.2212610244751}}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Paris is the capital of\",\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"regex\": \"(France|England)\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:42:32.331978Z",
     "iopub.status.busy": "2025-05-15T22:42:32.331650Z",
     "iopub.status.idle": "2025-05-15T22:42:37.498157Z",
     "shell.execute_reply": "2025-05-15T22:42:37.497417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:32] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:32] Decode batch. #running-req: 1, #token: 58, token usage: 0.00, cuda graph: False, gen throughput (token/s): 62.17, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:33] Decode batch. #running-req: 1, #token: 98, token usage: 0.00, cuda graph: False, gen throughput (token/s): 65.16, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:34] Decode batch. #running-req: 1, #token: 138, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.10, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:34] Decode batch. #running-req: 1, #token: 178, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.95, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:35] Decode batch. #running-req: 1, #token: 218, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.93, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:35] Decode batch. #running-req: 1, #token: 258, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.76, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:36] Decode batch. #running-req: 1, #token: 298, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.82, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:37] Decode batch. #running-req: 1, #token: 338, token usage: 0.02, cuda graph: False, gen throughput (token/s): 64.68, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:37] INFO:     127.0.0.1:53280 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Alright, the user asked for the information and population of the capital of France in JSON format. I know the capital is Paris, so that\\'s the first thing to note.\\n\\nI should figure out the population. I remember it\\'s around 9 million, but I\\'m not sure of the exact number. Maybe I should double-check or state that it\\'s an approximation.\\n\\nLooking at Paris\\'s characteristics, it\\'s a global city known for its culture, cuisine, and landmarks. Including that will give a bit more context.\\n\\nNow, structuring this in JSON. Correct syntax is important, so I\\'ll use proper braces, brackets, and commas.\\n\\nI\\'ll make sure to separate the key-value pairs with colons and spaces for clarity. Also, the population should be a number, either exact or an approximation.\\n\\nFinally, I\\'ll present the JSON neatly, maybe with line breaks for readability.\\n</think>\\n\\nSure! Here\\'s the information about the capital of France, Paris, in JSON format:\\n\\n```json\\n{\\n  \"capital\": \"Paris\",\\n  \"population\": 9_000_000,\\n  \"characteristics\": {\\n    \"Culture\": \"One of the world\\'s most vibrant cultural cities, known for its любом Congo cuisine, fashion, and international […\",\\n    \"enda\": \" landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral\",\\n    \"Economy\": \"A global economic power with diverse industries, fashion, and finance\",\\n    \"Education\": \"home to several prestigious universities and research institutions\"\\n  }\\n}\\n```\\n\\nLet me know if you\\'d like any further details!', 'meta_info': {'id': '03060dfe2ffa467bbbf43a46ba326b1b', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'completion_tokens': 333, 'cached_tokens': 22, 'e2e_latency': 5.155693292617798}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "payload = {\n",
    "    \"text\": text,\n",
    "    \"sampling_params\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structural_tag\": json.dumps(\n",
    "            {\n",
    "                \"type\": \"structural_tag\",\n",
    "                \"structures\": [\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_weather>\",\n",
    "                        \"schema\": schema_get_current_weather,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_date>\",\n",
    "                        \"schema\": schema_get_current_date,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                ],\n",
    "                \"triggers\": [\"<function=\"],\n",
    "            }\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "response = requests.post(f\"http://localhost:{port}/generate\", json=payload)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:42:37.500272Z",
     "iopub.status.busy": "2025-05-15T22:42:37.499881Z",
     "iopub.status.idle": "2025-05-15T22:42:37.544929Z",
     "shell.execute_reply": "2025-05-15T22:42:37.543848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 22:42:37] Child process unexpectedly failed with an exit code 9. pid=71469\n",
      "[2025-05-15 22:42:37] Child process unexpectedly failed with an exit code 9. pid=71402\n"
     ]
    }
   ],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:42:37.547536Z",
     "iopub.status.busy": "2025-05-15T22:42:37.547258Z",
     "iopub.status.idle": "2025-05-15T22:42:57.183050Z",
     "shell.execute_reply": "2025-05-15T22:42:57.182388Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.75s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.32s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.24s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sglang as sgl\n",
    "\n",
    "llm = sgl.Engine(\n",
    "    model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    reasoning_parser=\"deepseek-r1\",\n",
    "    grammar_backend=\"xgrammar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:42:57.185821Z",
     "iopub.status.busy": "2025-05-15T22:42:57.185203Z",
     "iopub.status.idle": "2025-05-15T22:43:03.515344Z",
     "shell.execute_reply": "2025-05-15T22:43:03.514796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of China, Beijing, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Beijing\",\n",
      "  \"capital\": \"Yes\",\n",
      "  \"population\": \"Over 30 million\",\n",
      "  \"founded\": \"1248\",\n",
      "  \"Nickname\": \"The Heaven on Earth\",\n",
      "  \"Location\": \"Northern China\",\n",
      "  \"OfficialLanguages\": [\n",
      "    \"Mandarin Chinese\",\n",
      "    \"Bingyuan Chinese\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\",\n",
      "    \"Yugoslav\",\n",
      "    \"Other\"\n",
      "  ],\n",
      "  \"KeySights\": [\n",
      "    \"The Great Wall\",\n",
      "    \"Tiananmen Square\",\n",
      "    \"Forbidden City\",\n",
      "    \"Beijing Museum\",\n",
      "    \"Yuanmingyuan\"\n",
      "  ],\n",
      "  \"Climate\": \"Temperate\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of France, Paris, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"country\": \"France\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 48.8566,\n",
      "    \"longitude\": 2.3522\n",
      "  },\n",
      "  \"founded\": \"1340\",\n",
      "  \"population\": \"9.7 million\",\n",
      "  \"area\": \"105.5 square kilometers\",\n",
      "  \"features\": {\n",
      "    \"bridges\": \"The Eiffel Tower, Notre-Dame, and the Seine River\",\n",
      "    \"landmarks\": \"The Louvre Museum, Montmartre, and the Champs-Élysées\"\n",
      "  },\n",
      "  \"elevation\": \"2 meters\",\n",
      "  \"time_zone\": \"Central European Time (CET)\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of Ireland in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": \"Dublin\",\n",
      "  \"official_name\": \"Dublin City\",\n",
      "  \"region\": \"Dublin\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 53.3489,\n",
      "    \"longitude\": -6.2009\n",
      "  },\n",
      "  \"founded\": \"1543\",\n",
      "  \"population\": 1,234,567,\n",
      "  \"area\": {\n",
      "    \"total\": 123.45,\n",
      "    \"land\": 112.34,\n",
      "    \"water\": 11.11\n",
      "  },\n",
      "  \"climate\": \" temperate\",\n",
      "  \"key_features\": [\n",
      "    \"City Walls\",\n",
      "    \"Trinity College\",\n",
      "    \"Leaving Certificate\",\n",
      "    \"St. Stephen's Cathedral\",\n",
      "    \"Glynn Bridge\"\n",
      "  ],\n",
      "  \"tourism\": [\n",
      "    \"The GAA\",\n",
      "    \"The National Library of Ireland\",\n",
      "    \"The SSE St. Patrick's Cathedral\",\n",
      "    \"The Phoenix Park\",\n",
      "    \"The Book of Kells\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any adjustments!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:43:03.517177Z",
     "iopub.status.busy": "2025-05-15T22:43:03.516957Z",
     "iopub.status.idle": "2025-05-15T22:43:07.971699Z",
     "shell.execute_reply": "2025-05-15T22:43:07.971151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of China, Beijing, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Beijing\",\n",
      "  \"capital\": \"Yes\",\n",
      "  \"population\": \"Over 30 million\",\n",
      "  \"founded\": \"1248\",\n",
      "  \"Nickname\": \"The Heaven on Earth\",\n",
      "  \"Location\": \"Northern China\",\n",
      "  \"OfficialLanguages\": [\n",
      "    \"Mandarin Chinese\",\n",
      "    \"Bingyuan Chinese\",\n",
      "    \"Tibetan\",\n",
      "    \"Hui\",\n",
      "    \"Mongolian\",\n",
      "    \"Yugoslav\",\n",
      "    \"Other\"\n",
      "  ],\n",
      "  \"KeySights\": [\n",
      "    \"The Great Wall\",\n",
      "    \"Forbidden City\",\n",
      "    \"Tiananmen Square\",\n",
      "    \"Beijing Museum\",\n",
      "    \"Yuanmingyuan\"\n",
      "  ],\n",
      "  \"Climate\": \"Temperate\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: \n",
      "Sure! Here's the information about the capital of France, Paris, in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"country\": \"France\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 48.8566,\n",
      "    \"longitude\": 2.3522\n",
      "  },\n",
      "  \"founded\": \"1340\",\n",
      "  \"population\": \"9.7 million\",\n",
      "  \"area\": \"105.5 square kilometers\",\n",
      "  \"WX\": {\n",
      "    \"averageTemperature\": \"12°C\",\n",
      "    \"precipitation\": \"540 mm/year\"\n",
      "  },\n",
      "  \"landmarks\": [\n",
      "    {\n",
      "      \"name\": \"Eiffel Tower\",\n",
      "      \"location\": \"City of Light\",\n",
      "      \"height\": \"330 meters\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Notre-Dame Cathedral\",\n",
      "      \"location\": \"Center of Paris\",\n",
      "      \"height\": \"415 meters\"\n",
      "    }\n",
      "  ],\n",
      "  \"Transport\": {\n",
      "    \"publicTransport\": \"Boulevards, trams, and subways\",\n",
      "    \"airport\": \"Paris International Airport\",\n",
      "    \"railway\": \"Le巴黎-Charles de Gaulle\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any other information!\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: \n",
      "Sure, here's the information about the capital of Ireland in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": \"Dublin\",\n",
      "  \"official_name\": \"Dublin City\",\n",
      "  \"region\": \"Dublin\",\n",
      "  \"coordinates\": {\n",
      "    \"latitude\": 53.3489,\n",
      "    \"longitude\": -6.2009\n",
      "  },\n",
      "  \"founded\": \"1241\",\n",
      "  \"population\": 1,234,567,\n",
      "  \"area\": {\n",
      "    \"total\": 123.45,\n",
      "    \"land\": 112.34,\n",
      "    \"water\": 11.11\n",
      "  },\n",
      "  \"climate\": \" temperate\",\n",
      "  \"key_features\": [\n",
      "    \"City Walls\",\n",
      "    \"Trinity College\",\n",
      "    \"Leaving Certificate\",\n",
      "    \"St. Stephen's Cathedral\",\n",
      "    \"Glynn Bridge\"\n",
      "  ],\n",
      "  \"tourism\": [\n",
      "    \"The GAA\",\n",
      "    \"The National Library of Ireland\",\n",
      "    \"The University of Dublin\",\n",
      "    \"The Phoenix Park\",\n",
      "    \"The SSE St. Patrick's Cathedral Quarter\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you need any adjustments!\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "sampling_params = {\"temperature\": 0, \"max_new_tokens\": 2048, \"json_schema\": json_schema}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:43:07.973494Z",
     "iopub.status.busy": "2025-05-15T22:43:07.973277Z",
     "iopub.status.idle": "2025-05-15T22:43:10.320273Z",
     "shell.execute_reply": "2025-05-15T22:43:10.319468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of France.\n",
      "Generated text: \n",
      "The capital of France is Paris.\n",
      "\n",
      "The problem is to build a function that returns the capital of a country given the country's name as a string.\n",
      "\n",
      "But since there are many countries, the function must efficiently determine the capital without hardcoding all the capitals.\n",
      "\n",
      "I need to think of a way to represent the countries and their capitals in a way that allows the function to look them up quickly.\n",
      "\n",
      "What data structure is good for looking up values by key quickly? A dictionary or a hash table would be efficient.\n",
      "\n",
      "So, perhaps create a dictionary where the keys are country names and the values are their capitals.\n",
      "\n",
      "But how to get the list of\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Germany.\n",
      "Generated text: \n",
      "The capital of Germany is Berlin. It's a city located in northern Germany, known for its rich history, vibrant culture, and numerous landmarks. Berlin is home to the Brandenburg Gate, the Berlin Wall Memorial, and the Reichstag building. It's a significant political, economic, and cultural center of Europe.\n",
      "\n",
      "Given that information, what is the population of Berlin? How does this population compare to the population of the entire country of Germany? Additionally, can you provide some information about the current economic status of Berlin?\n",
      "\n",
      "I need to present these facts clearly and in a way that is easy to understand, using appropriate formatting for headings and\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Italy.\n",
      "Generated text: \n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Which is the capital city of Russia?\n",
      "\n",
      "Moscow is the capital city of Russia.\n",
      "\n",
      "What's the currency used in Australia?\n",
      "\n",
      "The currency used in Australia is the Australian dollar.\n",
      "\n",
      "What's the capital city of Canada?\n",
      "\n",
      "Ottawa is the capital city of Canada.\n",
      "\n",
      "The capital city of Germany is Berlin.\n",
      "\n",
      "Which is the capital city of Brazil?\n",
      "\n",
      "Brasília is the capital city of Brazil.\n",
      "\n",
      "The capital city of Mexico is Mexico City.\n",
      "\n",
      "The capital city of India is New Delhi.\n",
      "\n",
      "What's the capital city of South Africa?\n",
      "\n",
      "Pretoria is the capital city of South Africa.\n",
      "\n",
      "The capital city\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of France.\",\n",
    "    \"Give me the information of the capital of Germany.\",\n",
    "    \"Give me the information of the capital of Italy.\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"ebnf\": (\n",
    "        \"root ::= city | description\\n\"\n",
    "        'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "        'description ::= city \" is \" status\\n'\n",
    "        'status ::= \"the capital of \" country\\n'\n",
    "        'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "    ),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:43:10.322455Z",
     "iopub.status.busy": "2025-05-15T22:43:10.322114Z",
     "iopub.status.idle": "2025-05-15T22:43:12.539115Z",
     "shell.execute_reply": "2025-05-15T22:43:12.538294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Please provide information about London as a major global city:\n",
      "Generated text:  its population, economic status, cultural significance, and location.\n",
      "\n",
      "9 sentences total. The first sentence should be an introduction, the second and third should provide specific data on population and economic status, the fourth should discuss cultural significance, and the fifth should cover its location and strategic advantages. Additionally, the fourth and fifth paragraphs should each have two supporting facts. The sixth sentence should be a conclusion.\n",
      "\n",
      "Make sure the information is accurate. If I have a mistake in my data, correct it. Also, the response should be concise, with no more than three paragraphs. Ensure that each paragraph is distinct and covers the assigned topic.\n",
      "\n",
      "Alright, I need\n",
      "===============================\n",
      "Prompt: Please provide information about Paris as a major global city:\n",
      "Generated text:  its economic status, cultural significance, and its role as a tourist attraction.\n",
      "\n",
      "2.1. What is Paris known for? (Choose all that apply)\n",
      "a) The Eiffel Tower\n",
      "b) Paris is the capital city of France\n",
      "c) Paris is the administrative capital of France\n",
      "d) Paris is known as the \"City of Light\"\n",
      "e) The Paris Agreement\n",
      "f) Paris is the financial capital of France\n",
      "g) Paris is the cultural center of Europe\n",
      "h) Paris is a global financial hub\n",
      "\n",
      "2.2. What is the economic status of Paris? (Choose all that apply)\n",
      "a) Paris is\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Please provide information about London as a major global city:\",\n",
    "    \"Please provide information about Paris as a major global city:\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"regex\": \"(France|England)\"}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:43:12.541080Z",
     "iopub.status.busy": "2025-05-15T22:43:12.540786Z",
     "iopub.status.idle": "2025-05-15T22:43:20.727524Z",
     "shell.execute_reply": "2025-05-15T22:43:20.726820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: <｜begin▁of▁sentence｜><｜Assistant｜>Give me the information and population of the capital of France in the JSON format.<｜end▁of▁sentence｜><｜Assistant｜><think>\n",
      "\n",
      "Generated text: Alright, so I need to find the information and population of the capital of France in JSON format. First, I should figure out where the capital is. I know that Paris is the capital of France, so that's the place I'm focusing on. \n",
      "\n",
      "Now, I need to determine the population of Paris. I remember that Paris is one of the most populous cities in the world, but I'm not exactly sure of the current number. I think it's around 2 million people, but I'm not certain. Maybe I should check some sources or recall recent updates. \n",
      "\n",
      "I believe the population figure for Paris has been increasing over the years. In the past decade or so, it might have grown a bit. I think it's somewhere between 2.1 and 2.2 million. To be precise, I think the population was approximately 2,165,000 as of 2023. I should make sure that's correct, but I don't recall any significant events that would drastically change the population in a short time. \n",
      "\n",
      "Next, I need to structure this information into a JSON format. JSON typically uses key-value pairs, so I'll need to define the keys and assign the corresponding values. The main pieces of information are the city name and the population. \n",
      "\n",
      "So, the JSON structure would look like this:\n",
      "\n",
      "{\n",
      "  \"City\": \"Paris\",\n",
      "  \"Population\": 2165000\n",
      "}\n",
      "\n",
      "I should ensure that the city name is in quotes and the population is a number without quotes. Also, the population number should be accurate and up-to-date. Since I'm not entirely sure about the exact number, maybe I should double-check a reliable source to confirm. \n",
      "\n",
      "Upon checking, I find that as of the latest estimates, Paris has a population of around 2,165,000. That matches my initial thought. Therefore, the JSON provided above should be correct. \n",
      "\n",
      "I should also consider if there are any additional details that might be useful, but since the user specifically asked for the capital's city name and population, those are the two key points. \n",
      "\n",
      "In summary, the process was identifying the correct city, determining the population, ensuring accuracy, and structuring it properly in JSON format. I think I've covered all necessary steps and arrived at a correct answer.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"City\": \"Paris\",\n",
      "  \"Population\": 2165000\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "prompts = [text]\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"structural_tag\": json.dumps(\n",
    "        {\n",
    "            \"type\": \"structural_tag\",\n",
    "            \"structures\": [\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_weather>\",\n",
    "                    \"schema\": schema_get_current_weather,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_date>\",\n",
    "                    \"schema\": schema_get_current_date,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "            ],\n",
    "            \"triggers\": [\"<function=\"],\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T22:43:20.729311Z",
     "iopub.status.busy": "2025-05-15T22:43:20.729049Z",
     "iopub.status.idle": "2025-05-15T22:43:20.752237Z",
     "shell.execute_reply": "2025-05-15T22:43:20.751477Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
