
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>OpenAI APIs - Completions &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=61678c10"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'backend/openai_api_completions';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="OpenAI APIs - Vision" href="openai_api_vision.html" />
    <link rel="prev" title="Sending Requests" href="send_request.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="May 14, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../start/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backend Tutorial</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/deepseek.html">DeepSeek Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/llama4.html">Llama4 Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="send_request.html">Sending Requests</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">OpenAI APIs - Completions</a></li>
<li class="toctree-l1"><a class="reference internal" href="openai_api_vision.html">OpenAI APIs - Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="openai_api_embeddings.html">OpenAI APIs - Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1"><a class="reference internal" href="server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention_backend.html">Attention Backend</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../supported_models/generative_models.html">Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/multimodal_language_models.html">Multimodal Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/embedding_models.html">Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/reward_models.html">Reward Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/support_new_models.html">How to Support New Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="function_calling.html">Tool and Function Calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="lora.html">LoRA Serving</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frontend Tutorial</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../frontend/frontend.html">SGLang Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend/choices_methods.html">Choices Methods in SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">SGLang Router</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../router/router.html">Router for Data Parallelism</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/general.html">General Guidance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/hardware.html">Hardware Supports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/advanced_deploy.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/performance_tuning.html">Performance Tuning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/backend/openai_api_completions.ipynb?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/backend/openai_api_completions.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fbackend/openai_api_completions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/backend/openai_api_completions.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>OpenAI APIs - Completions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Chat-Completions">Chat Completions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Usage">Usage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Parameters">Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#Enabling-Model-Thinking/Reasoning">Enabling Model Thinking/Reasoning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Completions">Completions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Usage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Structured-Outputs-(JSON,-Regex,-EBNF)">Structured Outputs (JSON, Regex, EBNF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Batches">Batches</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <style>
    .output_area.stderr, .output_area.stdout {
        color: #d3d3d3 !important; /* light gray */
    }
</style><section id="OpenAI-APIs---Completions">
<h1>OpenAI APIs - Completions<a class="headerlink" href="#OpenAI-APIs---Completions" title="Link to this heading">#</a></h1>
<p>SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models. A complete reference for the API is available in the <a class="reference external" href="https://platform.openai.com/docs/api-reference">OpenAI API Reference</a>.</p>
<p>This tutorial covers the following popular APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">chat/completions</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">completions</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batches</span></code></p></li>
</ul>
<p>Check out other tutorials to learn about <a class="reference external" href="https://docs.sglang.ai/backend/openai_api_vision.html">vision APIs</a> for vision-language models and <a class="reference external" href="https://docs.sglang.ai/backend/openai_api_embeddings.html">embedding APIs</a> for embedding models.</p>
<section id="Launch-A-Server">
<h2>Launch A Server<a class="headerlink" href="#Launch-A-Server" title="Link to this heading">#</a></h2>
<p>Launch the server in your terminal and wait for it to initialize.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.test.test_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_in_ci</span>

<span class="k">if</span> <span class="n">is_in_ci</span><span class="p">():</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">patch</span><span class="w"> </span><span class="kn">import</span> <span class="n">launch_server_cmd</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sglang.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">launch_server_cmd</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">wait_for_server</span><span class="p">,</span> <span class="n">print_highlight</span><span class="p">,</span> <span class="n">terminate_process</span>


<span class="n">server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
    <span class="s2">&quot;python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 --mem-fraction-static 0.8&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Server started on http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:12] server_args=ServerArgs(model_path=&#39;qwen/qwen2.5-0.5b-instruct&#39;, tokenizer_path=&#39;qwen/qwen2.5-0.5b-instruct&#39;, tokenizer_mode=&#39;auto&#39;, skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format=&#39;auto&#39;, trust_remote_code=False, dtype=&#39;auto&#39;, kv_cache_dtype=&#39;auto&#39;, quantization=None, quantization_param_path=None, context_length=None, device=&#39;cuda&#39;, served_model_name=&#39;qwen/qwen2.5-0.5b-instruct&#39;, chat_template=None, completion_template=None, is_embedding=False, revision=None, host=&#39;0.0.0.0&#39;, port=34079, mem_fraction_static=0.8, max_running_requests=200, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy=&#39;fcfs&#39;, schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=8914251, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level=&#39;info&#39;, log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, enable_request_time_stats_logging=False, api_key=None, file_storage_path=&#39;sglang_storage&#39;, enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method=&#39;round_robin&#39;, ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args=&#39;{}&#39;, lora_paths=None, max_loras_per_batch=8, lora_backend=&#39;triton&#39;, attention_backend=None, sampling_backend=&#39;flashinfer&#39;, grammar_backend=&#39;xgrammar&#39;, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type=&#39;qk&#39;, ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode=&#39;auto&#39;, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config=&#39;&#39;, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy=&#39;write_through_selective&#39;, flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode=&#39;null&#39;, disaggregation_bootstrap_port=8998, disaggregation_transfer_backend=&#39;mooncake&#39;, disaggregation_ib_device=None, pdlb_url=None)
[2025-05-14 22:09:26] Attention backend not set. Use fa3 backend by default.
[2025-05-14 22:09:26] Init torch distributed begin.
[2025-05-14 22:09:27] Init torch distributed ends. mem usage=0.00 GB
[2025-05-14 22:09:27] Load weight begin. avail mem=78.09 GB
[2025-05-14 22:09:29] Using model weights format [&#39;*.safetensors&#39;]
[2025-05-14 22:09:30] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02&lt;00:00,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02&lt;00:00,  2.10s/it]

[2025-05-14 22:09:32] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=46.29 GB, mem usage=31.80 GB.
[2025-05-14 22:09:32] KV Cache is allocated. #tokens: 20480, K size: 0.12 GB, V size: 0.12 GB
[2025-05-14 22:09:32] Memory pool end. avail mem=45.88 GB
[2025-05-14 22:09:33] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=200, context_len=32768
[2025-05-14 22:09:33] INFO:     Started server process [829231]
[2025-05-14 22:09:33] INFO:     Waiting for application startup.
[2025-05-14 22:09:33] INFO:     Application startup complete.
[2025-05-14 22:09:33] INFO:     Uvicorn running on http://0.0.0.0:34079 (Press CTRL+C to quit)
[2025-05-14 22:09:33] INFO:     127.0.0.1:38802 - &#34;GET /v1/models HTTP/1.1&#34; 200 OK
[2025-05-14 22:09:34] INFO:     127.0.0.1:38808 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
[2025-05-14 22:09:34] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 22:09:36] INFO:     127.0.0.1:38814 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 22:09:36] The server is fired up and ready to roll!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Server started on http://localhost:34079
</pre></div></div>
</div>
</section>
<section id="Chat-Completions">
<h2>Chat Completions<a class="headerlink" href="#Chat-Completions" title="Link to this heading">#</a></h2>
<section id="Usage">
<h3>Usage<a class="headerlink" href="#Usage" title="Link to this heading">#</a></h3>
<p>The server fully implements the OpenAI API. It will automatically apply the chat template specified in the Hugging Face tokenizer, if one is available. You can also specify a custom chat template with <code class="docutils literal notranslate"><span class="pre">--chat-template</span></code> when launching the server.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;http://127.0.0.1:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:38] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 22:09:39] Decode batch. #running-req: 1, #token: 70, token usage: 0.00, cuda graph: False, gen throughput (token/s): 6.62, #queue-req: 0
[2025-05-14 22:09:39] INFO:     127.0.0.1:38824 - &#34;POST /v1/chat/completions HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Response: ChatCompletion(id='0804f7ecb7e842ba84ce532fa89e8464', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sure, here are three countries and their respective capitals:\n\n1. **United States** - Washington, D.C.\n2. **Canada** - Ottawa\n3. **Australia** - Canberra', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=151645)], created=1747260578, model='qwen/qwen2.5-0.5b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=37, total_tokens=76, completion_tokens_details=None, prompt_tokens_details=None))</strong></div>
</div>
</section>
<section id="Parameters">
<h3>Parameters<a class="headerlink" href="#Parameters" title="Link to this heading">#</a></h3>
<p>The chat completions API accepts OpenAI Chat Completions API’s parameters. Refer to <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI Chat Completions API</a> for more details.</p>
<p>SGLang extends the standard API with the <code class="docutils literal notranslate"><span class="pre">extra_body</span></code> parameter, allowing for additional customization. One key option within <code class="docutils literal notranslate"><span class="pre">extra_body</span></code> is <code class="docutils literal notranslate"><span class="pre">chat_template_kwargs</span></code>, which can be used to pass arguments to the chat template processor.</p>
<section id="Enabling-Model-Thinking/Reasoning">
<h4>Enabling Model Thinking/Reasoning<a class="headerlink" href="#Enabling-Model-Thinking/Reasoning" title="Link to this heading">#</a></h4>
<p>You can use <code class="docutils literal notranslate"><span class="pre">chat_template_kwargs</span></code> to enable or disable the model’s internal thinking or reasoning process output. Set <code class="docutils literal notranslate"><span class="pre">&quot;enable_thinking&quot;:</span> <span class="pre">True</span></code> within <code class="docutils literal notranslate"><span class="pre">chat_template_kwargs</span></code> to include the reasoning steps in the response. This requires launching the server with a compatible reasoning parser (e.g., <code class="docutils literal notranslate"><span class="pre">--reasoning-parser</span> <span class="pre">qwen3</span></code> for Qwen3 models).</p>
<p>Here’s an example demonstrating how to enable thinking and retrieve the reasoning content separately (using <code class="docutils literal notranslate"><span class="pre">separate_reasoning:</span> <span class="pre">True</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ensure the server is launched with a compatible reasoning parser, e.g.:</span>
<span class="c1"># python3 -m sglang.launch_server --model-path QwQ/Qwen3-32B-250415 --reasoning-parser qwen3 ...</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># Modify OpenAI&#39;s API key and API base to use SGLang&#39;s API server.</span>
<span class="n">openai_api_key</span> <span class="o">=</span> <span class="s2">&quot;EMPTY&quot;</span>
<span class="n">openai_api_base</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://127.0.0.1:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/v1&quot;</span> <span class="c1"># Use the correct port</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="n">openai_api_base</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;QwQ/Qwen3-32B-250415&quot;</span> <span class="c1"># Use the model loaded by the server</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;9.11 and 9.8, which is greater?&quot;</span><span class="p">}]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">extra_body</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;chat_template_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;enable_thinking&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
        <span class="s2">&quot;separate_reasoning&quot;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;response.choices[0].message.reasoning_content: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">reasoning_content</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;response.choices[0].message.content: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Example Output:</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>response.choices[0].message.reasoning_content:
 Okay, so I need to figure out which number is greater between 9.11 and 9.8. Hmm, let me think. Both numbers start with 9, right? So the whole number part is the same. That means I need to look at the decimal parts to determine which one is bigger.
...
Therefore, after checking multiple methods—aligning decimals, subtracting, converting to fractions, and using a real-world analogy—it&#39;s clear that 9.8 is greater than 9.11.

response.choices[0].message.content:
 To determine which number is greater between **9.11** and **9.8**, follow these steps:
...
**Answer**:
9.8 is greater than 9.11.
</pre></div>
</div>
<p>Setting <code class="docutils literal notranslate"><span class="pre">&quot;enable_thinking&quot;:</span> <span class="pre">False</span></code> (or omitting it) will result in <code class="docutils literal notranslate"><span class="pre">reasoning_content</span></code> being <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>Here is an example of a detailed chat completion request using standard OpenAI parameters:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a knowledgeable historian who provides concise responses.&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me about ancient Rome&quot;</span><span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Ancient Rome was a civilization centered in Italy.&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What were their major achievements?&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Lower temperature for more focused responses</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>  <span class="c1"># Reasonable length for a concise response</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>  <span class="c1"># Slightly higher for better fluency</span>
    <span class="n">presence_penalty</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Mild penalty to avoid repetition</span>
    <span class="n">frequency_penalty</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Mild penalty for more natural language</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Single response is usually more stable</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>  <span class="c1"># Keep for reproducibility</span>
<span class="p">)</span>

<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:39] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 22:09:39] Decode batch. #running-req: 1, #token: 88, token usage: 0.00, cuda graph: False, gen throughput (token/s): 67.49, #queue-req: 0
[2025-05-14 22:09:40] Decode batch. #running-req: 1, #token: 128, token usage: 0.01, cuda graph: False, gen throughput (token/s): 87.91, #queue-req: 0
[2025-05-14 22:09:40] Decode batch. #running-req: 1, #token: 168, token usage: 0.01, cuda graph: False, gen throughput (token/s): 87.94, #queue-req: 0
[2025-05-14 22:09:40] INFO:     127.0.0.1:38824 - &#34;POST /v1/chat/completions HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Ancient Rome was a remarkable civilization that left a profound impact on the world. Some of its major achievements include:<br><br>1. **Architecture**: Rome was known for its grand architecture, including the Colosseum, the Pantheon, and the Roman Forum. The Colosseum is considered one of the greatest public works of all time.<br><br>2. **Cultural Achievements**: Rome was a center of learning and culture, with the development of Latin as a language. It was also home to many famous philosophers, such as Plato and Aristotle.<br><br>3. **Military Power**: Rome's military prowess was unparalleled, leading to its dominance over</strong></div>
</div>
<p>Streaming mode is also supported.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Say this is a test&quot;</span><span class="p">}],</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:40] INFO:     127.0.0.1:38824 - &#34;POST /v1/chat/completions HTTP/1.1&#34; 200 OK
[2025-05-14 22:09:40] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0
Yes, this is indeed a test. I&#39;m designed to assist with various tasks and provide helpful responses to users like you.[2025-05-14 22:09:41] Decode batch. #running-req: 1, #token: 60, token usage: 0.00, cuda graph: False, gen throughput (token/s): 82.94, #queue-req: 0
 If you have any specific questions or need help with anything, feel free to ask, and I&#39;ll do my best to help. Let me know if I can assist with any test or task.[2025-05-14 22:09:41] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: False, gen throughput (token/s): 90.61, #queue-req: 0
</pre></div></div>
</div>
</section>
</section>
</section>
<section id="Completions">
<h2>Completions<a class="headerlink" href="#Completions" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Usage<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Completions API is similar to Chat Completions API, but without the <code class="docutils literal notranslate"><span class="pre">messages</span></code> parameter or chat templates.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:41] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 22:09:42] Decode batch. #running-req: 1, #token: 49, token usage: 0.00, cuda graph: False, gen throughput (token/s): 76.56, #queue-req: 0
[2025-05-14 22:09:42] INFO:     127.0.0.1:38824 - &#34;POST /v1/completions HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Response: Completion(id='2b7ec1b9baa440eb8e91d9133375c496', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' 1. United States - Washington, D.C.\n2. Canada - Ottawa\n3. France - Paris\n4. Germany - Berlin\n5. Japan - Tokyo\n6. Italy - Rome\n7. Spain - Madrid\n8. United Kingdom - London\n9. Australia - Canberra\n10. New', matched_stop=None)], created=1747260581, model='qwen/qwen2.5-0.5b-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=64, prompt_tokens=8, total_tokens=72, completion_tokens_details=None, prompt_tokens_details=None))</strong></div>
</div>
</section>
<section id="id2">
<h3>Parameters<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>The completions API accepts OpenAI Completions API’s parameters. Refer to <a class="reference external" href="https://platform.openai.com/docs/api-reference/completions/create">OpenAI Completions API</a> for more details.</p>
<p>Here is an example of a detailed completions request:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Write a short story about a space explorer.&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>  <span class="c1"># Moderate temperature for creative writing</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>  <span class="c1"># Longer response for a story</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>  <span class="c1"># Balanced diversity in word choice</span>
    <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;THE END&quot;</span><span class="p">],</span>  <span class="c1"># Multiple stop sequences</span>
    <span class="n">presence_penalty</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Encourage novel elements</span>
    <span class="n">frequency_penalty</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Reduce repetitive phrases</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Generate one completion</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>  <span class="c1"># For reproducible results</span>
<span class="p">)</span>

<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:42] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 22:09:42] INFO:     127.0.0.1:38824 - &#34;POST /v1/completions HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Response: Completion(id='34c6f1688e1d44c5839ce88b295dc645', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=" **Title: The Red Planet's Echo**", matched_stop='\n\n')], created=1747260582, model='qwen/qwen2.5-0.5b-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18, completion_tokens_details=None, prompt_tokens_details=None))</strong></div>
</div>
</section>
</section>
<section id="Structured-Outputs-(JSON,-Regex,-EBNF)">
<h2>Structured Outputs (JSON, Regex, EBNF)<a class="headerlink" href="#Structured-Outputs-(JSON,-Regex,-EBNF)" title="Link to this heading">#</a></h2>
<p>For OpenAI compatible structured outputs API, refer to <a class="reference external" href="https://docs.sglang.ai/backend/structured_outputs.html#OpenAI-Compatible-API">Structured Outputs</a> for more details.</p>
</section>
<section id="Batches">
<h2>Batches<a class="headerlink" href="#Batches" title="Link to this heading">#</a></h2>
<p>Batches API for chat completions and completions are also supported. You can upload your requests in <code class="docutils literal notranslate"><span class="pre">jsonl</span></code> files, create a batch job, and retrieve the results when the batch job is completed (which takes longer but costs less).</p>
<p>The batches APIs are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">batches</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batches/{batch_id}/cancel</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batches/{batch_id}</span></code></p></li>
</ul>
<p>Here is an example of a batch job for chat completions, completions are similar.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;http://127.0.0.1:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">requests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;custom_id&quot;</span><span class="p">:</span> <span class="s2">&quot;request-1&quot;</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;POST&quot;</span><span class="p">,</span>
        <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;/chat/completions&quot;</span><span class="p">,</span>
        <span class="s2">&quot;body&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke about programming&quot;</span><span class="p">}</span>
            <span class="p">],</span>
            <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;custom_id&quot;</span><span class="p">:</span> <span class="s2">&quot;request-2&quot;</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;POST&quot;</span><span class="p">,</span>
        <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;/chat/completions&quot;</span><span class="p">,</span>
        <span class="s2">&quot;body&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is Python?&quot;</span><span class="p">}],</span>
            <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">]</span>

<span class="n">input_file_path</span> <span class="o">=</span> <span class="s2">&quot;batch_requests.jsonl&quot;</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">req</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">file_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">purpose</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">)</span>

<span class="n">batch_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">input_file_id</span><span class="o">=</span><span class="n">file_response</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="s2">&quot;/v1/chat/completions&quot;</span><span class="p">,</span>
    <span class="n">completion_window</span><span class="o">=</span><span class="s2">&quot;24h&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch job created with ID: </span><span class="si">{</span><span class="n">batch_response</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:42] INFO:     127.0.0.1:47974 - &#34;POST /v1/files HTTP/1.1&#34; 200 OK
[2025-05-14 22:09:42] INFO:     127.0.0.1:47974 - &#34;POST /v1/batches HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Batch job created with ID: batch_ef63d07a-7d63-4dcd-8a91-3baf7d61798b</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:42] Prefill batch. #new-seq: 2, #new-token: 20, #cached-token: 48, token usage: 0.00, #running-req: 0, #queue-req: 0
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">while</span> <span class="n">batch_response</span><span class="o">.</span><span class="n">status</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;completed&quot;</span><span class="p">,</span> <span class="s2">&quot;failed&quot;</span><span class="p">,</span> <span class="s2">&quot;cancelled&quot;</span><span class="p">]:</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch job status: </span><span class="si">{</span><span class="n">batch_response</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">...trying again in 3 seconds...&quot;</span><span class="p">)</span>
    <span class="n">batch_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">batch_response</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>

<span class="k">if</span> <span class="n">batch_response</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;completed&quot;</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch job completed successfully!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Request counts: </span><span class="si">{</span><span class="n">batch_response</span><span class="o">.</span><span class="n">request_counts</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">result_file_id</span> <span class="o">=</span> <span class="n">batch_response</span><span class="o">.</span><span class="n">output_file_id</span>
    <span class="n">file_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">content</span><span class="p">(</span><span class="n">result_file_id</span><span class="p">)</span>
    <span class="n">result_content</span> <span class="o">=</span> <span class="n">file_response</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">result_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span>
    <span class="p">]</span>

    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Request </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;custom_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">print_highlight</span><span class="p">(</span><span class="s2">&quot;Cleaning up files...&quot;</span><span class="p">)</span>
    <span class="c1"># Only delete the result file ID since file_response is just content</span>
    <span class="n">client</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">result_file_id</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch job failed with status: </span><span class="si">{</span><span class="n">batch_response</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">batch_response</span><span class="p">,</span> <span class="s2">&quot;errors&quot;</span><span class="p">):</span>
        <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Errors: </span><span class="si">{</span><span class="n">batch_response</span><span class="o">.</span><span class="n">errors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:42] Decode batch. #running-req: 2, #token: 60, token usage: 0.00, cuda graph: False, gen throughput (token/s): 57.33, #queue-req: 0
[2025-05-14 22:09:43] Decode batch. #running-req: 2, #token: 140, token usage: 0.01, cuda graph: False, gen throughput (token/s): 154.10, #queue-req: 0
Batch job status: validating...trying again in 3 seconds...
[2025-05-14 22:09:45] INFO:     127.0.0.1:47974 - &#34;GET /v1/batches/batch_ef63d07a-7d63-4dcd-8a91-3baf7d61798b HTTP/1.1&#34; 200 OK
Batch job completed successfully!
Request counts: BatchRequestCounts(completed=2, failed=0, total=2)
[2025-05-14 22:09:45] INFO:     127.0.0.1:47974 - &#34;GET /v1/files/backend_result_file-17897f6d-8b4f-43b5-8a72-ca9593121251/content HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Request request-1:</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Response: {'status_code': 200, 'request_id': 'batch_ef63d07a-7d63-4dcd-8a91-3baf7d61798b-req_0', 'body': {'id': 'batch_ef63d07a-7d63-4dcd-8a91-3baf7d61798b-req_0', 'object': 'chat.completion', 'created': 1747260582, 'model': 'qwen/qwen2.5-0.5b-instruct', 'choices': {'index': 0, 'message': {'role': 'assistant', 'content': "Sure, here's a programming-related joke for you:\nWhy did the programmer break up with the computer?\nBecause it couldn't stand the team of comments!\nThis joke plays on the idea that programmers are sometimes seen as overly detail-oriented and analytical, and", 'tool_calls': None, 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'length', 'matched_stop': None}, 'usage': {'prompt_tokens': 35, 'completion_tokens': 50, 'total_tokens': 85}, 'system_fingerprint': None}}</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Request request-2:</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Response: {'status_code': 200, 'request_id': 'batch_ef63d07a-7d63-4dcd-8a91-3baf7d61798b-req_1', 'body': {'id': 'batch_ef63d07a-7d63-4dcd-8a91-3baf7d61798b-req_1', 'object': 'chat.completion', 'created': 1747260582, 'model': 'qwen/qwen2.5-0.5b-instruct', 'choices': {'index': 0, 'message': {'role': 'assistant', 'content': 'Python is a high-level programming language that was created and developed by the Python Software Foundation. It is the most popular general-purpose programming language used for web development, data analysis, artificial intelligence, machine learning, automation, cloud computing, mobile development, and', 'tool_calls': None, 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'length', 'matched_stop': None}, 'usage': {'prompt_tokens': 33, 'completion_tokens': 50, 'total_tokens': 83}, 'system_fingerprint': None}}</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Cleaning up files...</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:45] INFO:     127.0.0.1:47974 - &#34;DELETE /v1/files/backend_result_file-17897f6d-8b4f-43b5-8a72-ca9593121251 HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<p>It takes a while to complete the batch job. You can use these two APIs to retrieve the batch job status or cancel the batch job.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">batches/{batch_id}</span></code>: Retrieve the batch job status.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batches/{batch_id}/cancel</span></code>: Cancel the batch job.</p></li>
</ol>
<p>Here is an example to check the batch job status.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;http://127.0.0.1:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">requests</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">requests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;custom_id&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;request-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;POST&quot;</span><span class="p">,</span>
            <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;/chat/completions&quot;</span><span class="p">,</span>
            <span class="s2">&quot;body&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: You are a helpful AI assistant&quot;</span><span class="p">,</span>
                    <span class="p">},</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a detailed story about topic. Make it very long.&quot;</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">],</span>
                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">}</span>
    <span class="p">)</span>

<span class="n">input_file_path</span> <span class="o">=</span> <span class="s2">&quot;batch_requests.jsonl&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">req</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">uploaded_file</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">purpose</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">)</span>

<span class="n">batch_job</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">input_file_id</span><span class="o">=</span><span class="n">uploaded_file</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="s2">&quot;/v1/chat/completions&quot;</span><span class="p">,</span>
    <span class="n">completion_window</span><span class="o">=</span><span class="s2">&quot;24h&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Created batch job with ID: </span><span class="si">{</span><span class="n">batch_job</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial status: </span><span class="si">{</span><span class="n">batch_job</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="n">max_checks</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_checks</span><span class="p">):</span>
    <span class="n">batch_details</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">batch_id</span><span class="o">=</span><span class="n">batch_job</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>

    <span class="n">print_highlight</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Batch job details (check </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">max_checks</span><span class="si">}</span><span class="s2">) // ID: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2"> // Status: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2"> // Created at: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">created_at</span><span class="si">}</span><span class="s2"> // Input file ID: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">input_file_id</span><span class="si">}</span><span class="s2"> // Output file ID: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">output_file_id</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">print_highlight</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;&lt;strong&gt;Request counts: Total: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">request_counts</span><span class="o">.</span><span class="n">total</span><span class="si">}</span><span class="s2"> // Completed: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">request_counts</span><span class="o">.</span><span class="n">completed</span><span class="si">}</span><span class="s2"> // Failed: </span><span class="si">{</span><span class="n">batch_details</span><span class="o">.</span><span class="n">request_counts</span><span class="o">.</span><span class="n">failed</span><span class="si">}</span><span class="s2">&lt;/strong&gt;&quot;</span>
    <span class="p">)</span>

    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:45] INFO:     127.0.0.1:47978 - &#34;POST /v1/files HTTP/1.1&#34; 200 OK
[2025-05-14 22:09:45] INFO:     127.0.0.1:47978 - &#34;POST /v1/batches HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Created batch job with ID: batch_d6f445bd-c44d-4113-be92-29325d18e397</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Initial status: validating</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:45] Prefill batch. #new-seq: 6, #new-token: 180, #cached-token: 18, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 22:09:45] Prefill batch. #new-seq: 14, #new-token: 430, #cached-token: 42, token usage: 0.01, #running-req: 6, #queue-req: 0
[2025-05-14 22:09:46] Decode batch. #running-req: 20, #token: 1363, token usage: 0.07, cuda graph: False, gen throughput (token/s): 254.24, #queue-req: 0
[2025-05-14 22:09:55] INFO:     127.0.0.1:55100 - &#34;GET /v1/batches/batch_d6f445bd-c44d-4113-be92-29325d18e397 HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Batch job details (check 1 / 5) // ID: batch_d6f445bd-c44d-4113-be92-29325d18e397 // Status: completed // Created at: 1747260585 // Input file ID: backend_input_file-c814fa6c-8c18-4778-9d74-444cbcf6f3c7 // Output file ID: backend_result_file-6359c2de-e50e-4974-a8db-33a50bed8e71</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><strong>Request counts: Total: 20 // Completed: 20 // Failed: 0</strong></strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:09:58] INFO:     127.0.0.1:55100 - &#34;GET /v1/batches/batch_d6f445bd-c44d-4113-be92-29325d18e397 HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Batch job details (check 2 / 5) // ID: batch_d6f445bd-c44d-4113-be92-29325d18e397 // Status: completed // Created at: 1747260585 // Input file ID: backend_input_file-c814fa6c-8c18-4778-9d74-444cbcf6f3c7 // Output file ID: backend_result_file-6359c2de-e50e-4974-a8db-33a50bed8e71</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><strong>Request counts: Total: 20 // Completed: 20 // Failed: 0</strong></strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:10:01] INFO:     127.0.0.1:55100 - &#34;GET /v1/batches/batch_d6f445bd-c44d-4113-be92-29325d18e397 HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Batch job details (check 3 / 5) // ID: batch_d6f445bd-c44d-4113-be92-29325d18e397 // Status: completed // Created at: 1747260585 // Input file ID: backend_input_file-c814fa6c-8c18-4778-9d74-444cbcf6f3c7 // Output file ID: backend_result_file-6359c2de-e50e-4974-a8db-33a50bed8e71</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><strong>Request counts: Total: 20 // Completed: 20 // Failed: 0</strong></strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:10:04] INFO:     127.0.0.1:55100 - &#34;GET /v1/batches/batch_d6f445bd-c44d-4113-be92-29325d18e397 HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Batch job details (check 4 / 5) // ID: batch_d6f445bd-c44d-4113-be92-29325d18e397 // Status: completed // Created at: 1747260585 // Input file ID: backend_input_file-c814fa6c-8c18-4778-9d74-444cbcf6f3c7 // Output file ID: backend_result_file-6359c2de-e50e-4974-a8db-33a50bed8e71</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><strong>Request counts: Total: 20 // Completed: 20 // Failed: 0</strong></strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:10:07] INFO:     127.0.0.1:55100 - &#34;GET /v1/batches/batch_d6f445bd-c44d-4113-be92-29325d18e397 HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Batch job details (check 5 / 5) // ID: batch_d6f445bd-c44d-4113-be92-29325d18e397 // Status: completed // Created at: 1747260585 // Input file ID: backend_input_file-c814fa6c-8c18-4778-9d74-444cbcf6f3c7 // Output file ID: backend_result_file-6359c2de-e50e-4974-a8db-33a50bed8e71</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><strong>Request counts: Total: 20 // Completed: 20 // Failed: 0</strong></strong></div>
</div>
<p>Here is an example to cancel a batch job.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;http://127.0.0.1:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">requests</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5000</span><span class="p">):</span>
    <span class="n">requests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;custom_id&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;request-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;POST&quot;</span><span class="p">,</span>
            <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;/chat/completions&quot;</span><span class="p">,</span>
            <span class="s2">&quot;body&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">,</span>
                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: You are a helpful AI assistant&quot;</span><span class="p">,</span>
                    <span class="p">},</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a detailed story about topic. Make it very long.&quot;</span><span class="p">,</span>
                    <span class="p">},</span>
                <span class="p">],</span>
                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">}</span>
    <span class="p">)</span>

<span class="n">input_file_path</span> <span class="o">=</span> <span class="s2">&quot;batch_requests.jsonl&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">req</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">uploaded_file</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">purpose</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">)</span>

<span class="n">batch_job</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">input_file_id</span><span class="o">=</span><span class="n">uploaded_file</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="s2">&quot;/v1/chat/completions&quot;</span><span class="p">,</span>
    <span class="n">completion_window</span><span class="o">=</span><span class="s2">&quot;24h&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Created batch job with ID: </span><span class="si">{</span><span class="n">batch_job</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial status: </span><span class="si">{</span><span class="n">batch_job</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">cancelled_job</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">cancel</span><span class="p">(</span><span class="n">batch_id</span><span class="o">=</span><span class="n">batch_job</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cancellation initiated. Status: </span><span class="si">{</span><span class="n">cancelled_job</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">cancelled_job</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;cancelling&quot;</span>

    <span class="c1"># Monitor the cancellation process</span>
    <span class="k">while</span> <span class="n">cancelled_job</span><span class="o">.</span><span class="n">status</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;failed&quot;</span><span class="p">,</span> <span class="s2">&quot;cancelled&quot;</span><span class="p">]:</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">cancelled_job</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">batch_job</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
        <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current status: </span><span class="si">{</span><span class="n">cancelled_job</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verify final status</span>
    <span class="k">assert</span> <span class="n">cancelled_job</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;cancelled&quot;</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="s2">&quot;Batch job successfully cancelled&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error during cancellation: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">raise</span> <span class="n">e</span>

<span class="k">finally</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">del_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">files</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">uploaded_file</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">del_response</span><span class="o">.</span><span class="n">deleted</span><span class="p">:</span>
            <span class="n">print_highlight</span><span class="p">(</span><span class="s2">&quot;Successfully cleaned up input file&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">input_file_path</span><span class="p">)</span>
            <span class="n">print_highlight</span><span class="p">(</span><span class="s2">&quot;Successfully deleted local batch_requests.jsonl file&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error cleaning up: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">e</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:10:10] INFO:     127.0.0.1:41708 - &#34;POST /v1/files HTTP/1.1&#34; 200 OK
[2025-05-14 22:10:10] INFO:     127.0.0.1:41708 - &#34;POST /v1/batches HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Created batch job with ID: batch_2b633121-bbf6-449c-bedc-08c009b6126a</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Initial status: validating</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:10:11] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 256, token usage: 0.01, #running-req: 0, #queue-req: 0
[2025-05-14 22:10:11] Prefill batch. #new-seq: 26, #new-token: 432, #cached-token: 450, token usage: 0.03, #running-req: 8, #queue-req: 0
[2025-05-14 22:10:12] Decode batch. #running-req: 34, #token: 1499, token usage: 0.07, cuda graph: False, gen throughput (token/s): 37.60, #queue-req: 0
[2025-05-14 22:10:12] Prefill batch. #new-seq: 73, #new-token: 2190, #cached-token: 299, token usage: 0.09, #running-req: 34, #queue-req: 0
[2025-05-14 22:10:12] Prefill batch. #new-seq: 49, #new-token: 1470, #cached-token: 245, token usage: 0.20, #running-req: 107, #queue-req: 4844
[2025-05-14 22:10:13] Prefill batch. #new-seq: 11, #new-token: 330, #cached-token: 55, token usage: 0.37, #running-req: 155, #queue-req: 4833
[2025-05-14 22:10:13] Prefill batch. #new-seq: 3, #new-token: 90, #cached-token: 15, token usage: 0.39, #running-req: 165, #queue-req: 4830
[2025-05-14 22:10:13] Decode batch. #running-req: 167, #token: 10145, token usage: 0.50, cuda graph: False, gen throughput (token/s): 4136.79, #queue-req: 4830
[2025-05-14 22:10:13] Decode batch. #running-req: 163, #token: 16507, token usage: 0.81, cuda graph: False, gen throughput (token/s): 11137.86, #queue-req: 4830
[2025-05-14 22:10:14] Decode out of memory happened. #retracted_reqs: 24, #new_token_ratio: 0.5816 -&gt; 0.9157
[2025-05-14 22:10:14] Prefill batch. #new-seq: 24, #new-token: 720, #cached-token: 120, token usage: 0.69, #running-req: 107, #queue-req: 4830
[2025-05-14 22:10:14] Decode batch. #running-req: 131, #token: 15381, token usage: 0.75, cuda graph: False, gen throughput (token/s): 9631.80, #queue-req: 4830
[2025-05-14 22:10:14] Prefill batch. #new-seq: 2, #new-token: 60, #cached-token: 10, token usage: 0.75, #running-req: 130, #queue-req: 4828
[2025-05-14 22:10:14] Prefill batch. #new-seq: 105, #new-token: 3160, #cached-token: 515, token usage: 0.07, #running-req: 26, #queue-req: 4723
[2025-05-14 22:10:15] Decode batch. #running-req: 131, #token: 7000, token usage: 0.34, cuda graph: False, gen throughput (token/s): 8028.12, #queue-req: 4723
[2025-05-14 22:10:15] Prefill batch. #new-seq: 11, #new-token: 332, #cached-token: 53, token usage: 0.37, #running-req: 130, #queue-req: 4712
[2025-05-14 22:10:15] Prefill batch. #new-seq: 4, #new-token: 120, #cached-token: 20, token usage: 0.40, #running-req: 139, #queue-req: 4708
[2025-05-14 22:10:15] Prefill batch. #new-seq: 2, #new-token: 60, #cached-token: 10, token usage: 0.42, #running-req: 140, #queue-req: 4706
[2025-05-14 22:10:15] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 5, token usage: 0.50, #running-req: 140, #queue-req: 4705
[2025-05-14 22:10:15] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 5, token usage: 0.53, #running-req: 140, #queue-req: 4704
[2025-05-14 22:10:15] Decode batch. #running-req: 141, #token: 12464, token usage: 0.61, cuda graph: False, gen throughput (token/s): 8572.41, #queue-req: 4704
[2025-05-14 22:10:16] Decode batch. #running-req: 140, #token: 18063, token usage: 0.88, cuda graph: False, gen throughput (token/s): 9789.54, #queue-req: 4704
[2025-05-14 22:10:16] Prefill batch. #new-seq: 15, #new-token: 465, #cached-token: 60, token usage: 0.75, #running-req: 119, #queue-req: 4689
[2025-05-14 22:10:16] Prefill batch. #new-seq: 4, #new-token: 120, #cached-token: 20, token usage: 0.79, #running-req: 132, #queue-req: 4685
[2025-05-14 22:10:16] Prefill batch. #new-seq: 96, #new-token: 3010, #cached-token: 350, token usage: 0.16, #running-req: 36, #queue-req: 4589
[2025-05-14 22:10:16] Decode batch. #running-req: 132, #token: 7387, token usage: 0.36, cuda graph: False, gen throughput (token/s): 8089.87, #queue-req: 4589
[2025-05-14 22:10:17] Prefill batch. #new-seq: 19, #new-token: 584, #cached-token: 81, token usage: 0.41, #running-req: 131, #queue-req: 4570
[2025-05-14 22:10:17] Prefill batch. #new-seq: 13, #new-token: 397, #cached-token: 58, token usage: 0.40, #running-req: 149, #queue-req: 4557
[2025-05-14 22:10:17] Prefill batch. #new-seq: 3, #new-token: 90, #cached-token: 15, token usage: 0.43, #running-req: 153, #queue-req: 4554
[2025-05-14 22:10:17] Prefill batch. #new-seq: 5, #new-token: 155, #cached-token: 20, token usage: 0.42, #running-req: 152, #queue-req: 4549
[2025-05-14 22:10:17] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 5, token usage: 0.44, #running-req: 155, #queue-req: 4548
[2025-05-14 22:10:17] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 5, token usage: 0.50, #running-req: 154, #queue-req: 4547
[2025-05-14 22:10:17] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 5, token usage: 0.51, #running-req: 154, #queue-req: 4546
[2025-05-14 22:10:17] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 5, token usage: 0.55, #running-req: 152, #queue-req: 4545
[2025-05-14 22:10:17] Decode batch. #running-req: 153, #token: 11620, token usage: 0.57, cuda graph: False, gen throughput (token/s): 8468.00, #queue-req: 4545
[2025-05-14 22:10:18] Decode batch. #running-req: 150, #token: 17422, token usage: 0.85, cuda graph: False, gen throughput (token/s): 10620.91, #queue-req: 4545
[2025-05-14 22:10:18] Prefill batch. #new-seq: 4, #new-token: 123, #cached-token: 17, token usage: 0.83, #running-req: 135, #queue-req: 4541
[2025-05-14 22:10:18] Prefill batch. #new-seq: 3, #new-token: 90, #cached-token: 15, token usage: 0.84, #running-req: 135, #queue-req: 4538
[2025-05-14 22:10:18] Decode batch. #running-req: 138, #token: 5867, token usage: 0.29, cuda graph: False, gen throughput (token/s): 9319.73, #queue-req: 4538
[2025-05-14 22:10:18] Prefill batch. #new-seq: 85, #new-token: 2702, #cached-token: 273, token usage: 0.29, #running-req: 47, #queue-req: 4453
[2025-05-14 22:10:19] Prefill batch. #new-seq: 24, #new-token: 735, #cached-token: 105, token usage: 0.49, #running-req: 131, #queue-req: 4429
[2025-05-14 22:10:19] Prefill batch. #new-seq: 24, #new-token: 758, #cached-token: 82, token usage: 0.43, #running-req: 138, #queue-req: 4405
[2025-05-14 22:10:19] Prefill batch. #new-seq: 21, #new-token: 650, #cached-token: 85, token usage: 0.39, #running-req: 150, #queue-req: 4384
[2025-05-14 22:10:19] Prefill batch. #new-seq: 7, #new-token: 217, #cached-token: 28, token usage: 0.42, #running-req: 168, #queue-req: 4377
[2025-05-14 22:10:19] Prefill batch. #new-seq: 5, #new-token: 152, #cached-token: 23, token usage: 0.44, #running-req: 171, #queue-req: 4372
[2025-05-14 22:10:19] Prefill batch. #new-seq: 2, #new-token: 60, #cached-token: 10, token usage: 0.45, #running-req: 175, #queue-req: 4370
[2025-05-14 22:10:19] Decode batch. #running-req: 174, #token: 10920, token usage: 0.53, cuda graph: False, gen throughput (token/s): 8304.83, #queue-req: 4370
[2025-05-14 22:10:20] Decode batch. #running-req: 169, #token: 17328, token usage: 0.85, cuda graph: False, gen throughput (token/s): 11554.06, #queue-req: 4370
[2025-05-14 22:10:20] Decode out of memory happened. #retracted_reqs: 27, #new_token_ratio: 0.5214 -&gt; 0.8862
[2025-05-14 22:10:20] Decode batch. #running-req: 135, #token: 19449, token usage: 0.95, cuda graph: False, gen throughput (token/s): 10335.93, #queue-req: 4397
[2025-05-14 22:10:20] Decode out of memory happened. #retracted_reqs: 17, #new_token_ratio: 0.8581 -&gt; 1.0000
[2025-05-14 22:10:20] Prefill batch. #new-seq: 82, #new-token: 2578, #cached-token: 292, token usage: 0.25, #running-req: 118, #queue-req: 4332
[2025-05-14 22:10:20] INFO:     127.0.0.1:59804 - &#34;POST /v1/batches/batch_2b633121-bbf6-449c-bedc-08c009b6126a/cancel HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Cancellation initiated. Status: cancelling</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:10:23] INFO:     127.0.0.1:59804 - &#34;GET /v1/batches/batch_2b633121-bbf6-449c-bedc-08c009b6126a HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Current status: cancelled</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Batch job successfully cancelled</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 22:10:23] INFO:     127.0.0.1:59804 - &#34;DELETE /v1/files/backend_input_file-312b73d3-eef9-43d3-93b8-5c430bcdc9e3 HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Successfully cleaned up input file</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Successfully deleted local batch_requests.jsonl file</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="send_request.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Sending Requests</p>
      </div>
    </a>
    <a class="right-next"
       href="openai_api_vision.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">OpenAI APIs - Vision</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Chat-Completions">Chat Completions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Usage">Usage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Parameters">Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#Enabling-Model-Thinking/Reasoning">Enabling Model Thinking/Reasoning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Completions">Completions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Usage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Structured-Outputs-(JSON,-Regex,-EBNF)">Structured Outputs (JSON, Regex, EBNF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Batches">Batches</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023-2025, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on May 14, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>