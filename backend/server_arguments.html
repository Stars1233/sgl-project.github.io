
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Server Arguments &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=0c96e96c"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'backend/server_arguments';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sampling Parameters" href="sampling_params.html" />
    <link rel="prev" title="Offline Engine API" href="offline_engine_api.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="0.4.3.post4" />
    <meta name="docbuild:last-update" content="Mar 06, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="SGLang - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../start/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backend Tutorial</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="openai_api_completions.html">OpenAI APIs - Completions</a></li>
<li class="toctree-l1"><a class="reference internal" href="openai_api_vision.html">OpenAI APIs - Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="openai_api_embeddings.html">OpenAI APIs - Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="function_calling.html">Tool and Function Calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frontend Tutorial</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../frontend/frontend.html">SGLang Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend/choices_methods.html">Choices Methods in SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">SGLang Router</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../router/router.html">Router for Data Parallelism</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/deepseek.html">DeepSeek Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/general.html">General Guidance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/hardware.html">Hardware Supports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/advanced_deploy.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/performance_tuning.html">Performance Tuning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/backend/server_arguments.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/backend/server_arguments.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fbackend/server_arguments.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/backend/server_arguments.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Server Arguments</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-launch-commands">Common launch commands</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-tokenizer">Model and tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-http-api">Serving: HTTP &amp; API</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#http-server-configuration">HTTP Server configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-configuration">API configuration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelism">Parallelism</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallelism">Tensor parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-parallelism">Data parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expert-parallelism">Expert parallelism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-and-scheduling">Memory and scheduling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-runtime-options">Other runtime options</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logging">Logging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-distributed-serving">Multi-node distributed serving</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-backend">Kernel backend</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-decoding">Constrained Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#speculative-decoding">Speculative decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#double-sparsity">Double Sparsity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-options">Debug options</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="server-arguments">
<h1>Server Arguments<a class="headerlink" href="#server-arguments" title="Link to this heading">#</a></h1>
<section id="common-launch-commands">
<h2>Common launch commands<a class="headerlink" href="#common-launch-commands" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>To enable multi-GPU tensor parallelism, add <code class="docutils literal notranslate"><span class="pre">--tp</span> <span class="pre">2</span></code>. If it reports the error “peer access is not supported between these two devices”, add <code class="docutils literal notranslate"><span class="pre">--enable-p2p-check</span></code> to the server launch command.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span>--tp<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<ul class="simple">
<li><p>To enable multi-GPU data parallelism, add <code class="docutils literal notranslate"><span class="pre">--dp</span> <span class="pre">2</span></code>. Data parallelism is better for throughput if there is enough memory. It can also be used together with tensor parallelism. The following command uses 4 GPUs in total. We recommend <a class="reference internal" href="../router/router.html"><span class="std std-doc">SGLang Router</span></a> for data parallelism.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang_router.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span>--dp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--tp<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If you see out-of-memory errors during serving, try to reduce the memory usage of the KV cache pool by setting a smaller value of <code class="docutils literal notranslate"><span class="pre">--mem-fraction-static</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">0.9</span></code>.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7
</pre></div>
</div>
<ul class="simple">
<li><p>See <a class="reference internal" href="hyperparameter_tuning.html"><span class="std std-doc">hyperparameter tuning</span></a> on tuning hyperparameters for better performance.</p></li>
<li><p>If you see out-of-memory errors during prefill for long prompts, try to set a smaller chunked prefill size.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span>--chunked-prefill-size<span class="w"> </span><span class="m">4096</span>
</pre></div>
</div>
<ul class="simple">
<li><p>To enable torch.compile acceleration, add <code class="docutils literal notranslate"><span class="pre">--enable-torch-compile</span></code>. It accelerates small models on small batch sizes. This does not work for FP8 currently. You can refer to <a class="reference external" href="https://docs.sglang.ai/backend/hyperparameter_tuning.html#enabling-cache-for-torch-compile"><code class="docutils literal notranslate"><span class="pre">Enabling</span> <span class="pre">cache</span> <span class="pre">for</span> <span class="pre">torch.compile</span></code></a> for more details.</p></li>
<li><p>To enable torchao quantization, add <code class="docutils literal notranslate"><span class="pre">--torchao-config</span> <span class="pre">int4wo-128</span></code>. It supports other <a class="reference external" href="https://github.com/sgl-project/sglang/blob/v0.3.6/python/sglang/srt/server_args.py#L671">quantization strategies (INT8/FP8)</a> as well.</p></li>
<li><p>To enable fp8 weight quantization, add <code class="docutils literal notranslate"><span class="pre">--quantization</span> <span class="pre">fp8</span></code> on a fp16 checkpoint or directly load a fp8 checkpoint without specifying any arguments.</p></li>
<li><p>To enable fp8 kv cache quantization, add <code class="docutils literal notranslate"><span class="pre">--kv-cache-dtype</span> <span class="pre">fp8_e5m2</span></code>.</p></li>
<li><p>If the model does not have a chat template in the Hugging Face tokenizer, you can specify a <a class="reference internal" href="custom_chat_template.html"><span class="std std-doc">custom chat template</span></a>.</p></li>
<li><p>To run tensor parallelism on multiple nodes, add <code class="docutils literal notranslate"><span class="pre">--nnodes</span> <span class="pre">2</span></code>. If you have two nodes with two GPUs on each node and want to run TP=4, let <code class="docutils literal notranslate"><span class="pre">sgl-dev-0</span></code> be the hostname of the first node and <code class="docutils literal notranslate"><span class="pre">50000</span></code> be an available port, you can use the following commands. If you meet deadlock, please try to add <code class="docutils literal notranslate"><span class="pre">--disable-cuda-graph</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Node 0</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span>--tp<span class="w"> </span><span class="m">4</span><span class="w"> </span>--dist-init-addr<span class="w"> </span>sgl-dev-0:50000<span class="w"> </span>--nnodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--node-rank<span class="w"> </span><span class="m">0</span>

<span class="c1"># Node 1</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span>--tp<span class="w"> </span><span class="m">4</span><span class="w"> </span>--dist-init-addr<span class="w"> </span>sgl-dev-0:50000<span class="w"> </span>--nnodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--node-rank<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p>Please consult the documentation below to learn more about the parameters you may provide when launching a server.</p>
</section>
<section id="model-and-tokenizer">
<h2>Model and tokenizer<a class="headerlink" href="#model-and-tokenizer" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_path</span></code>: Path to the model that will be served.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer_path</span></code>: Defaults to the <code class="docutils literal notranslate"><span class="pre">model_path</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer_mode</span></code>: By default <code class="docutils literal notranslate"><span class="pre">auto</span></code>, see <a class="reference external" href="https://huggingface.co/docs/transformers/en/main_classes/tokenizer">here</a> for different mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">load_format</span></code>: The format the weights are loaded in. Defaults to <code class="docutils literal notranslate"><span class="pre">*.safetensors</span></code>/<code class="docutils literal notranslate"><span class="pre">*.bin</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trust_remote_code</span></code>:  If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will use locally cached config files, otherwise use remote configs in HuggingFace.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dtype</span></code>: Dtype used for the model, defaults to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kv_cache_dtype</span></code>: Dtype of the kv cache, defaults to the <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">context_length</span></code>: The number of tokens our model can process <em>including the input</em>. Note that extending the default might lead to strange behavior.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device</span></code>: The device we put the model, defaults to <code class="docutils literal notranslate"><span class="pre">cuda</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">chat_template</span></code>: The chat template to use. Deviating from the default might lead to unexpected responses. For multi-modal chat templates, refer to <a class="reference external" href="https://docs.sglang.ai/backend/openai_api_vision.ipynb#Chat-Template">here</a>. <strong>Make sure the correct</strong> <code class="docutils literal notranslate"><span class="pre">chat_template</span></code> <strong>is passed, or performance degradation may occur!!!!</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_embedding</span></code>: Set to true to perform <a class="reference internal" href="openai_api_embeddings.html"><span class="std std-doc">embedding</span></a> / <a class="reference external" href="https://docs.sglang.ai/backend/native_api#Encode-(embedding-model)">encode</a> and <a class="reference external" href="https://docs.sglang.ai/backend/native_api#Classify-(reward-model)">reward</a> tasks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">revision</span></code>: Adjust if a specific version of the model should be used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">skip_tokenizer_init</span></code>: Set to true to provide the tokens to the engine and get the output tokens directly, typically used in RLHF. Please see this <a class="reference external" href="https://github.com/sgl-project/sglang/blob/main/examples/runtime/token_in_token_out/">example for reference</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">json_model_override_args</span></code>: Override model config with the provided JSON.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">delete_ckpt_after_loading</span></code>: Delete the model checkpoint after loading the model.</p></li>
</ul>
</section>
<section id="serving-http-api">
<h2>Serving: HTTP &amp; API<a class="headerlink" href="#serving-http-api" title="Link to this heading">#</a></h2>
<section id="http-server-configuration">
<h3>HTTP Server configuration<a class="headerlink" href="#http-server-configuration" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">port</span></code> and <code class="docutils literal notranslate"><span class="pre">host</span></code>: Setup the host for HTTP server. By default <code class="docutils literal notranslate"><span class="pre">host:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&quot;127.0.0.1&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">port:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">30000</span></code></p></li>
</ul>
</section>
<section id="api-configuration">
<h3>API configuration<a class="headerlink" href="#api-configuration" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">api_key</span></code>: Sets an API key for the server and the OpenAI-compatible API.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">file_storage_path</span></code>: Directory for storing uploaded or generated files from API calls.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_cache_report</span></code>: If set, includes detailed usage of cached tokens in the response usage.</p></li>
</ul>
</section>
</section>
<section id="parallelism">
<h2>Parallelism<a class="headerlink" href="#parallelism" title="Link to this heading">#</a></h2>
<section id="tensor-parallelism">
<h3>Tensor parallelism<a class="headerlink" href="#tensor-parallelism" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tp_size</span></code>: The number of GPUs the model weights get sharded over. Mainly for saving memory rather than for high throughput, see <a class="reference external" href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html#how-tensor-parallel-works">this blogpost</a>.</p></li>
</ul>
</section>
<section id="data-parallelism">
<h3>Data parallelism<a class="headerlink" href="#data-parallelism" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dp_size</span></code>: Will be deprecated. The number of data-parallel copies of the model. <a class="reference internal" href="../router/router.html"><span class="std std-doc">SGLang router</span></a> is recommended instead of the current naive data parallel.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">load_balance_method</span></code>: Will be deprecated. Load balancing strategy for data parallel requests.</p></li>
</ul>
</section>
<section id="expert-parallelism">
<h3>Expert parallelism<a class="headerlink" href="#expert-parallelism" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">enable_ep_moe</span></code>: Enables expert parallelism that distributes the experts onto multiple GPUs for MoE models.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ep_size</span></code>: The size of EP. Please shard the model weights with <code class="docutils literal notranslate"><span class="pre">tp_size=ep_size</span></code>, for detailed benchmarking refer to <a class="reference external" href="https://github.com/sgl-project/sglang/pull/2203">this PR</a>. If not set, <code class="docutils literal notranslate"><span class="pre">ep_size</span></code> will be automatically set to <code class="docutils literal notranslate"><span class="pre">tp_size</span></code>.</p></li>
</ul>
</section>
</section>
<section id="memory-and-scheduling">
<h2>Memory and scheduling<a class="headerlink" href="#memory-and-scheduling" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mem_fraction_static</span></code>: Fraction of the free GPU memory used for static memory like model weights and KV cache. If building KV cache fails, it should be increased. If CUDA runs out of memory, it should be decreased.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_running_requests</span></code>: The maximum number of requests to run concurrently.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_total_tokens</span></code>: The maximum number of tokens that can be stored into the KV cache. Use mainly for debugging.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">chunked_prefill_size</span></code>: Perform the prefill in chunks of these size. Larger chunk size speeds up the prefill phase but increases the VRAM consumption. If CUDA runs out of memory, it should be decreased.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_prefill_tokens</span></code>: Token budget of how many tokens to accept in one prefill batch. The actual number is the max of this parameter and the <code class="docutils literal notranslate"><span class="pre">context_length</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">schedule_policy</span></code>: The scheduling policy to control the processing order of waiting prefill requests in a single engine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">schedule_conservativeness</span></code>: Can be used to decrease/increase the conservativeness of the server when taking new requests. Highly conservative behavior leads to starvation, but low conservativeness leads to slowed-down performance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cpu_offload_gb</span></code>: Reserve this amount of RAM in GB for offloading of model parameters to the CPU.</p></li>
</ul>
</section>
<section id="other-runtime-options">
<h2>Other runtime options<a class="headerlink" href="#other-runtime-options" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">stream_interval</span></code>: Interval (in tokens) for streaming responses. Smaller values lead to smoother streaming, and larger values lead to better throughput.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_seed</span></code>: Can be used to enforce more deterministic behavior.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">watchdog_timeout</span></code>: Adjusts the watchdog thread’s timeout before killing the server if batch generation takes too long.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">download_dir</span></code>: Use to override the default Hugging Face cache directory for model weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">base_gpu_id</span></code>: Use to adjust first GPU used to distribute the model across available GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">allow_auto_truncate</span></code>: Automatically truncate requests that exceed the maximum input length.</p></li>
</ul>
</section>
<section id="logging">
<h2>Logging<a class="headerlink" href="#logging" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">log_level</span></code>: Global log verbosity.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_level_http</span></code>: Separate verbosity level for the HTTP server logs (if unset, defaults to <code class="docutils literal notranslate"><span class="pre">log_level</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_requests</span></code>: Logs the inputs and outputs of all requests for debugging.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">show_time_cost</span></code>: Prints or logs detailed timing info for internal operations (helpful for performance tuning).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_metrics</span></code>: Exports Prometheus-like metrics for request usage and performance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">decode_log_interval</span></code>: How often (in tokens) to log decode progress.</p></li>
</ul>
</section>
<section id="multi-node-distributed-serving">
<h2>Multi-node distributed serving<a class="headerlink" href="#multi-node-distributed-serving" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist_init_addr</span></code>: The TCP address used for initializing PyTorch’s distributed backend (e.g. <code class="docutils literal notranslate"><span class="pre">192.168.0.2:25000</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nnodes</span></code>: Total number of nodes in the cluster. Refer to how to run the <a class="reference external" href="https://docs.sglang.ai/references/llama_405B.html#run-405b-fp16-on-two-nodes">Llama 405B model</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">node_rank</span></code>: Rank (ID) of this node among the <code class="docutils literal notranslate"><span class="pre">nnodes</span></code> in the distributed setup.</p></li>
</ul>
</section>
<section id="lora">
<h2>LoRA<a class="headerlink" href="#lora" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">lora_paths</span></code>: You may provide a list of adapters to your model as a list. Each batch element will get model response with the corresponding lora adapter applied. Currently <code class="docutils literal notranslate"><span class="pre">cuda_graph</span></code> and <code class="docutils literal notranslate"><span class="pre">radix_attention</span></code> are not supported with this option so you need to disable them manually. We are still working on through these <a class="reference external" href="https://github.com/sgl-project/sglang/issues/2929">issues</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_loras_per_batch</span></code>: Maximum number of LoRAs in a running batch including base model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lora_backend</span></code>: The backend of running GEMM kernels for Lora modules, can be one of <code class="docutils literal notranslate"><span class="pre">triton</span></code> or <code class="docutils literal notranslate"><span class="pre">flashinfer</span></code>. Defaults to be <code class="docutils literal notranslate"><span class="pre">triton</span></code>.</p></li>
</ul>
</section>
<section id="kernel-backend">
<h2>Kernel backend<a class="headerlink" href="#kernel-backend" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">attention_backend</span></code>: The backend for attention computation and KV cache management.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sampling_backend</span></code>: The backend for sampling.</p></li>
</ul>
</section>
<section id="constrained-decoding">
<h2>Constrained Decoding<a class="headerlink" href="#constrained-decoding" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">grammar_backend</span></code>: The grammar backend for constraint decoding. Detailed usage can be found in this <a class="reference external" href="https://docs.sglang.ai/backend/structured_outputs.html">document</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constrained_json_whitespace_pattern</span></code>: Use with <code class="docutils literal notranslate"><span class="pre">Outlines</span></code> grammar backend to allow JSON with syntatic newlines, tabs or multiple spaces. Details can be found <a class="reference external" href="https://dottxt-ai.github.io/outlines/latest/reference/generation/json/#using-pydantic">here</a>.</p></li>
</ul>
</section>
<section id="speculative-decoding">
<h2>Speculative decoding<a class="headerlink" href="#speculative-decoding" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">speculative_draft_model_path</span></code>: The draft model path for speculative decoding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">speculative_algorithm</span></code>: The algorithm for speculative decoding. Currently only <a class="reference external" href="https://arxiv.org/html/2406.16858v1">Eagle</a> is supported. Note that the radix cache, chunked prefill, and overlap scheduler are disabled when using eagle speculative decoding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">speculative_num_steps</span></code>: How many draft passes we run before verifying.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">speculative_num_draft_tokens</span></code>: The number of tokens proposed in a draft.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">speculative_eagle_topk</span></code>: The number of top candidates we keep for verification at each step for <a class="reference external" href="https://arxiv.org/html/2406.16858v1">Eagle</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">speculative_token_map</span></code>: Optional, the path to the high frequency token list of <a class="reference external" href="https://arxiv.org/html/2502.14856v1">FR-Spec</a>, used for accelerating <a class="reference external" href="https://arxiv.org/html/2406.16858v1">Eagle</a>.</p></li>
</ul>
</section>
<section id="double-sparsity">
<h2>Double Sparsity<a class="headerlink" href="#double-sparsity" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">enable_double_sparsity</span></code>: Enables <a class="reference external" href="https://arxiv.org/html/2408.07092v2">double sparsity</a> which increases throughput.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ds_channel_config_path</span></code>: The double sparsity config. For a guide on how to generate the config for your model see <a class="reference external" href="https://github.com/andy-yang-1/DoubleSparse/tree/main/config">this repo</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ds_heavy_channel_num</span></code>: Number of channel indices to keep for each layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ds_heavy_token_num</span></code>: Number of tokens used for attention during decode. Skip sparse decoding if <code class="docutils literal notranslate"><span class="pre">min_seq_len</span></code> in batch &lt; this number.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ds_heavy_channel_type</span></code>: The type of heavy channels. Either <code class="docutils literal notranslate"><span class="pre">q</span></code>, <code class="docutils literal notranslate"><span class="pre">k</span></code> or <code class="docutils literal notranslate"><span class="pre">qk</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ds_sparse_decode_threshold</span></code>: Don’t apply sparse decoding if <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> in batch &lt; this threshold.</p></li>
</ul>
</section>
<section id="debug-options">
<h2>Debug options<a class="headerlink" href="#debug-options" title="Link to this heading">#</a></h2>
<p><em>Note: We recommend to stay with the defaults and only use these options for debugging for best possible performance.</em></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">disable_radix_cache</span></code>: Disable <a class="reference external" href="https://lmsys.org/blog/2024-01-17-sglang/">Radix</a> backend for prefix caching.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">disable_cuda_graph</span></code>: Disable <a class="reference external" href="https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/">cuda graph</a> for model forward. Use if encountering uncorrectable CUDA ECC errors.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">disable_cuda_graph_padding</span></code>: Disable cuda graph when padding is needed. In other case still use cuda graph.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">disable_outlines_disk_cache</span></code>: Disable disk cache for outlines grammar backend.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">disable_custom_all_reduce</span></code>: Disable usage of custom all reduce kernel.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">disable_mla</span></code>: Disable <a class="reference external" href="https://arxiv.org/html/2405.04434v5">Multi-Head Latent Attention</a> for Deepseek model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">disable_overlap_schedule</span></code>: Disable the <a class="reference external" href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#zero-overhead-batch-scheduler">Overhead-Scheduler</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_nan_detection</span></code>: Turning this on makes the sampler print a warning if the logits contain <code class="docutils literal notranslate"><span class="pre">NaN</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_p2p_check</span></code>: Turns off the default of allowing always p2p check when accessing GPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">triton_attention_reduce_in_fp32</span></code>: In triton kernels this will cast the intermediate attention result to <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p></li>
</ul>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h2>
<p><em>Note: Some of these options are still in experimental stage.</em></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">enable_mixed_chunk</span></code>: Enables mixing prefill and decode, see <a class="reference external" href="https://github.com/sgl-project/sglang/discussions/1163">this discussion</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_dp_attention</span></code>: Enable <a class="reference external" href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models">Data Parallelism Attention</a> for Deepseek models. Note that you need to choose <code class="docutils literal notranslate"><span class="pre">dp_size</span> <span class="pre">=</span> <span class="pre">tp_size</span></code> for this.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_torch_compile</span></code>: Torch compile the model. Note that compiling a model takes a long time but have a great performance boost. The compiled model can also be <a class="reference external" href="https://docs.sglang.ai/backend/hyperparameter_tuning.html#enabling-cache-for-torch-compile">cached for future use</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch_compile_max_bs</span></code>: The maximum batch size when using <code class="docutils literal notranslate"><span class="pre">torch_compile</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_graph_max_bs</span></code>: Adjust the maximum batchsize when using cuda graph. By default this is chosen for you based on GPU specifics.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_graph_bs</span></code>: The batch sizes to capture by <code class="docutils literal notranslate"><span class="pre">CudaGraphRunner</span></code>. By default this is done for you.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torchao_config</span></code>: Experimental feature that optimizes the model with <a class="reference external" href="https://github.com/pytorch/ao">torchao</a>. Possible choices are: int8dq, int8wo, int4wo-&lt;group_size&gt;, fp8wo, fp8dq-per_tensor, fp8dq-per_row.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">triton_attention_num_kv_splits</span></code>: Use to adjust the number of KV splits in triton kernels. Default is 8.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_flashinfer_mla</span></code>: Use the attention backend with flashinfer MLA wrapper for deepseek models. When providing this argument, <code class="docutils literal notranslate"><span class="pre">attention_backend</span></code> argument is overridden.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flashinfer_mla_disable_ragged</span></code>: Disable usage of ragged prefill wrapper for flashinfer mla attention backend. Should be used when <code class="docutils literal notranslate"><span class="pre">enable_flashinfer_mla</span></code> is turned on.</p></li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="offline_engine_api.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Offline Engine API</p>
      </div>
    </a>
    <a class="right-next"
       href="sampling_params.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sampling Parameters</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-launch-commands">Common launch commands</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-tokenizer">Model and tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-http-api">Serving: HTTP &amp; API</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#http-server-configuration">HTTP Server configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-configuration">API configuration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelism">Parallelism</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallelism">Tensor parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-parallelism">Data parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expert-parallelism">Expert parallelism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-and-scheduling">Memory and scheduling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-runtime-options">Other runtime options</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logging">Logging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-distributed-serving">Multi-node distributed serving</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora">LoRA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-backend">Kernel backend</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-decoding">Constrained Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#speculative-decoding">Speculative decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#double-sparsity">Double Sparsity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-options">Debug options</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization">Optimization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023-2025, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Mar 06, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>